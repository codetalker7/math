# Vector Calculus

Much of this chapter is inspired from [@hubbardcalculus]. Throughout this chapter, we'll be using 
the word *manifold* to refer to *smooth manifolds embedded in $\mathbb{R}^n$*.

## The Inverse and Implicit function theorems

This section just contains the statements of these two theorems without proof.
For proofs, I'll refer the reader to Lectures 9 through 11 of these amazing notes:
https://www.cmi.ac.in/~pramath/ANA2/Lectures/.

::: {.theorem #inverse-function-theorem name="Inverse Function Theorem"}

Let $U$ be an open subset of $\mathbb{R}^n$, $\boldsymbol{f}: U\to\mathbb{R}^n$ be
a $\mathscr{C}^1$ map, $\boldsymbol{a}\in U$ be a point such that $\boldsymbol{f}'(\boldsymbol{a})$ is
invertible, and let $\boldsymbol{b} = \boldsymbol{a}$. Then there exists an open neighborhood $V$ of 
$\boldsymbol{a}$ in $U$, an open neighborhood $W$ of $\boldsymbol{b}$ in $\mathbb{R}^n$ such that
$W = \boldsymbol{f}(V)$, $\boldsymbol{f}|_{V}:V\to W$ is one-to-one, and the inverse function
$\boldsymbol{\varphi}^{-1}:W\to V$ of $\boldsymbol{f}|_{V}$ is also $\mathscr{C}^1$.

:::

::: {.theorem #implicit-function-theorem name="Implicit Function Theorem"}

Let $d, m$ be non-negative integers, $n = d + m$, and write all points of
$\mathbb{R}^n = \mathbb{R}^d\times \mathbb{R}^m$ in the form $(\boldsymbol{x}, \boldsymbol{y})$ with 
$\boldsymbol{x}\in\mathbb{R}^d$ and $\boldsymbol{y}\in\mathbb{R}^m$. Let $U$ be an open subset of 
$\mathbb{R}^n$, $\boldsymbol{p} = (\boldsymbol{a}, \boldsymbol{b})$ a point in $U$, and 
$\boldsymbol{\varphi}:U\to\mathbb{R}^m$ a $\mathscr{C}^1$ map such that the $m\times m$ matrix

\begin{equation}
\begin{split}
    \frac{\partial \boldsymbol{\varphi}(\boldsymbol{p})}{\partial \boldsymbol{y}} := 
    \begin{bmatrix}
        D_{d + 1}\boldsymbol{\varphi}(\boldsymbol{p}) & D_{d + 2}\boldsymbol{\varphi}(\boldsymbol{p}) & \cdots &  D_{n}\boldsymbol{\varphi}(\boldsymbol{p})
    \end{bmatrix} 
\end{split}
\end{equation}

is invertible. Let $\boldsymbol{c} = \boldsymbol{\varphi}(\boldsymbol{p})$. Then there exists an open 
neighborhood $W$ of $\boldsymbol{a}$ in $\mathbb{R}^d$, and a unique $\mathscr{C}^1$ map
$\boldsymbol{f}:W\to\mathbb{R}^m$ with the following properties:

1. $\boldsymbol{f}(\boldsymbol{a}) = \boldsymbol{b}$. 
2. $(\boldsymbol{x}, \boldsymbol{f}(\boldsymbol{x}))\in U$ for all $\boldsymbol{x}\in W$.
3. $\boldsymbol{f}$ is an implicit solution of the equation $\boldsymbol{\varphi}(\boldsymbol{x}, \boldsymbol{y}) = \boldsymbol{c}$, i.e 

\begin{equation}
\begin{split}
    \boldsymbol{\varphi}(\boldsymbol{x}, \boldsymbol{f}(\boldsymbol{x})) &= \boldsymbol{c}\quad\forall \boldsymbol{x}\in W 

\end{split}
\end{equation}

:::


## Smooth manifolds in $\mathbb{R}^n$

The goal of this section is to define smooth manifolds in $\mathbb{R}^n$,
*without* resorting to tools from general topological manifolds. This will 
be helpful to computer scientists who don't want to deal with abstract notions
pertaining to manifold theory.

::: {.definition #manifold name="Smooth Manifolds"}

A subset $M\subset \mathbb{R}^n$ is a ***smooth $k$-dimensional manifold*** if locally
it is the graph of a $\mathscr{C}^1$ mapping $\boldsymbol{f}$ expressing $n - k$ 
variables as functions of the other $k$ variables. In this book, we don't 
deal with *topological* manifolds, but only subsets of $\mathbb{R}^n$ for any $n$.

More precisely, suppose we write each point $\boldsymbol{p}\in\mathbb{R}^n$ as a point
$(\boldsymbol{x}, \boldsymbol{y})\in\mathbb{R}^k\times \mathbb{R}^{n - k}$, where we
have implicitly assumed that the first $k$ coordinates are independent. Then, 
this definition implies that for every $(\boldsymbol{x}, \boldsymbol{y})\in M$, there
is an open neighborhood $\boldsymbol{x}\in U\subset \mathbb{R}^k$ of $\boldsymbol{x}$ 
and an open neighborhood $W\subset\mathbb{R}^n$ of $(\boldsymbol{x}, \boldsymbol{y})$
such that

\begin{equation}
\begin{split}
M\cap W &= \left\{(\boldsymbol{z}, \boldsymbol{f}(\boldsymbol{z})): z\in U\right\}
\end{split}
\end{equation}

:::

### Identifying manifolds, and manifolds under smooth transformations.

::: {.theorem #manifolds-as-eqns name="Using the implicit function theorem to identify manifolds"}

Here's how to show that a set represented as a locus is a smooth manifold:

1. Let $U\subset \mathbb{R}^n$ be open, $\boldsymbol{F}:U\to\mathbb{R}^{n - k}$ be a $\mathscr{C}^1$ mapping. Let $M$ be a subset of $\mathbb{R}^n$ such that 
    $$
        \begin{aligned}
           M\cap U  &= \left\{\boldsymbol{z}\in U | \boldsymbol{F}(\boldsymbol{z}) = \boldsymbol{0}\right\}
        \end{aligned}
    $$
    If $D\boldsymbol{F}(\boldsymbol{z})$ is onto for every $\boldsymbol{z}\in M\cap U$, then $M\cap U$ is a smooth
    $k$-dimensional manifold embedded in $\mathbb{R}^n$. If every $z\in M$ is in such
    a $U$, then $M$ is a $k$-dimensional manifold. 

2. Conversely, if $M$ is a smooth $k$-dimensional manifold embedded in $\mathbb{R}^n$,
    then every point $\boldsymbol{z}\in M$ has a neighborhood $U\subset\mathbb{R}^n$ such that
    there exists a $\mathscr{C}^1$ mapping $\boldsymbol{F}:U\to\mathbb{R}^{n - k}$ with 
    $D\boldsymbol{F}(\boldsymbol{z})$ onto and $M\cap U = \left\{\boldsymbol{y}|\boldsymbol{F}(\boldsymbol{y}) = 0\right\}$. 

:::

::: {.remark}

The first part of the above theorem follows from the **Implicit Function Theorem**
\@ref(thm:implicit-function-theorem). For the second part, it is enough to consider 
the map

\begin{equation}
\begin{split}
    (\boldsymbol{x}, \boldsymbol{y}) \mapsto \boldsymbol{y} - \boldsymbol{f}(\boldsymbol{x}) 
\end{split}
\end{equation}

where the map $\boldsymbol{f}$ is considered right from the definition of a 
smooth manifold \@ref(def:manifold).

:::

The above theorem makes it easy to identify/prove that a certain object is a smooth manifold, without
knowing the overall geometry of the object. The next theorem shows that the inverse image of a manifold
under a sufficiently nice transformation still results in a smooth manifold.

::: {.theorem #inverse-image-of-manifold}

Let $M\subset \mathbb{R}^m$ be a $k$-dimensional manifold, $U$ an open 
subset of $\mathbb{R}^n$, and $\boldsymbol{f}:U\to \mathbb{R}^m$ a $\mathscr{C}^1$ 
mapping whose derivative $D\boldsymbol{f}(\boldsymbol{x})$ is surjective at every 
point $\boldsymbol{x}\in \boldsymbol{f}^{-1}(M)$. Then the inverse image 
$\boldsymbol{f}^{-1}(M)$ is a submanifold of $\mathbb{R}^n$ of 
dimension $k + n - m$.

:::

::: {.remark name="Manifolds under rotations and translations are still manifolds"}

As a corollary of Theorem \@ref(thm:inverse-image-of-manifold), it's easy to show that for a 
mapping of the form $\boldsymbol{x}\mapsto A\boldsymbol{x} + \boldsymbol{c}$ with $A$ invertible, 
the image of a manifold is still a manifold.

:::

### Parametrizations of manifolds

Another nice way of representing manifolds is via their *parametrizations*.  

::: {.definition #parametrization-def name="Parametrization of a manifold"}

A ***parametrization*** of a $k$-dimensional manifold $M\subset\mathbb{R}^n$ is
a mapping $\gamma:U\subset \mathbb{R}^k\to M$ satisfying the following conditions:

1. $U$ is open.
2. $\gamma$ is $\mathscr{C}^1$, one-to-one, and onto on $M$.
3. $D\gamma(\boldsymbol{u})$ is one-to-one for every $\boldsymbol{u}\in U$. 

:::

::: {.theorem name="Existence of local parametrizations"}

Let $M\subset\mathbb{R}^m$ be a $k$-dimensional manifold. For every point 
$\boldsymbol{p}\in M$, there is an open neighborhood $U\subset\mathbb{R}^m$ of 
$\boldsymbol{p}$ such that $M\cap U$ admits a parametrization 
$\gamma:\mathbb{R}^k\to M\cap U$.

:::

::: {.proof} 

Without loss of generality, write 
$\boldsymbol{p} = (\boldsymbol{x}, \boldsymbol{y})\in\mathbb{R}^k\times\mathbb{R}^{n - k}$ 
where the first $k$ variables are indepdendent. Let $W\subset\mathbb{R}^k$ be an open 
neighborhood of $\boldsymbol{x}$ such that 

$$
    \begin{aligned}
        M\cap U &= \left\{\boldsymbol{z}, \boldsymbol{f}(\boldsymbol{z}): \boldsymbol{z}\in W\right\} 
    \end{aligned}
$$

Now, define the mapping $\gamma:W\to M\cap U$ by 

$$
    \begin{aligned}
         \boldsymbol{z}\mapsto (\boldsymbol{z}, \boldsymbol{f}(\boldsymbol{z}))
    \end{aligned}
$$

Clearly, $\gamma$ is one-one and onto. Moreover, observe that

$$
    \begin{aligned}
         D\gamma(\boldsymbol{z}) &=
            \begin{bmatrix}
                I \\ D\boldsymbol{f}(\boldsymbol{z})
            \end{bmatrix}
    \end{aligned}
$$
Clearly, the first $k$ rows of the Jacobian are linearly independent; hence, the matrix 
is invertible. So, $\gamma$ is indeed a parametrization of $M\cap U$.

:::


## Tangent spaces

::: {.definition #tangent-spaces name="Tangent spaces"}

Let $M\subset \mathbb{R}^n$ be a $k$-dimensional manifold given by the equation
$\boldsymbol{y} = \boldsymbol{f}(\boldsymbol{x})$. The *tangent space* to $M$ at 
$\boldsymbol{z}_0 := (\boldsymbol{x}_0, \boldsymbol{y}_0)$, 
denoted by $T_{\boldsymbol{z}_0}M$, is the graph of the linear map 
$D\boldsymbol{f}(\boldsymbol{x}_0)$.

:::

### Computing tangent spaces

We can use the machinery built by the implicit function theorem and parametrizations
to easily compute tangent spaces. Without proof, here are two theorems (you can check
the book for proofs) which essentially tell us what the tangent spaces really are.

::: {.theorem}

Suppose $\boldsymbol{F}(\boldsymbol{z}) = \boldsymbol{0}$ describes a manifold $M$, and 
$D\boldsymbol{F}(\boldsymbol{z}_0)$ is onto for some $\boldsymbol{z}_0\in M$. Then

\begin{equation}
\begin{split}
     T_{\boldsymbol{z}_0}M &= \text{ker}\,D\boldsymbol{F}(\boldsymbol{z}_0) 
\end{split}
\end{equation}

:::

::: {.theorem} 

Let $U\subset \mathbb{R}^k$ be an open set and let $\gamma:U\to\mathbb{R}^n$ be 
a parametrization of a manifold $M$. Then

$$
    \begin{aligned}
        T{\gamma(\boldsymbol{u})}M = \text{img}\,D\gamma(\boldsymbol{u}) 
    \end{aligned}
$$


:::

## Differentiable maps on manifolds

::: {.definition #differentiable-map-manifold name="Smooth maps on manifolds"}

Let $M\subset\mathbb{R}^n$ be an $m$-dimensional manifold, and let 
$\boldsymbol{f}:M\to\mathbb{R}^k$ be a map. Then $\boldsymbol{f}$ is of class 
$\mathscr{C}^p$ if every $\boldsymbol{x}\in M$ has a neighborhood $U\subset \mathbb{R}^n$
such that there exists a $\mathscr{C}^p$ map $\boldsymbol{\tilde{f}}:U\to\mathbb{R}^k$
with $\boldsymbol{\tilde{f}}|_{U\cap M} = \boldsymbol{f}|_{U\cap M}$. Moreover, 
for $p\ge 1$, we define the ***derivative*** of $\boldsymbol{f}$ to be

\begin{equation}
\begin{split}
    D\boldsymbol{f}(\boldsymbol{x}): T_{\boldsymbol{x}}M\to \mathbb{R}^k := D\boldsymbol{\tilde{f}}|_{T_{\boldsymbol{x}}M} 
\end{split}
\end{equation}

:::

::: {.theorem name="Choice of extension doesn't matter"}

In Definition \@ref(def:differentiable-map-manifold), any choice of the extension
map $\boldsymbol{\tilde{f}}$ results in the same derivative. 

:::

::: {.theorem #chain-rule name="Chain rule for maps on manifolds."}

Let $M\subset\mathbb{R}^n$ be a manifold, and let $\boldsymbol{f}:M\to\mathbb{R}^k$ be
a $\mathscr{C}^1$ map. Let $U\subset\mathbb{R}^l$ be open and let 
$\boldsymbol{g}:U\to M$ be a $\mathscr{C}^1$ map. Then,

\begin{equation}
\begin{split}
    D(\boldsymbol{f}\circ\boldsymbol{g})(\boldsymbol{x}) &= D\boldsymbol{f}(\boldsymbol{g}(\boldsymbol{x}))D\boldsymbol{g}(\boldsymbol{x})\quad \text{for all }\boldsymbol{x}\in U
\end{split}
\end{equation}

:::

## Taylor polynomials in multiple variables

### Taylor's theorem in one variable

::: {.theorem #taylor-thm-one-variable name="Taylor's theorem without remainder in one variable"}

Let $U\subseteq\mathbb{R}$ be an open subset and let $f:U\to\mathbb{R}$ be $k$-times 
continuously differentiable on $U$. Then, for any $a\in U$, the polynomial

\begin{equation}
\begin{split}
    p_{f, a}^k(a + h) := \sum_{i = 0}^k \frac{f^{(i)}(a)}{i!}h^i
\end{split}
\end{equation}

is the unique polynomial of degree $\le k$ such that

\begin{equation}
\begin{split}
    \lim_{h\to 0} \frac{f(a + h) - p_{f, a}^k(a + h)}{h^k} = 0 
\end{split}
\end{equation}

In other words, $p_{f, a}^k$ is the best local approximation to $f$ at the point $a$
by a polynomial of degree atmost $k$.

:::

### Multi-exponents

To represent Taylor polynomials in higher dimensions, the *multi-exponent notation*
comes in handy. Formally, a *multi-exponent* $I$ is just an ordered finite list of 
non-negative whole numbers:

\begin{equation}
\begin{split}
    I := (i_1, \cdots, i_n) 
\end{split}
\end{equation}

The *total degree* of $I$ is the sum $\sum_{k = 1}^N i_k$, and the *factorial* of $I$
is the product $\prod_{k = 1}^n i_k!$. For a multi-exponent $I$, the notation
$\boldsymbol{x}^I := x_1^{i_1}\cdots x_n^{i_n}$, i.e a monomial on $\mathbb{R}^n$. So,
any polynomial $p$ of degree $m$ in $n$ variables can be written using the notation

\begin{equation}
\begin{split}
     p(\boldsymbol{x}) &= \sum_{k = 0}^m\sum_{I}a_I\boldsymbol{x}^I
\end{split}
\end{equation}

We use the notation $\mathcal{I}^m_n$ to denote the set of all multi-exponents of total
degree $m$ in $n$ variables.

### Equality of mixed partials

Mutli-exponents can be used to nicely state a theorem about equality of mixed
partials of multivariate function. For a function $f:U\to \mathbb{R}$ with 
$U\subset \mathbb{R}^n$ being open and a multi-exponent $I$, we define

\begin{equation}
\begin{split}
    D_If := D_1^{i_1}D_2^{i_2}\cdots D_n^{i_n}f 
\end{split}
\end{equation}

The use of multi-exponents in mixed partials can be justified by the following theorem on equality of crossed partials:

::: {.theorem #equality-of-mixed-partials name="Equality of mixed partials"}

Let $U$ be an open subset of $\mathbb{R}^n$, and $f:U\to\mathbb{R}$ a function
such that all first partial derivatives $D_if$ are differentiable at $\boldsymbol{a}\in U$.
Then, for any $i, j\in[n]$, we have

\begin{equation}
\begin{split}
    D_j(D_if)(\boldsymbol{a}) &= D_i(D_jf)(\boldsymbol{a})
\end{split}
\end{equation}

:::

::: {.corollary #kth-order-mixed-partials}

If $f:U\to\mathbb{R}$ is a function, all of whose partial derivatives
upto order $k$ are continuous, then the partial derivatives of order upto 
$k$ do not depend on the order in which they are computed.

:::

A special case of this theorem is to compute derivatives of a polynomial,
which can be succintly written using multi-exponents:

::: {.proposition #polynomial-coefficients name="Coefficients expressed in terms of partial derivatives at 0"}

Let $p$ be a polynomial

\begin{equation}
\begin{split}
    p(\boldsymbol{x}) := \sum_{m = 0}^k\sum_{J} a_J\boldsymbol{x}^J
\end{split}
\end{equation}

:::

with $\boldsymbol{x}$ a monomial with $n$ variables. Then, for any multi-exponent
$I$, we have

\begin{equation}
\begin{split}
     I!a_I &= D_Ip(\boldsymbol{0})
\end{split}
\end{equation}

### Taylor polynomials in higher dimensions

::: {.definition #taylor-polynomials-higher-dimensions name="Taylor polynomial in higher dimensions"}

Let $U\subset\mathbb{R}^n$ be an open subset and let $f:U\to\mathbb{R}$ be a 
$\mathscr{C}^k$ function. Then the polynomial of degree $k$

\begin{equation}
\begin{split}
    P_{f, \boldsymbol{a}}^k(\boldsymbol{a} + \boldsymbol{h}) := \sum_{m = 0}^k\sum_{I} \frac{D_If(\boldsymbol{a})\boldsymbol{h}^I}{I!} 
\end{split}
\end{equation}

is called the ***Taylor polynomial*** of degree $k$ of $f$ at $\boldsymbol{a}$. If
$\boldsymbol{f}:U\to\mathbb{R}^n$ is a $\mathscr{C}^k$ function, its Taylor polynomial
is the polynomial map $U\to\mathbb{R}^n$ whose coordinate functions are the Taylor 
polynomials of the coordinate functions of $\boldsymbol{f}$.

:::

::: {.theorem #taylors-theorem-higher-dimensions name="Taylor's theorem without remainder in higher dimensions"}

Let $U\subseteq\mathbb{R}^n$ be open, $\boldsymbol{a}\in U$ be a point, and 
$f:U\to\mathbb{R}$ be a $\mathscr{C}^k$ function.

1. The polynomial $P_{f, \boldsymbol{a}}^k(\boldsymbol{a} + \boldsymbol{h})$ is
    the unique polynomial of degree $k$ with the same partial derivatives up
    to order $k$ at $\boldsymbol{a}$ as $f$.

2. It best approximates $f$ near $\boldsymbol{a}$: it is the unique polynomial 
    of degree atmost $k$ such that

    \begin{equation}
    \begin{split}
       \lim_{\boldsymbol{h}\to \boldsymbol{0}} \frac{f(\boldsymbol{a} + \boldsymbol{h}) - P^k_{f, \boldsymbol{a}}(\boldsymbol{a} + \boldsymbol{h})}{\lVert h\rVert^k} = 0
    \end{split}
    \end{equation}

:::

## Quadratic Forms

::: {.definition #quadratic-forms name="Quadratic Forms"}

A ***quadratic form*** $Q:\mathbb{R}^n\to\mathbb{R}$ is a polynomial all of whose terms are
of degree $2$.

:::

### Quadratic forms as sums of squares

::: {.theorem}

1. For any quadratic form $Q:\mathbb{R}^n\to\mathbb{R}$, there exist $m = k + l$ linearly independent linear functions $\alpha_1,\cdots,\alpha_m:\mathbb{R}^n\to\mathbb{R}$
    such that
    $$
        \begin{aligned}
            Q(\boldsymbol{x}) &= (\alpha_1(\boldsymbol{x}))^2 + \cdots + (\alpha_k(\boldsymbol{x}))^2 - (\alpha_{k + 1}(\boldsymbol{x}))^2 - \cdots + (\alpha_{k + l}(\boldsymbol{x}))^2
        \end{aligned}
    $$

2. The number $k$ of plus signs and the number $l$ of minus signs only depend on $Q$. The pair $(k, l)$ is called the ***signature*** of the quadratic form $Q$.
:::

::: {.definition name="Positive and negative definite forms"}

A quadratic form $Q$ is said to be ***positive definite*** if $Q(\boldsymbol{x}) > 0$ for $\boldsymbol{x}\ne \boldsymbol{0}$, and ***negative definite*** if $Q(\boldsymbol{x}) < 0$
whenever $\boldsymbol{x} \ne \boldsymbol{0}$.

:::

::: {.proposition}

Suppose $Q(\boldsymbol{x}) = (\alpha_1(\boldsymbol{x}))^2 + \cdots + (\alpha_k(\boldsymbol{x}))^2 - (\alpha_{k + 1}(\boldsymbol{x}))^2 - \cdots + (\alpha_{k + l}(\boldsymbol{x}))^2$
where the $\alpha_i$'s are linearly independent linear functions from $\mathbb{R}^n\to\mathbb{R}$. Then, $k$ is the largest dimension of a subspace of $\mathbb{R}^n$ on which $Q$
is positive definite, and $l$ is the largest dimension of a subspace of $\mathbb{R}^n$ on which $Q$ is negative definite.

:::

::: {.corollary}

If $Q:\mathbb{R}^n\to\mathbb{R}$ is a quadratic form and $A:\mathbb{R}^n\to\mathbb{R}$ is invertible, then $Q\circ A$ is a quadratic form with the same signature as $Q$.

:::

### Classification of quadratic forms

::: {.definition name="Rank of a quadratic form"}

The ***rank*** of a quadratic form is the number of linearly independent squares that appear when the quadratic form
is represented as a sum of linearly independent squares. Equivalently, if $(k, l)$ is the signature of the form, 
it's ***rank*** is the sum $k + l$.

:::

::: {.definition name="Degeneracy of forms"}

A quadratic form on $\mathbb{R}^n$ is ***non-degenerate*** if it's rank is $n$. Otherwise, it's said to be
***degenerate***.

:::

::: {.proposition}

If $Q:\mathbb{R}^n\to\mathbb{R}$ is a positive definite quadratic form, then there exists a constant $C > 0$ such that

$$
    \begin{aligned}
        Q(\boldsymbol{x}) \ge C\lVert\boldsymbol{x}\rVert^2 
    \end{aligned}
$$

for all $\boldsymbol{x}\in\mathbb{R}^n$.

:::

### Quadratic forms as symmetric matrices

::: {.theorem}

The mapping $A\mapsto Q_A$, where $Q_A(\boldsymbol{x}) = \boldsymbol{x}^TA \boldsymbol{x}$, is a bijective map from the space of
$n\times n$ symmetric matrices to the space of quadratic forms on $\mathbb{R}^n$.

:::

## Classifying critical points of a function

Here's a well-known theorem used for classifying the critical points of a single-variable function:

::: {.theorem #extremaOneVariable name="Extrema of functions of one variable"}

Let $U\subset\mathbb{R}$ be an open set and let $f:U\to\mathbb{R}$ be a differentiable function.

1. If $x_0\in U$ is a local minimum or a local maximum of $f$, then $f'(x_0) = 0$.
2. If $f$ is twice differentiable, and $f'(x_0) = 0$ and $f''(x_0) > 0$, then $x_0$ is a strict local minimum of $f$.
3. If $f$ is twice differentiable, and $f'(x_0) = 0$ and $f''(x_0) < 0$, then $x_0$ is a strict local maximum of $f$.

:::

We'll now see some machinery for higher-dimensional functions.

::: {.theorem  name="Critical points and critical values."}

Let $U\subseteq \mathbb{R}^n$ be open and let $f:U\to\mathbb{R}$ be a differentiable function. A ***critical point***
of $f$ is a point where the derivative vanishes. The value of $f$ at a critical points is a ***critical value***.

:::

A simple consequence of Theorem \@ref(thm:extremaOneVariable) is the following high-dimensional version:

::: {.theorem}

Let $U\subset\mathbb{R}^n$ be an open subset and let $f:U\to\mathbb{R}$ be a differentiable function. If $\boldsymbol{x}_0\in U$
is a local minimum or local maximum of $f$, then $Df(\boldsymbol{x}_0) = \boldsymbol{0}$.

:::

### Second derivative conditions

Just like in one dimension, we usually look at the second derivative of a function to classify a point as being a local maximum or a local
minimum. The situation for higher dimensions is similar, where instead we look at the quadratic form consisting of the even degree terms
of the Taylor polynomial.

::: {.theorem  name="Signature of a critical point"}

Let $U\subset \mathbb{R}^n$ be an open set, and let $f:U\to\mathbb{R}$ be of class
\mathscr{C}^2. Let $\boldsymbol{a}\in U$ be a critical point of $f$. The ***signature***
of the critical point $\boldsymbol{a}$ is the signature of the quadratic form

$$
    \begin{aligned}
Q_{f, \boldsymbol{a}}(\boldsymbol{h}) := \sum_{I\in\mathcal{I}^2_n} \frac{1}{I!}(D_If(\boldsymbol{a}))\boldsymbol{h}^I
    \end{aligned}
$$

:::

It turns out that the symmetric matrix associated with the quadratic 
form $Q_{f, \boldsymbol{a}}$ is the Hessian of $f$, i.e

$$
    \begin{aligned}
        Q_{f, \boldsymbol{a}}(\boldsymbol{h}) &= \frac{1}{2}(\boldsymbol{h}^TH \boldsymbol{h})
    \end{aligned}
$$

where $H$ is the Hessian of $f$ at $\boldsymbol{a}$.

::: {.definition}

A critical point $\boldsymbol{a}$ of $f$ is said to be ***degenerate*** or 
***non-degenerate*** precisely when the quadratic form $Q_{f, \boldsymbol{a}}$ 
is degenerate or non-degenerate.

:::

::: {.theorem name="Quadratic forms and extrema"}

Let $U\subset\mathbb{R}^n$ be an open set, $f:U\to\mathbb{R}$ be of class $\mathscr{C}^2$,
and let $\boldsymbol{a}\in U$ be a critical point of $f$.

1. If the signature of $\boldsymbol{a}$ is $(n, 0)$, i.e if $Q_{f, \boldsymbol{a}}$ is 
    positive definite, then $\boldsymbol{a}$ is a strict local minimum of $f$. If the 
    signature of $\boldsymbol{a}$ is $(k, l)$ with $l > 0$, then $\boldsymbol{a}$ is not
    a local minimum.

2. An analogous result holds for when the quadratic form is negative definite, with local
    minimum replaced by local maximum.

:::


::: {.definition name="Saddle points"}

If $\boldsymbol{a}$ is a critical point of a $\mathscr{C}^2$ function $f$, and the
quadratic form $Q_{f, \boldsymbol{a}}$ has signature $(k, l)$ with $k, l > 0$, then
$\boldsymbol{a}$ is said to be a ***saddle*** point.

:::

::: {.theorem name="Behavior of a function near a saddle"}

Let $U\subset\mathbb{R}^n$ be an open set, and let $f:U\to\mathbb{R}$ be a 
$\mathscr{C}^2$ function. If $f$ has a saddle at $\boldsymbol{a}\in U$, then in
every neighborhood of $\boldsymbol{a}$ there are points $\boldsymbol{b}$ and 
$\boldsymbol{c}$ with $f(\boldsymbol{b}) > f(\boldsymbol{a})$ and 
$f(\boldsymbol{c}) < f(\boldsymbol{a})$.

:::

## Constrainted critical points and Lagrange Multipliers

::: {.definition name="Critical point of functions defined on manifolds"}

Let $X\subset \mathbb{R}^n$ be a manifold, and let $f:X\to\mathbb{R}$ be a
$\mathscr{C}^1$ function. A ***critical point*** of $f$ is a point $\boldsymbol{x}\in X$
where $Df(\boldsymbol{x}) = \boldsymbol{0}$. Such points are also called 
***constrained critical points***.

:::

Analogous to the unconstrained optimization problem, we have the following theorem:

::: {.theorem}

Let $X\subset\mathbb{R}^n$ be a manifold, $f:X\to\mathbb{R}$ a $\mathscr{C}^1$ function,
and $\boldsymbol{c}\in X$ a local extremum of $f$. Then $\boldsymbol{c}$ is a constrained
critical point of $f$.

:::

### Lagrange Multipliers

Suppose we're given a differentiable function on an open set, and we want to optimize it
on a subset which is *constrained* by another function. So, we have an *objective* function
and a *constrain* function as part of the problem data. In such a scenario, it turns out
that computing critical points amounts to calculating a bunch of scalars called 
*Lagrange Multipliers*. Here's the formal statement of the fact:

::: {.theorem  name="Lagrange Multipliers"}

Let $U\subset\mathbb{R}^n$ be open, and let $\boldsymbol{F}:U\to\mathbb{R}^m$ be a 
$\mathscr{C}^1$ mapping defining a manifold $X$, with $D\boldsymbol{F}(\boldsymbol{x})$
onto for every $\boldsymbol{x}\in X$. Let $f:U\to\mathbb{R}$ be a $\mathscr{C}^1$
mapping. Then $\boldsymbol{a}\in X$ is a critical point of $f|_{X}$ if and only if
there exist numbers $\lambda_1, ..., \lambda_m$ such that

$$
    \begin{aligned}
        Df(\boldsymbol{a}) &= \sum_{i = 1}^m \lambda_i DF_i(\boldsymbol{a}) 
    \end{aligned}
$$

The numbers $(\lambda_i)_{i = 1}^m$ are called ***Lagrange Multipliers***.

:::

## Integration

I'll try to cover all fundamental definitions of the integral right from the basics.
I feel this is important since different sources might treat the integral in different
ways, so it's better to have one coherent structure and go with it.

### Preliminary definitions

::: {.definition name="Support of a function"}

Let $f:\mathbb{R}^n\to\mathbb{R}$. Then the ***support*** of $f$ is defined as

$$
    \begin{aligned}
        \text{Supp}(f) := \overline{\left\{\boldsymbol{x}\in\mathbb{R}^n: f(\boldsymbol{x})\ne 0\right\}} 
    \end{aligned}
$$

where the bar denotes the ***closure***.

:::

::: {.definition name="Oscillations"}

The ***oscillation*** of a function $f$ over a set $A$, denoted by $\text{osc}_A(f)$,
is defined as

$$
    \begin{aligned}
        \text{osc}_A(f) := M_A(f) - m_A(f)
    \end{aligned}
$$

where $M_A(f) := \sup_{\boldsymbol{x}\in A}f(\boldsymbol{x})$, and $m_A(f)$ is
similarly defined with the supremum replaced by the infimum.

:::

When $|f|$ is bounded and $f$ has bounded support, all of the above quantities are 
well-defined.

### Dyadic pavings

With the oscillation now defined, we'll define the so called ***dyadic pavings*** of
$\mathbb{R}^n$; this is just for convenience. Infact we can use any kind of paving
to define the integral.

::: {.definition name="Dyadic cube"}

A ***dyadic cube*** $C_{\boldsymbol{k}, N}\subset \mathbb{R}^n$ is defined as

$$
    \begin{aligned}
C_{\boldsymbol{k}, N} := \left\{\boldsymbol{x}\in\mathbb{R}^n: \frac{k_i}{2^N}\le x_i\le \frac{k_i + 1}{2^N} \text{ for }1\le i\le n\right\} 
    \end{aligned}
$$

where $\boldsymbol{k}\in\mathbb{Z}^n$ is a vector of integers. The collection of all such
cubes forms the $N$th ***dyadic paving*** of $\mathbb{R}^n$. The ***volume*** of such a 
cube is simply defined to be

$$
    \begin{aligned}
\text{vol}_n(C) := \frac{1}{2^{Nn}} 
    \end{aligned}
$$

Having defined this, the usual Riemann integral of a function is then defined to be 
the limit of upper/lower Riemann sums with respect to the dyadic paving. Note that,
as part of the definition of Riemann integrability, we require the function in question
to be both bounded and have bounded support. The ***Lebesgue integral*** is more suited
to handle more general functions.

:::

### Two useful integral rules 

Here are two rules of integration which are quite useful in some proofs.

::: {.proposition}

1. A bounded function $f$ with bounded support is integrable if and only if both $f^+$
and $f^-$ are integrable.

2. If $f$ and $g$ are integrable, then so are $\sup(f, g)$ and $\inf(f, g)$.

3. If $f_1:\mathbb{R}^n\to\mathbb{R}$ and $f_2:\mathbb{R}^m\to\mathbb{R}$ are integrable,
    then the function

    $$
        \begin{aligned}
    g(\boldsymbol{x}, \boldsymbol{y}) := f_1(\boldsymbol{x})f_2(\boldsymbol{y}) 
        \end{aligned}
    $$

    on $\mathbb{R}^{n + m}$ is integrable, and

    $$
        \begin{aligned}
           \int_{\mathbb{R}^{n + m}}g|d^n \boldsymbol{x}||d^m \boldsymbol{y}| &= 
            \left(\int_{\mathbb{R}^n} f_1 |d^n \boldsymbol{x}|\right)\left(\int_{\mathbb{R}^m}f_2 |d^m \boldsymbol{y}|\right) 
        \end{aligned}
    $$

:::

### Volumes

::: {.definition name="Higher dimensional volume"}

For any set $A\subset\mathbb{R}^n$, we define the ***volume*** of $A$ as 

$$
    \begin{aligned}
        \text{vol}_n(A) := \int_{\mathbb{R}^n}\boldsymbol{1}_A|d^n \boldsymbol{x}| 
    \end{aligned}
$$

whenever the indicator function is integrable. If the integral is well-defined, then $A$
is said to be ***pavable***.

:::

::: {.proposition name="Volume is invariant under translation"}

Let $A$ be any pavable subset of $\mathbb{R}^n$ and $\boldsymbol{v}\in \mathbb{R}^n$
by any vector. Then, $A + \boldsymbol{v}$ is pavable, and

$$
    \begin{aligned}
       \text{vol}_n(A + \boldsymbol{v})  &= \text{vol}_n(A) 
    \end{aligned}
$$

:::

::: {.proposition name="Set with volume 0"}

A bounded set $X\subset \mathbb{R}^n$ has volume $0$ if and only if for every
$\epsilon > 0$ there exists some $N$ such that

$$
    \begin{aligned}
       \sum_{C\in\mathcal{D}_N(\mathbb{R}^n)\\ C\cap X\ne \phi} \text{vol}_n(C) \le \epsilon 
    \end{aligned}
$$

where above $\mathcal{D}_N$ is the set of all dyadic cubes at depth $N$.

:::

Motivated by the above proposition, we can also define the notion of ***$k$-dimensional 
volume*** $0$ in $\mathbb{R}^n$, where $k < n$.

::: {.definition name="Lower dimensional volume 0"}

1. A bounded subset $X\subset \mathbb{R}^n$ has $k$-dimensional volume $0$ if

    $$
        \begin{aligned}
             \lim_{N\to\infty} \sum_{C\in\mathcal{D}_N(\mathbb{R}^n)\\C\cap X\ne \phi} \left(\frac{1}{2^N}\right)^k&= 0 
        \end{aligned}
    $$

2. An arbitrary subset $X\subset\mathbb{R}^n$ has $k$-dimensional volume $0$ if for all
    $R > 0$, the bounded set $X\cap B_R(\boldsymbol{0})$ has $k$-dimensional volume $0$.

:::

We have the following useful theorem, which is intuitive, but very useful.

::: {.theorem #manifold-volumes name="Lower dimensional volumes of manifolds"}

If integers $m, k, n$ satisfy $0\le m < k \le n$ and $M\subset \mathbb{R}^n$ is a
manifold of dimension $m$, any closed subset $X\subset M$ has $k$-dimensional
volume $0$.

:::

::: {.proposition name="Scaling volume"}

If $A\subset\mathbb{R}^n$ has volume and $t\in\mathbb{R}$, then the set 
$tA$ has volume, and 

$$
    \begin{aligned}
        \text{vol}_n(tA) &= |t|^n\text{vol}_n(A) 
    \end{aligned}
$$

:::

### What functions can be integrated?

::: {.theorem name="Criterion for integrability"}

A function $f:\mathbb{R}^n\to\mathbb{R}$ is integrable if and only if it is bounded
with bounded support, and for all $\epsilon > 0$, there exists $N$ such that

$$
    \begin{aligned}
\sum_{C\in\mathcal{D}_N : \text{osc}_C(f) > \epsilon} \text{vol}_n(C) < \epsilon
    \end{aligned}
$$

where above, $\mathcal{D}_N$ is the set of all dyadic cubes at depth $N$.

:::

As a consequence of this criterion for integrability of functions, the following facts 
about volumes can be proven:

::: {.proposition name="Bounded part of graph of integrable function has volume 0"}

Let $f:\mathbb{R}^n\to\mathbb{R}$ be an integrable function with graph $\Gamma(f)$, and 
let $C_0\in\mathbb{R}^n$ be any dyadic cube. Then

$$
    \begin{aligned}
         \text{vol}_{n + 1}(\Gamma(f)\cap (C_0\times \mathbb{R})) &= 0
    \end{aligned}
$$

:::

::: {.theorem}

Any continuous function $\mathbb{R}^n\to\mathbb{R}$ with bounded support is integrable.

:::

::: {.corollary}

Let $X\subset \mathbb{R}^n$ be compact and let $f:X\to\mathbb{R}$ be continuous. Then
the graph $\Gamma_f\subset \mathbb{R}^{n + 1}$ has volume $0$.

:::

::: {.theorem}

A function $f:\mathbb{R}^n\to\mathbb{R}$, bounded with bounded support, is integrable 
if it is continuous except on a set of volume $0$.

:::

::: {.corollary}

Let $f:\mathbb{R}^n\to\mathbb{R}$ be integrable, and let $g:\mathbb{R}^n\to\mathbb{R}$
be a bounded function. If $f = g$ except on a set of volume $0$, then $g$ is integrable
and

$$
    \begin{aligned}
        \int_{\mathbb{R}^n}f|d^n \boldsymbol{x}| &= \int_{\mathbb{R}^n}g|d^n \boldsymbol{x}|
    \end{aligned}
$$

:::

### Measure zero

The notion of *measure zero* is more important than the notion of *volume zero* from a 
theoretical viewpoint, since it allows a nice characterization of integrable functions.
In the following definition, a ***box*** in $\mathbb{R}^n$ of sidelength $\delta > 0$
is a cube of the form

$$
\begin{aligned}
\left\{\boldsymbol{x}\in\mathbb{R}^n: a_i < x_i < a_i + \delta, i = 1, \cdots, n\right\} 
    \end{aligned}
$$

::: {.definition name="Measure 0"}

A set $X\subset \mathbb{R}^n$ has ***measure*** $0$ if for every $\epsilon > 0$,
there exists a countable sequence of open boxes $B_i$ such that

$$
    \begin{aligned}
        X\subset \bigcup B_i \text{ and } \sum \text{vol}_n (B_i) \le \epsilon
    \end{aligned}
$$

:::

::: {.theorem}

A countable union of sets of measure $0$ has measure $0$.

:::

::: {.corollary}

Let $B_R(\boldsymbol{0})$ be the ball of radius $R$ centered at the origin. If
for all $R\ge 0$ the subset $X\subset \mathbb{R}^n$ satisfies 
$\text{vol}_n(X\cap B_R(\boldsymbol{0})) = 0$, then $X$ has measure $0$.

:::

::: {.proposition}

Let $X$ be a subspace of $\mathbb{R}^n$ of dimension $k < n$. Then $X$ has measure $0$,
and any translate of $X$ has measure $0$.

:::

::: {.proof}

Clearly, $X$ is a $k$-dimensional manifold. Hence, by Theorem \@ref(thm:manifold-volumes),
the $n$-dimensional volume of $X\cap B_R(\boldsymbol{0})$ is $0$ for all $R\ge 0$, and
applying the previous corollary the claim follows.

:::

### Integrability of almost continuous functions

::: {.theorem name="What functions are integrable"}

Let $f:\mathbb{R}^n\to\mathbb{R}$ be bounded with bounded support. Then $f$ is integrable
if and only if it is continuous except on a set of measure $0$.

:::

As a result of this theorem, we can also prove the following intuitive fact, but often
very useful in computations.

::: {.theorem name="Boundaries have volume zero"}

Let $A\subset\mathbb{R}^n$ be a set such that $\partial A$ has a well-defined 
$n$-dimensional volume. Then, $\text{vol}_n(\partial A) = 0$, i.e the volume must
be zero.

:::

::: {.proof}

By definition, since $\partial A$ has a volume, the indicator 
$\boldsymbol{1}_{\partial A}$ is integrable, i.e it is continuous almost everywhere.
If $S$ is the set of points where $\boldsymbol{1}_{\partial A}$ is continous, we claim
that

$$
    \begin{aligned}
        S\subset \left\{\boldsymbol{x} : \boldsymbol{x}\notin \partial A\right\} 
    \end{aligned}
$$

But this is obvious: note that if $\boldsymbol{1}_{\partial A}$ is continuous at some 
$\boldsymbol{z}$, then for a sufficiently small open ball around $\boldsymbol{z}$, all
$\boldsymbol{y}$ in the ball will have 
$|\boldsymbol{1}_{\partial A}(\boldsymbol{z}) - \boldsymbol{1}_{\partial A}(\boldsymbol{y})|$ 
to be arbitrarily small, which will further imply that 
$\boldsymbol{1}_{\partial A}(\boldsymbol{z}) = \boldsymbol{1}_{\partial A}(\boldsymbol{y})$ 
for all $\boldsymbol{y}$ in that open ball. Clearly, this must mean that 
$\boldsymbol{z}\notin \partial A$, since for every neighborhood of a point in the boundary
of a set, we can find points in the set and points not in the set. This proves the claim.

Taking compliments, the claim implies that $\partial A\subset S^c$, where $S^c$ is the
set of discontinuities. Since $S^c$ has measure zero (integrability), it follows that
$\partial A$ also has measure $0$. Since $\text{vol}_n(\partial A)$ is well defined,
this proves that $\text{vol}_n(\partial A) = 0$.

:::

### Fubini's Theorem and Iterated Integrals

This is one of the most important tools to compute higher dimensional integrals.
Essentially, ***Fubini's Theorem*** reduces the problem of computing higher-dimensional
integrals to the problem of single variable integrals.

::: {.theorem #fubini name="Fubini's Theorem"}

Let $f:\mathbb{R}^n\times \mathbb{R}^m\to\mathbb{R}$ be an integrable function.
For a fixed $\boldsymbol{x}$, let the function 
$f_{\boldsymbol{x}}:\mathbb{R}^m\to\mathbb{R}$ be defined by 
$\boldsymbol{y}\mapsto f(\boldsymbol{x}, \boldsymbol{y})$, and let 
$f^{\boldsymbol{y}}:\mathbb{R}^n\to\mathbb{R}$ be similarly defined. Then,
the functions $U(f_{\boldsymbol{x}})$, $L(f_{\boldsymbol{x}})$, 
$U(f^{\boldsymbol{y}})$, $L(f^{\boldsymbol{y}})$ are all integrable, and

$$
    \begin{aligned}
       \int_{\mathbb{R}^n} U(f_{\boldsymbol{x}}) |d^n \boldsymbol{x}| &= \int_{\mathbb{R}^n} L(f_{\boldsymbol{x}})|d^n \boldsymbol{x}| = \int_{\mathbb{R}^m} U(f_{\boldsymbol{y}}) |d^n \boldsymbol{y}| = \int_{\mathbb{R}^m} L(f_{\boldsymbol{y}})|d^n \boldsymbol{y}| = \int_{\mathbb{R}^n\times\mathbb{R}^m} f|d^n \boldsymbol{x}||d^m \boldsymbol{y}| 
    \end{aligned}
$$

Here, $U$ and $L$ denote the upper and lower integrals of the respective functions.

:::

As a corollary of this theorem, we have the following useful facts:

::: {.corollary}
    
The set of $\boldsymbol{x}$ such that $U(f_{\boldsymbol{x}})\ne L(f_{\boldsymbol{x}})$ 
and the set of $\boldsymbol{y}$ such that $U(f^\boldsymbol{y})\ne L(f^\boldsymbol{y})$
both have measure $0$. Thus, the set of $\boldsymbol{x}$ such that 
$f_{\boldsymbol{x}}$ is not integrable has $n$-dimensional measure $0$, and the set
of $\boldsymbol{y}$ where $f^{\boldsymbol{y}}$ is not integrable has $m$-dimensional
measure $0$.

:::

### General Pavings

While dyadic pavings simplify proofs and give us a concrete paving to work with,
nothing stops us from developing the same theory using other pavings. Infact, we'll
give precise definitions of what a ***paving*** really is, and we'll mention theorems
stating that it doesn't matter which paving we really use to develop the theory
of integration.

::: {.definition name="Pavings"}

A ***paving*** of a subset $X\subset \mathbb{R}^n$ is a collection $\mathcal{P}$ of
bounded subsets $P\subset X$ such that the following hold: 

1. $\bigcup_{P\in\mathcal{P}}P = X$.
2. $\text{vol}_n(P_1\cap P_2) = 0$ when $P_1, P_2\in\mathcal{P}$ and $P_1\ne P_2$.
3. Any bounded subset of $X$ intersects only finitely many $P\in\mathcal{P}$.
4. For all $P\in\mathcal{P}$, we have $\text{vol}_n(\partial P) = 0$.

For any bounded function $f$ with bounded support, we can define an upper sum
$U_{\mathcal{P}_N}(f)$ and a lower sum $L_{\mathcal{P}_N}(f)$ with respect to
any paving:

$$
    \begin{aligned}
        U_{\mathcal{P}_N}(f) := \sum_{P\in\mathcal{P}_N} M_P(f)\text{vol}_n(P)\text{ and } U_{\mathcal{P}_N}(f) := \sum_{P\in\mathcal{P}_N} m_P(f)\text{vol}_n(P)
    \end{aligned}
$$
:::

::: {.definition name="Nested Partitions"}

A sequence $\mathcal{P}_N$ of pavings of $X\subset\mathbb{R}^n$ is called a 
***nested partition*** of $X$ if the following hold:

1. $\mathcal{P}_{N + 1}$ is a *refinement* of $\mathcal{P}_N$; i.e, every piece of 
    $\mathcal{P}_{N + 1}$ is contained in a piece of $\mathcal{P}_N$.

2. The pieces of $\mathcal{P}_N$ shrink to points as $N\to\infty$, i.e
    $$
        \begin{aligned}
            \lim_{N\to\infty} \sup_{P\in\mathcal{P}_N} \text{diam}(P) = 0 
        \end{aligned}
    $$

:::

::: {.theorem name="Integrals using arbitrary pavings"}

Let $X\subset\mathbb{R}^n$ be a bounded subset, and let $\mathcal{P}_N$ be a nested
partition of $X$.

1. If $f:\mathbb{R}^n\to\mathbb{R}$ is integrable, then the limits
    $$
        \begin{aligned}
            \lim_{N\to\infty}U_{\mathcal{P}_N}(f\boldsymbol{1}_X)\text{ and }\lim_{N\to\infty}L_{\mathcal{P}_N}(f\boldsymbol{1}_X) 
        \end{aligned}
    $$
    exist and are both equal to

    $$
        \begin{aligned}
    \int_X f(\boldsymbol{x})|d^n \boldsymbol{x}| :=  \int_{\mathbb{R}^n} f(\boldsymbol{x})\boldsymbol{1}_X(\boldsymbol{x})|d^n \boldsymbol{x}|
        \end{aligned}
    $$

2. Conversely, if the mentioned limits are equal, then $f \boldsymbol{1}_X$ is integrable
and 

    $$
        \begin{aligned}
            \int_{\mathbb{R}^n} f(\boldsymbol{x})\boldsymbol{1}_X(\boldsymbol{x})|d^n \boldsymbol{x}|
        \end{aligned}
    $$
    
    is equal to the common limit.
:::

### Volumes and determinants

::: {.theorem #determinant-scales-volumes name="Determinant scales volumes"}

Let $T:\mathbb{R}^n\to\mathbb{R}^n$ be a linear transformation given by the matrix $[T]$.
Then for any pavable set $A\subset\mathbb{R}^n$, the image $T(A)$ is pavable, and

$$
    \begin{aligned}
        \text{vol}_n T(A) &= |\det[T]|\text{vol}_n A 
    \end{aligned}
$$

:::

::: {.definition name="k-parallelograms"}

Let $\boldsymbol{v}_1, ..., \boldsymbol{v}_k$ be $k$ vectors in $\mathbb{R}^n$.
The ***$k$-parallelogram*** spanned by these vectors is the set of all 

$$
    \begin{aligned}
         t_1 \boldsymbol{v}_1 + \cdots + t_k \boldsymbol{v}_k 
    \end{aligned}
$$

with $0\le t_i\le 1$ for all $1\le i\le k$. It is denoted 
$P(\boldsymbol{v}_1, ..., \boldsymbol{v}_k)$.

:::

An application of \@ref(thm:determinant-scales-volumes) gives the following result.

::: {.proposition #parallelogram-volume name="Volume of parallelograms"}

Let $\boldsymbol{v}_1,...,\boldsymbol{v}_n$ be vectors in $\mathbb{R}^n$. Then

$$
    \begin{aligned}
        \text{vol}_n P(\boldsymbol{v}_1,...,\boldsymbol{v}_n) &= |\det[\boldsymbol{v}_1, ..., \boldsymbol{v}_n]| 
    \end{aligned}
$$

:::

#### Linear change of variables

We'll state a more general change of variables theorem. Here's a simpler version of the
theorem, where we use an invertible linear transformation as our substitution.

::: {.theorem name="Linear change of variables"}

Let $T:\mathbb{R}^n\to\mathbb{R}^n$ be an invertible linear transformation, and let 
$f:\mathbb{R}^n\to\mathbb{R}$ be an integrable function. Then $f\circ T$ is integrable,
and 

$$
    \begin{aligned}
        \int_{\mathbb{R}^n} f(\boldsymbol{y}) |d^n \boldsymbol{y}| &= |\det T| \int_{\mathbb{R}^n} f(T(\boldsymbol{x})) |d^n \boldsymbol{x}|
    \end{aligned}
$$

:::

### The general change of variables formula

::: {.theorem #change-of-variables name="Change of variables formula"}

Let $X$ be a compact subset of $\mathbb{R}^n$ with boundary $\partial X$ of volume $0$;
let $U\subset \mathbb{R}^n$ be an open set containing $X$. Let 
$\boldsymbol{\Phi}:U\to\mathbb{R}^n$ be a $\mathscr{C}^1$ map that is injective on
$(X - \partial X)$ and has Lipschitz derivative, with 
$[D\boldsymbol{\Phi}(\boldsymbol{x})]$ invertible at every 
$\boldsymbol{x}\in (X - \partial X)$. Set $Y = \boldsymbol{\Phi}(X)$.

Then, if $f:Y\to\mathbb{R}$ is integrable, 
$f\circ \boldsymbol{\Phi}|\det [D \boldsymbol{\Phi}]|$ is integrable on $X$, and

$$
    \begin{aligned}
        \int_Y f(\boldsymbol{y})|d^n \boldsymbol{y}| &= \int_X (f\circ \boldsymbol{\Phi})(\boldsymbol{x})|\det [D \boldsymbol{\Phi}(\boldsymbol{x})]||d^n \boldsymbol{x}|
    \end{aligned}
$$

:::

### Common change of variable transformations and coordinate systems

#### Polar coordinates

Consider the compact subset $X = [0, R]\times [0, 2\pi]$ of $\mathbb{R}^2$. 
Let $U$ be the disc centered at the origin of radius $R + 2\pi$; so, $U$
is an open set containing $X$. 
Clearly, the boundary $\partial X$ has $2$-dimensional volume $0$.
The ***polar coordinate map*** $\mathbb{R}^2\to\mathbb{R}^2$ maps a point $(r, \theta)$ 
to a point $(x, y)$ via the following transformation:

$$
    \begin{aligned}
         (r, \theta) \mapsto (r\cos\theta, r\sin\theta) 
    \end{aligned}
$$

It is easy to see that the map $P$ is $\mathscr{C}^\infty$. Moreover, the map $P$
is *injective* on $(X - \partial X)$, and the derivative of $P$ is given by

$$
    \begin{aligned}
        DP(r, \theta) &= 
        \begin{bmatrix}
            \cos\theta & \sin\theta\\
            -r\sin\theta & r\cos\theta\\
        \end{bmatrix}
    \end{aligned}
$$

In particular, $\det DP(r, \theta) = r$, and hence $DP(r, \theta)$ is invertible in
$(X - \partial X)$. 

Next, we check that $DP$ is Lipschitz, where we restrict the domain of $P$ to $U$. To that 
end, we consider $DP:U\to\mathbb{R}^4$ defined by

$$
    \begin{aligned}
        (r, \theta) \mapsto (\cos\theta, \sin\theta, r\cos\theta, r\sin\theta) 
    \end{aligned}
$$

In that case, note that

$$
    \begin{aligned}
        D^2P(r, \theta) &= 
        \begin{bmatrix}
            0 & 0 & \cos\theta & \sin\theta \\
            -\sin\theta & \cos\theta & -r\sin\theta & r\cos\theta \\
        \end{bmatrix}
    \end{aligned}
$$

In particular, we see that

$$
    \begin{aligned}
       \lVert D^2P(r, \theta)\rVert^2  &= 1 + 1 + r^2 
    \end{aligned}
$$

Hence, the above norm is bounded for all points in $U$, since $U$ is itself bounded.
So, we've proven that the map $P$ satisfies all conditions of Theorem 
\@ref(thm:change-of-variables). In particular, we have proven that for any integrable
function $f:\mathbb{R}^2\to\mathbb{R}$ (bounded with bounded support, by definition),
we have

$$
    \begin{aligned}
        \int_{\mathbb{R}^2} f(x, y) |dx dy| &= \int_{[0, R]\times [0, 2\pi]} f(r\cos \theta, r\sin\theta) r|dr d\theta| 
    \end{aligned}
$$

#### Spherical coordinates

The ***spherical coordinate map*** maps a point $(r, \theta,\varphi)$ to the point
$(r\cos\theta\cos\varphi, r\sin\theta\cos\varphi, r\sin\varphi)$. We can repeat a similar
analysis as in the case of the polar coordinates map to derive a change of variables
formula for $S$. In this case, the compact subset $X$ which we'll have to consider will
be $X = [0, R]\times [-\pi/2, \pi/2]\times [0, 2\pi]$.

### Lebesgue Integrals

The following definitions and motivation behind the Lebesgue integral have been taken
from [@hubbardcalculus]. This approach is quite different from the standard one, where 
Lebesgue integrals are defined from the ground up using the notion of *simple functions*.
Nonetheless, this approach is better suited for the *computation* of Lebesgue integrals,
and is equivalent to the standard approach.

#### Integrals and Limits

One of the motivating properties behind defining the Lebesgue integral is that
the Lebesgue integral behaves really well w.r.t limits. For the standard Riemann
integral, *uniform convergence* is required, which is often not satisfied.

::: {.theorem name="Convergence for Riemann integrals"}

Let $(f_k)$ be a sequence of integrable functions $\mathbb{R}^n\to\mathbb{R}$,
all with support in a fixed bal $B\subset\mathbb{R}^n$, and converging uniformly
to a function $f$. Then $f$ is integrable, and

$$
    \begin{aligned}
        \lim_{k\to\infty}\int_{\mathbb{R}^n}f_k(\boldsymbol{x})|d^n \boldsymbol{x}| &= \int_{\mathbb{R}^n} f(\boldsymbol{x})|d^n \boldsymbol{x}| 
\end{aligned}
$$

:::

We also state the ***dominated convergence theorem*** for Riemann integrals, which
requires one to assume that the limit function is itself integrable.

::: {.theorem name="Dominated convergence theorem for Riemann integrals"}

Let $f_k:\mathbb{R}^n\to\mathbb{R}$ be a sequence of Riemann integrable functions. 
Suppose there exists $R > 0$ such that all $f_k$ have their support in 
$B_R(\boldsymbol{0})$ and satisfy $|f_k|\le R$. Let $f:\mathbb{R}^n\to\mathbb{R}$
be a Riemann integrable function such that 
$f(\boldsymbol{x}) = \lim_{k\to\infty} f_k(\boldsymbol{x})$ almost everywhere. Then

$$
    \begin{aligned}
        \lim_{k\to\infty}\int_{\mathbb{R}^n} f_k(\boldsymbol{x})|d^n \boldsymbol{x}| &= \int_{\mathbb{R}^n} f(\boldsymbol{x})|d^n \boldsymbol{x}| 
    \end{aligned}
$$

:::

#### Lebesgue equality and the Lebesgue integral as a limit of Riemann integrals

The following theorem will be our basis of defining higher dimensional 
Lebesgue integrals.

::: {.theorem name="Convergence except on a set of measure 0"}

If $f_k$ is a sequence of Riemann-integrable functions on $\mathbb{R}^n$
such that

$$
    \begin{aligned}
        \sum_{k = 1}^\infty \int_{\mathbb{R}^n} |f_k(\boldsymbol{x})| |d^n \boldsymbol{x}|< \infty 
    \end{aligned}
$$

then the series $\sum_{k = 1}^\infty f_k(\boldsymbol{x})$ converges almost everywhere.

:::

Motivated by this fact, we define our version of the ***Lebesgue integral***.

::: {.theorem name="Lebesgue integral"}

Let $(f_k)$ and $(g_k)$ be two sequences of Riemann integrable functions
$\mathbb{R}^n\to\mathbb{R}$ such that

$$
    \begin{aligned}
        \sum_{k = 1}^\infty \int_{\mathbb{R}^n} |f_k(\boldsymbol{x})| |d^n \boldsymbol{x}|< \infty \text{ and } \sum_{k = 1}^\infty \int_{\mathbb{R}^n} |g_k(\boldsymbol{x})| |d^n \boldsymbol{x}|< \infty
    \end{aligned}
$$

and that

$$
    \begin{aligned}
        \sum_{k = 1}^\infty f_k = \sum_{k = 1}^\infty g_k 
    \end{aligned}
$$

almost everywhere. Then

$$
    \begin{aligned}
         \sum_{k = 1}^\infty \int_{\mathbb{R}^n} |f_k(\boldsymbol{x})| |d^n \boldsymbol{x}|= \sum_{k = 1}^\infty \int_{\mathbb{R}^n} |g_k(\boldsymbol{x})| |d^n \boldsymbol{x}|
    \end{aligned}
$$

In that case, we define the ***Lebesgue integral*** of $f := \sum_{k = 1}^\infty f_k$ by

$$
    \begin{aligned}
    \int_{\mathbb{R}^n} f(\boldsymbol{x})|d^n \boldsymbol{x}| := \sum_{k = 1}^\infty \int_{\mathbb{R}^n} f_k(\boldsymbol{x})|d^n \boldsymbol{x}| 
    \end{aligned}
$$

:::

#### Important properties of the Lebesgue integral

::: {.proposition}

The following hold:

1. The Lebesgue integral is linear.
2. If $f$ is Lebesgue integrable on $\mathbb{R}^n$, and $g$ is Riemann integrable, then
    $fg$ is Lebesgue integrable.
3. If $f$ and $g$ are Lebesgue integrable on $\mathbb{R}^n$ and if $f\le g$ almost
    everywhere, then 
    $$
        \begin{aligned}
            \int_{\mathbb{R}^n}f(\boldsymbol{x})|d^n \boldsymbol{x}| \le \int_{\mathbb{R}^n}g(\boldsymbol{x})|d^n \boldsymbol{x}|
        \end{aligned}
    $$

:::

#### Important theorems about Lebesgue integrals

::: {.theorem name="A first limit theorem for Lebesgue integrals"}

Let $(f_k)$ be a sequence of Lebesgue integrable functions such that 

$$
    \begin{aligned}
        \sum_{k = 1}^\infty \int_{\mathbb{R}^n} |f_k(\boldsymbol{x})||d^n \boldsymbol{x}| < \infty 
    \end{aligned}
$$

Then $f(\boldsymbol{x}) := \sum_{k = 1}^\infty f_k(\boldsymbol{x})$ exists almost 
everywhere, the function $f$ is Lebesgue integrable, and 

$$
    \begin{aligned}
        \int_{\mathbb{R}^n} f(\boldsymbol{x})|d^n \boldsymbol{x}| &= \sum_{k = 1}^\infty \int_{\mathbb{R}^n} f_k(\boldsymbol{x})|d^n \boldsymbol{x}| 
    \end{aligned}
$$

:::

::: {.theorem name="Monotone convergence theorem"}

Let $0\le f_1\le f_2\le \cdots$ be a sequence of Lebesgue integrable functions, where
each inequality holds almost everywhere. If

$$
    \begin{aligned}
        \sup_{k} \int_{\mathbb{R}^n} f_k(\boldsymbol{x})|d^n \boldsymbol{x}| < \infty 
    \end{aligned}
$$

then the limit $f(\boldsymbol{x}) := \lim_{k\to\infty} f_k(\boldsymbol{x})$ exists for
almost all $\boldsymbol{x}$, the function $f$ is Lebesgue integrable, and

$$
    \begin{aligned}
        \int_{\mathbb{R}^n} f(\boldsymbol{x})|d^n \boldsymbol{x}| = \sup_{k} \int_{\mathbb{R}^n} f_k(\boldsymbol{x})|d^n \boldsymbol{x}| 
    \end{aligned}
$$

Conversely, if $f$ exists almost everywhere and 
$\sup_{k}\int_{\mathbb{R}^n} f_k(\boldsymbol{x})|d^n \boldsymbol{x}| = \infty$, then 
$f$ is not Lebesgue integrable.

:::

::: {.theorem name="Dominated convergence theorem"}

Let $(f_k)$ be a sequence of Lebesgue integrable functions that converges
pointwise almost everywhere to some function $f$. Suppose there is some Lebesgue
integrable function $F:\mathbb{R}^n\to\mathbb{R}$ such that 
$|f_k(\boldsymbol{x})|\le F(\boldsymbol{x})$ for almost all $\boldsymbol{x}$. Then
$f$ is Lebesgue integrable and

$$
    \begin{aligned}
       \int_{\mathbb{R}^n} f(\boldsymbol{x})|d^n \boldsymbol{x}| &= \lim_{k\to\infty} \int_{\mathbb{R}^n} f_k(\boldsymbol{x}) |d^n \boldsymbol{x}| 
    \end{aligned}
$$

:::

::: {.theorem name="Change of variables for Lebesgue integrals"}

Let $U, V$ be open subsets of $\mathbb{R}^n$, and let $\boldsymbol{\Phi}:U\to V$ be 
bijective, of class $\mathscr{C}^1$, with inverse of class $\mathscr{C}^1$, such that 
both $\boldsymbol{\Phi}$ and $\boldsymbol{\Phi}^{-1}$ have Lipschitz derivatives. 
Let $f:V\to\mathbb{R}$ be defined except perhaps on a set of measure $0$. Then $f$ is 
Lebesgue integrable on $V$ if and only if 
$f\circ \boldsymbol{\Phi}(\det [D\boldsymbol{\Phi}])$ is Lebesgue integrable 
on $U$, and

$$
    \begin{aligned}
         \int_{V} f(\boldsymbol{v}) |d^n \boldsymbol{v}| &= \int_{U} (f\circ \boldsymbol{\Phi})(\boldsymbol{u})|\det [D\boldsymbol{\Phi}(\boldsymbol{u})]| |d^n \boldsymbol{u}|
    \end{aligned}
$$

:::

::: {.theorem name="Fubini's theorem for Lebesgue integrals"}

Let $f:\mathbb{R}^n\times \mathbb{R}^m\to\mathbb{R}$ be a Lebesgue integrable function. 
Then the function

$$
    \begin{aligned}
        \boldsymbol{y} \mapsto \int_{\mathbb{R}^n} f(\boldsymbol{x}, \boldsymbol{y})|d^n \boldsymbol{x}| 
    \end{aligned}
$$

is defined for almost all $\boldsymbol{y}\in\mathbb{R}^m$, is Lebesgue integrable on
$\mathbb{R}^m$, and

$$
    \begin{aligned}
        \int_{\mathbb{R}^n\times\mathbb{R}^m} f(\boldsymbol{x}, \boldsymbol{y}) |d^n \boldsymbol{x}||d^m \boldsymbol{y}| &= \int_{\mathbb{R}^m}\int_{\mathbb{R}^n} f(\boldsymbol{x}, \boldsymbol{y})|d^n \boldsymbol{x}||d^m \boldsymbol{y}| 
    \end{aligned}
$$

Conversely, if $f:\mathbb{R}^n\times\mathbb{R}^m\to\mathbb{R}$ is a function such that

1. every point $(\boldsymbol{x}, \boldsymbol{y})\in\mathbb{R}^{n + m}$ is the center
    of a ball on which $f$ is Lebesgue integrable

2. The function $\boldsymbol{x}\to |f(\boldsymbol{x}, \boldsymbol{y})|$ is Lebesgue 
    integrable on $\mathbb{R}^n$ for almost all $\boldsymbol{y}$

3. The function $\boldsymbol{y}\to \int_{\mathbb{R}^n} |f(\boldsymbol{x}, \boldsymbol{y})||d^n \boldsymbol{x}|$ is Lebesgue integrable on $\mathbb{R}^m$.

Then $f$ is Lebesgue integrable on $\mathbb{R}^{n + m}$, and the usual equation of Fubini's Theorem holds.

:::

Quite often, one needs to differentiate functions which are expressed as integrals.
This is usually referred to as *differentiating under the integral sign*, and there
are many theorems related to when this is possible. We state one such theorem below.

::: {.theorem name="Differentiating under the integral"}

Let $f(t, \boldsymbol{x}):\mathbb{R}^{n + 1}\to\mathbb{R}$ be a function such that
for each fixed $t$, the integral

$$
    \begin{aligned}
        F(t) := \int_{\mathbb{R}^n} f(t, \boldsymbol{x})|d^n \boldsymbol{x}| 
    \end{aligned}
$$

exists. Suppose $D_tf$ exists for almost all $\boldsymbol{x}$. If there exists
$\epsilon > 0$ and a Lebesgue integrable function $g$ such that for all $s\ne t$,

$$
    \begin{aligned}
       |s - t| < \epsilon  \implies \left|\frac{f(s, \boldsymbol{x}) - f(t, \boldsymbol{x})}{s - t}\right| \le g(\boldsymbol{x})
    \end{aligned}
$$

then $F$ is differentiable, and it's derivative is given by

$$
    \begin{aligned}
        F'(t) &= \int_{\mathbb{R}^n} D_t f(t, \boldsymbol{x}) |d^n \boldsymbol{x}| 
    \end{aligned}
$$

:::

## Volume of manifolds

In the section on integration, we briefly discussed volumes of manifolds. This chapter
will mostly cover more details about volumes of manifolds.

### Parallelograms and their volumes

We start with a simple formula, which allows us to extend the definition of volumes
to $k$-parallelograms in $\mathbb{R}^n$.

::: {.proposition name="Volume of a parallelogram"}

Let $\boldsymbol{v}_1, ..., \boldsymbol{v}_k$ be $k$ vectors in $\mathbb{R}^k$, and let
$T = [\boldsymbol{v}_1, ..., \boldsymbol{v}_k]$ be the corresponding $k\times k$ square
matrix. Then

$$
    \begin{aligned}
        \text{vol}_kP(\boldsymbol{v}_1, ..., \boldsymbol{v}_k) &= \sqrt{\det(T^TT)} 
    \end{aligned}
$$

:::

Note that the above formula agrees with the one Proposition 
\@ref(prp:parallelogram-volume), but it also allows us to *define* $k$-dimensional
volumes of manifolds in $\mathbb{R}^n$, even when $k < n$.

::: {.definition name="General volumes of parallelograms"}

Let $T = [\boldsymbol{v}_1, ... \boldsymbol{v}_k]$ be an $n\times k$ matrix. Then
the ***$k$-dimensional volume*** of $P(\boldsymbol{v}_1, ..., \boldsymbol{v}_k)$ 
(the parallelogram spanned by the vectors) is defined to be

$$
    \begin{aligned}
        \text{vol}_k P(\boldsymbol{v}_1, ..., \boldsymbol{v}_k) := \sqrt{\det(T^TT)} 
    \end{aligned}
$$

:::

### Parametrizations

Next, we'll define a relaxed version of *parametrizations* of manifolds, which
will help us compute volumes. The main reason behind not using the more stricter
version of \@ref(def:parametrization-def) is to allow things to go wrong on sets 
of volume $0$, which are irrelevant when computing volumes.

::: {.definition name="Relaxed parametrizations of manifolds"}

Let $M\subseteq\mathbb{R}^n$ be a $k$-dimensional manifold and let $U\subset\mathbb{R}^k$
be a subset with boundary of $k$-dimensional volume $0$. Let $X\subset U$ be such that
$U - X$ is open. Then a continuous mapping $\gamma:U\to\mathbb{R}^n$ ***parametrizes***
$M$ if

1. $\gamma(U)\supset M$;
2. $\gamma(U - X)\subset M$;
3. $\gamma:(U - X)\to M$ is one to one, and of class $\mathscr{C}^1$;
4. the derivative $D\gamma(\boldsymbol{u})$ is one to one for all $\boldsymbol{u}$
    in $U - X$.
5. $X$ has $k$-dimensional volume $0$, as does $\gamma(X)\cap C$ for any compact subset
    $C\subset M$.

:::

::: {.theorem name="Existence of relaxed parametrizations"}

All manifolds can be parametrized with a relaxed parametrization.

:::

### Defining volumes of manifolds

Let $M$ be a $k$-dimensional manifold embedded in $\mathbb{R}^n$. We'll use
the definition of the $k$-dimensional volume of a $k$-parallelogram in $\mathbb{R}^n$,
along with the change of variables formula, as motivation to define generalized volumes
of manifolds such as $M$. We denote the ***volume*** of $M$ by the following notation:

$$
    \begin{aligned}
        \text{vol}_k M &= \int_M |d^k \boldsymbol{x}| 
    \end{aligned}
$$

The integrand $|d^k \boldsymbol{x}|$ is called the ***$k$-th dimensional element of 
volume***.

Intuitively, we define volumes of manifolds via their *relaxed parametrizations* 
(which we know exist for all manifolds). Then, to compute the volume, we
compute values of $k$-parallelograms spanned by the partial derivatives of $\gamma$,
sum them, and take the limit as the tiling becomes finer.

::: {.theorem name="Volume of a manifold"}

Let $M\subset\mathbb{R}^n$ be a smooth $k$-dimensional manifold, $U$ a pavable
subset of $\mathbb{R}^k$, and let $\gamma: U\to M$ be a relaxed parametrization of $M$.
Let $X$ be as in the definition of the relaxed parametrization. Then

$$
    \begin{aligned}
        \text{vol}_k M := \int_{U - X} \sqrt{\det(D\gamma(\boldsymbol{u})^T D\gamma(\boldsymbol{u}))}|d^k \boldsymbol{u}| 
    \end{aligned}
$$

:::

Using this notion, we can also define ***integrals over manifolds w.r.t volumes***.

::: {.definition name="Integrals over manifolds w.r.t volumes"}

Let $M\subset \mathbb{R}^n$ be a smooth $k$-dimensional manifold, $U$ be a pavable
subset of $\mathbb{R}^k$, and $\gamma: U\to M$ be a relaxed parametrization. Let
$X$ be as in the definition of a relaxed parametrization. Then, $f:M\to\mathbb{R}$
is said to be ***integrable*** over $M$ with respect to volume if the following integral
exists:

$$
    \begin{aligned}
        \int_{M} f(\boldsymbol{x})|d^k \boldsymbol{x}| &= \int_{U - X} f(\gamma(\boldsymbol{u}))\sqrt{\det(D\gamma(\boldsymbol{u})^TD\gamma(\boldsymbol{u}))}|d^k \boldsymbol{u}| 
    \end{aligned}
$$

:::

The following fact ensures that it doesn't matter which parametrization we use:

::: {.proposition name="Integrals are independent of parametrizations"}

Let $M$ be a $k$-dimensional manifold in $\mathbb{R}^n$ and $f:M\to\mathbb{R}$
a function. Let $U_1$, $U_2$ be subsets of $\mathbb{R}^k$, and let $\gamma_1: U_1\to M$,
$\gamma_2:U_2\to M$ be two parametrizations of $M$. Then

$$
    \begin{aligned}
        \int_{U_1 - X_1} f(\gamma_1(\boldsymbol{u})) \sqrt{\det(D\gamma_1(\boldsymbol{u})^T D\gamma_1(\boldsymbol{u}))}|d^k \boldsymbol{u}|
    \end{aligned}
$$

exists if and only if the integral

$$
    \begin{aligned}
        \int_{U_2 - X_2} f(\gamma_2(\boldsymbol{u})) \sqrt{\det(D\gamma_2(\boldsymbol{u})^T D\gamma_2(\boldsymbol{u}))}|d^k \boldsymbol{u}|
    \end{aligned}
$$

exists, and in that case, both the integrals are equal.

:::

## Forms and vector calculus

[@hubbardcalculus] is, as mentioned before, the book on which the definitions and theory
here are based. However, [@arnoldmechanics] is one of the best sources for understanding
the intuition and applications of many of these concepts.

### Forms on $\mathbb{R}^n$

Intuitively, ***forms*** are just analogs of the *determinant*, i.e they are 
multilinear alternating functions. The only difference is that forms in $\mathbb{R}^n$
may operate on fewer than $n$ vectors (recall that the determinant is only defined
for square matrices). Forms, just like the derivative, are useful since they have
connections to *signed volumes*.

::: {.theorem name="k-forms"}

A ***$k$-form*** on $\mathbb{R}^n$ is a function $\varphi:\mathbb{R}^n\to\mathbb{R}$
which is multilinear and *antisymmetric*, i.e switching two arguments in $\varphi$
switches the sign of the output.

:::

::: {.example name="Standard forms"}

Let $i_1, ..., i_k$ be any $k$ integers in the set $[1, n]$. We define a form
$dx_{i_1}\land \cdots\land dx_{i_k}$ which takes $k$ vectors 
$\boldsymbol{v}_1,..., \boldsymbol{v}_k$ in $\mathbb{R}^n$, stacks these vectors
horizontally to make an $n\times k$ matrix, and selects the $k$ rows of this matrix
indexed by $i_1,...,i_k$ (in order), and computes the determinant of the resultant matrix.
It is clear that this form is multilinear and antisymmetric.

A ***$0$-form*** can be identified by a constant $\in\mathbb{R}$. It is also
straightforward to see that if $k > n$, any $k$-form is the trivial form, i.e the
zero function.

:::

### A geometric interpretation of standard forms

Consider a $k$-form $\varphi = dx_{i_1}\land \cdots\land dx_{i_k}$ in $\mathbb{R}^n$. 
From our definition, we see that evaluating $\varphi$ on a set of vectors
$\boldsymbol{v}_1,...,\boldsymbol{v}_k$ returns the *signed* $k$-dimensional volume
of the projection of the parallelogram spanned by 
$\boldsymbol{v}_1,...,\boldsymbol{v}_k$ onto the $(i_1,..., i_k)$-axes (the projection
will itself be a parallelogram). In general, a $k$-form takes in a $k$-parallelogram
(it takes as input $k$ vectors, but we interpret those vectors as the parallogram
which they span), and returns a number proportional to the $k$-dimensional
volume of the parallelogram. The details of this intuitive description of $k$-forms
can be found in this discussion: https://math.stackexchange.com/questions/548131/whats-the-geometrical-intuition-behind-differential-forms. Here is another interesting
article about the intuition behind forms (and other notions of integration)
by Terry Tao: https://terrytao.wordpress.com/2007/12/25/pcm-article-differential-forms/.

As we'll see, $k$-forms are just the kind of integrand
we need to integrate over $k$-dimensional manifolds with orientation. Intuitively,
a $k$-form will give us the signed volume of a parallelogram based on a point on 
a manifold, spanned by vectors of partial derivatives of a parametrization of the
manifold. This is very similar to when we compute unsigned volumes of manifolds,
with the only difference here being that the signed volume takes orientation into account.

::: {.definition name="Elementary forms"}

An ***elementary $k$-form*** is a form of type

$$
    \begin{aligned}
        d_{x_1}\land\cdots\land dx_{i_k} 
    \end{aligned}
$$

where $1\le i_1 < i_2 < \cdots < i_k\le n$. The only elementary $0$-form
is the form, denoted by $1$, which evaluated on zero vectors returns $1$.

:::

::: {.definition name="Space of forms"}

The space of $k$-forms on $\mathbb{R}^n$ is a vector space and is denoted by
$A^k_c(\mathbb{R}^n)$.

:::

::: {.theorem name="Elementary k-forms form a basis of the space of forms"}

The elementary $k$-forms form a basis of $A^k_c(\mathbb{R}^n)$. Hence, every $k$-form
$\varphi$ can be uniquely written as a linear combination

$$
    \begin{aligned}
        \varphi &= \sum_{1\le i_1 < \cdots < i_k\le n} a_{i_1 ,..., i_k}dx_{i_1}\land \cdots\land dx_{i_k} 
    \end{aligned}
$$

where the coefficients are given by

$$
    \begin{aligned}
        a_{i_1,...,i_k} &= \varphi(\boldsymbol{e}_{i_1}, ..., \boldsymbol{e}_{i_k})
    \end{aligned}
$$

Hence, $\dim A^k_c(\mathbb{R}^n) = \binom{n}{k}$.

:::

### The wedge (exterior) product

The ***wedge product*** takes two forms and returns another form which takes more 
arguments. The definition will justify the use of the $\land$ symbol in elementary
forms.

::: {.definition name="Wedge product"}

The *wedge product* of two forms $\varphi\in A^k_c(\mathbb{R}^n)$ and 
$\omega\in A^l_c(\mathbb{R}^n)$ is the element 
$\varphi\land\omega\in A^{k + l}_c(\mathbb{R}^n)$ defined by

$$
    \begin{aligned}
        (\varphi\land\omega)(\boldsymbol{v}_1,...,\boldsymbol{v}_{k + l}) &= \sum_{\sigma\in \text{Perm}(k, l)}\text{sgn}(\sigma)\varphi(\boldsymbol{v}_{\sigma(1)}, ..., \boldsymbol{v}_{\sigma(k)})\omega(\boldsymbol{v}_{\sigma(k + 1)}, ..., \boldsymbol{v}_{\sigma(k + l)}) 
    \end{aligned}
$$

where above, $\text{Perm}(k, l)$ is the set of all permutations $\sigma$ of $[1, k + l]$
such that $\sigma(1) < \sigma(2) \cdots < \sigma(k)$ and 
$\sigma(k + 1) < \cdots < \sigma(k + l)$.

:::

::: {.proposition name="Properties of wedge products"}

The wedge product satisfies the following properties:

1. Distributivity, i.e $\varphi\land (\omega_1 + \omega_2) = \varphi\land \omega_1 + \varphi\land \omega_2$.
2. Associativity, i.e $(\varphi_1\land\varphi_2)\land \varphi_3 = \varphi_1\land (\varphi_2\land \varphi_3)$.
3. Skew commutativity, i.e if $\varphi$ is a $k$-form and $\omega$ is an $l$-form, then

$$
    \begin{aligned}
        \varphi\land\omega &= (-1)^{kl}\omega\land\varphi 
    \end{aligned}
$$

:::

### Form fields

In general, an $x$-field on a set $U\subset\mathbb{R}^n$ is just a mapping that assigns
to each point of $U$ an object of type $x$. This is exactly what a ***form field*** is
going to be; these fields are also called ***differential forms***.

::: {.definition name="Form fields"}

A ***$k$-form field*** on an open subset $U\subset\mathbb{R}^n$ is a map 
$\varphi:U\to A^k_c(\mathbb{R}^n)$. The space of $k$-form fields on $U$ is denoted by
$A^k(U)$.

:::

Let's disect the definition a bit and see what's really going on. Let $\varphi\in A^k(U)$
be a $k$-form field on $U$. So, for each $\boldsymbol{x}\in U$, 
$\varphi(\boldsymbol{x})\in A^k_c(\mathbb{R}^n)$. We can write this form as a linear
combination of the elementary forms (since they are a basis):

$$
    \begin{aligned}
        \varphi(\boldsymbol{x}) &= \sum_{1\le i_1 < i_2 < \cdots < i_k\le n} a_{i_1,...,i_k}(\boldsymbol{x})dx_{i_1}\land\cdots \land dx_{i_k} 
    \end{aligned}
$$

Note that the coefficients $a_{i_1,...,i_k}$ are really *functions* $U\to\mathbb{R}$ 
(Equivalently, the coefficients are $0$-form fields, which are just functions). So,
such a form field $\varphi$ is to be said of class $\mathscr{C}^p$ if each of these 
coefficient functions are of class $\mathscr{C}^p$.

From this, we can immediately (kind-of) see a notion of *integration of forms fields*.
Note that, a form field associates to $U$ a set of functions 
(the coefficient functions) and signed volume elements (namely the elementary forms);
this is exactly the data we need to perform integration. However, as we'll see later,
to make such a notion of integration well-defined, we'll also need a notion of 
*orienation*.

Finally, an important interpretation: let $\varphi\in A^k(U)$ be a $k$-form field on $U$.
To each point  $\boldsymbol{x}\in U$, the form field associated a $k$-form 
$\varphi(\boldsymbol{x})$ to $\boldsymbol{x}$. Recall that $k$-forms are nothing but
objects that take as input a parallelogram, and return a number proportional to the 
$k$-dimensional volume of a projection of the parallelogram. To that end, we introduce
the notation

$$
    \begin{aligned}
        \varphi(P_{\boldsymbol{x}}(\boldsymbol{v}_1,...,\boldsymbol{v}_k)) := \varphi(\boldsymbol{x})(\boldsymbol{v}_1,...,\boldsymbol{v}_k) 
    \end{aligned}
$$

where above, as usual, $P_{\boldsymbol{x}}(\boldsymbol{v}_1 ,..., \boldsymbol{v}_k)$ is 
the parallelogram spanned by these vectors. This geometric intuition is important 
to understand why we deal with forms in the first place: they provide a notion of 
a signed integrand, helpful to perform integration with orientation.

### Integrating form fields over parametrized domains

First, we'll define what it means to integrate a form over a manifold given by a 
parametrization. In this setting, the *orientation* of the manifold will be part
of the parametrization.

::: {.definition name="Integrating a form field over a parametrized domain"}

Let $U\subset\mathbb{R}^k$ be a bounded open set with $\text{vol}_k \partial U = 0$.
Let $\gamma: U\to\mathbb{R}^n$ be a $\mathscr{C}^1$ mapping. The set $\gamma(U)$
is said to be a ***domain in $\mathbb{R}^n$ parametrized by $U$***, and the pair
$(U, \gamma)$ is denoted by $[\gamma(U)]$ (the pair is usually thought of as the 
image set along with some extra data, i.e how $\gamma$ defines an implicit orientation
on $U$). 

Let $V\subset\mathbb{R}^n$ be open, and suppose $\gamma(U)\subset V$. Also,
let $\varphi$ be a $k$-form field on $V$. The ***integral*** of $\varphi$ over $[\gamma(U)]$ is defined as

$$
    \begin{aligned}
        \int_{[\gamma(U)]}\varphi := \int_U \varphi\left(P_{\gamma(\boldsymbol{u})}(D_1\gamma(\boldsymbol{u})), ..., D_k(\gamma(\boldsymbol{u}))\right) |d^k \boldsymbol{u}|
    \end{aligned}
$$

i.e we compute the form field over the parallelogram spanned by the partial derivatives
of the parametrization $\gamma$.

:::

::: {.remark name="Parametrized domains need not be parametrizations of manifolds"}

It should be noted that in this definition, $\gamma$ is *any* $\mathscr{C}^1$ map (and
need not satisfy the definitions of parametrizations/relaxed parametrizations of 
manifolds that we have defined).

:::

## Exercises for Section 6.2

**6.2.1** In this exercise, we'll compute integrals of some form fields over parametrized
        domains.


a. $\int_{[\gamma(I)]}xdy + ydz$, where $I = [-1, 1]$ and 
$\gamma(t) = (\sin t, \cos t, t)$.  
For this one, the calculation is straightforward:
$$
    \begin{aligned}
       \int_{[\gamma(I)]}xdy + ydz &= \int_{-1}^1(xdy + ydz)\begin{pmatrix}\partial \sin t/\partial t \\ \partial\cos t/\partial t \\ \partial t / \partial t\end{pmatrix} |dt|\\
        &= \int_{-1}^1(xdy + ydz)\begin{pmatrix}\cos t\\ -\sin t\\ 1\end{pmatrix} |dt|\\
        &= \int_{-1}^1 (\cos t(-\sin t) - \sin t)|dt|\\
        &= \int_{-1}^1 -\sin t(1 + \cos t)|dt| 
    \end{aligned}
$$

b. $\int_{\gamma(U)} x_1dx_2\land dx_3 + x_2dx_3\land dx_4$, where $U = \left\{(u, v): 0\le u, v; u + v\le 2\right\}$ and $\gamma(u, v) = (uv, u^2 + v^2, u - v, \ln(u + v + 1))$.
We have the following:
$$
    \begin{aligned}
        \int_{\gamma(U)} x_1dx_2\land dx_3 + x_2dx_3\land dx_4 &= \int_U (x_1dx_2\land dx_3 + x_2dx_3\land dx_4)\begin{pmatrix} v & u \\ 2u & 2v \\ 1 & -1 \\ \frac{1}{u + v - 1} & \frac{1}{u + v - 1}\end{pmatrix} |du||dv|\\
        &= \int_{U} -2uv(u + v) + (u^2 + v^2)\cdot 0 |du||dv|\\
        &= \int_U -2uv(u + v) |du||dv|\\
        &= \int_{0}^2\int_{0}^{2 - v}-2uv(u + v)|du||dv|
    \end{aligned}
$$

## Orientation of manifolds

We first begin by defining *orientations of vector spaces*.
