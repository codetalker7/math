[["index.html", "Math Encyclopedia Chapter 1 Overview", " Math Encyclopedia Siddhant Chaudhary 2024-12-20 Chapter 1 Overview The aim of this book is to have the most important definitions, lemmas, propositions and theorems together in the same place, but without including proofs of hard theorems. The goal is to have these things ready as a reference; I did this since I don’t bother with hard proofs as I forget them sooner or later. "],["modern-probability.html", "Chapter 2 Modern Probability", " Chapter 2 Modern Probability The main reference I used for building these pages was (Klenke 2020). This book is one of the most detailed I’ve read on this subject, so condensing the notes was a good challenge! References Klenke, Achim. 2020. Probability Theory: A Comprehensive Course. 3rd ed. Springer. "],["some-important-collections-of-sets.html", "2.1 Some important collections of sets", " 2.1 Some important collections of sets Throughout, \\(\\Omega\\ne \\phi\\) is any set, and \\(\\mathcal{A}\\subseteq 2^\\Omega\\) is used to denote a collection of subsets of \\(\\Omega\\). Definition 2.1 (Sigma algebras) A collection \\(\\mathcal{A}\\subseteq 2^\\Omega\\) is said to be a \\(\\sigma\\)-algebra if it satisfies the following conditions: \\(\\Omega\\in\\mathcal{A}\\). \\(\\mathcal{A}\\) is closed under complements. \\(\\mathcal{A}\\) is closed under countable unions. Intuitively, \\(\\sigma\\)-algebras are used to define the collection of events of a random experiment which might be of interest to us. Theorem 2.1 (Generated Sigma Algebras) Let \\(\\mathcal{E}\\subset 2^\\Omega\\) be any collection of sets. Then there exists a smallest \\(\\sigma\\)-algebra \\(\\sigma(\\mathcal{E})\\) with \\(\\mathcal{E}\\subset\\sigma(\\mathcal{E})\\), particularly: \\[\\begin{equation} \\begin{split} \\sigma(\\mathcal{E}) := \\bigcap_{\\mathcal{A}\\subset 2^\\Omega\\text{ is a $\\sigma$-algebra}\\\\ \\quad\\quad\\mathcal{A}\\supset \\mathcal{E}}\\mathcal{A} \\end{split} \\tag{2.1} \\end{equation}\\] \\(\\sigma(\\mathcal{E})\\) is called the \\(\\sigma\\)-algebra generated by \\(\\mathcal{E}\\). Definition 2.2 (liminf and limsup) Let \\((A_n)_{n\\in\\mathbb{N}}\\) be a sequence of subsets of \\(\\Omega\\). Then \\[\\begin{equation} \\begin{split} A_* := \\liminf_{n\\to\\infty}A_n &amp;:= \\bigcup_{n = 1}^{\\infty} \\bigcap_{m = n}^{\\infty} A_m\\\\ A^* := \\limsup_{n\\to\\infty}A_n &amp;:= \\bigcap_{n = 1}^{\\infty} \\bigcup_{m = n}^{\\infty} A_m \\end{split} \\tag{2.2} \\end{equation}\\] Equivalently, \\[\\begin{equation} \\begin{split} A_* := \\liminf_{n\\to\\infty}A_n &amp;= \\left\\{\\omega\\in\\Omega: \\#\\left\\{n\\in\\mathbb{N}: \\omega\\notin A_n\\right\\} &lt; \\infty\\right\\}\\\\ A^* := \\limsup_{n\\to\\infty}A_n &amp;= \\left\\{\\omega\\in\\Omega: \\#\\left\\{n\\in\\mathbb{N}: \\omega\\notin A_n\\right\\} = \\infty\\right\\} \\end{split} \\tag{2.3} \\end{equation}\\] Or, in simple words, the limes inferior is the event where eventually all of the \\(A_n\\) occur, and the limes superior is the event when infinitely many of the \\(A_n\\) occur. Lemma 2.1 Let \\(\\mathcal{A}\\) be a \\(\\sigma\\)-algebra, and let \\((A_n)_{n\\in\\mathbb{N}}\\) be a sequence of sets such that \\(A_n\\in \\mathcal{A}\\). Then, \\(A_*\\in\\mathcal{A}\\) and \\(A^*\\in\\mathcal{A}\\). Definition 2.3 (Borel Sigma Algebra) Let \\(d\\) be a metric on \\(\\Omega\\), and let \\(B_r(x)\\) denote an open ball in \\(\\Omega\\) of radius \\(r\\) centered at \\(x\\). The usual class of open sets is a topology: \\[\\begin{equation} \\begin{split} \\tau &amp;= \\left\\{\\bigcup_{(x, r)\\in F} B_r(x): F\\subset \\Omega\\times (0, \\infty)\\right\\} \\end{split} \\tag{2.1} \\end{equation}\\] making \\((\\Omega, \\tau)\\) a topological spaec. The \\(\\sigma\\)-algebra \\[\\begin{equation} \\begin{split} \\mathcal{B}(\\Omega) := \\mathcal{B}(\\omega, \\tau) := \\sigma(\\tau) \\end{split} \\tag{2.1} \\end{equation}\\] generated by open sets is called the Borel \\(\\sigma\\)-algebra on \\(\\Omega\\). Sets \\(A\\in\\mathcal{B}(\\Omega, \\tau)\\) are called Borel sets or Borel measurable sets. Definition 2.4 (Trace of a class of sets) Let \\(\\mathcal{A}\\subset 2^\\Omega\\) be an arbitrary class of subsets of \\(\\Omega\\) and let \\(A\\in 2^\\Omega\\setminus \\{\\phi\\}\\). The class \\[\\begin{equation} \\begin{split} \\mathcal{A}|_{A} := \\left\\{A\\cap B: B\\in\\mathcal{A}\\right\\}\\subset 2^A \\end{split} \\tag{2.1} \\end{equation}\\] is called the trace or restriction of \\(\\mathcal{A}\\) onto \\(A\\). Theorem 2.2 Let \\(A\\subset \\Omega\\) be a non-empty set and let \\(\\mathcal{A}\\) be a \\(\\sigma\\)-algebra. Then the trace \\(\\mathcal{A}|_{A}\\) is a \\(\\sigma\\)-algebra on \\(A\\). "],["set-functions.html", "2.2 Set Functions", " 2.2 Set Functions The goal of set functions is to assign weights to each event of a \\(\\sigma\\)-algebra. This development eventually leads to what we call probability spaces. Definition 2.5 Let \\(\\mathcal{A}\\subset 2^\\Omega\\) be a class of sets and let \\(\\mu:\\mathcal{A}\\to[0, \\infty]\\) be a set function. Then, \\(\\mu\\) is monotone if \\(\\mu(A)\\le \\mu(B)\\) for any two sets \\(A, B\\in\\mathcal{A}\\) with \\(A\\subset B\\). additive if \\(\\mu\\left(\\biguplus_{i = 1}^n A_i\\right) = \\sum_{i = 1}^n \\mu(A_i)\\) for any choice of finitely many mutually disjoint sets \\((A_i)_{i = 1}^n\\in\\mathcal{A}\\) with \\(\\bigcup_{i = 1}^n A_i\\in \\mathcal{A}\\). \\(\\sigma\\)-additive if \\(\\mu\\left(\\biguplus_{i = 1}^\\infty A_i\\right) = \\sum_{i = 1}^\\infty \\mu(A_i)\\) for any choice of countably many mutually disjoint sets \\((A_i)_{i = 1}^\\infty\\in\\mathcal{A}\\) with \\(\\bigcup_{i = 1}^\\infty A_i\\in \\mathcal{A}\\). subadditive if for any choice of finitely many sets \\(A\\in\\mathcal{A}\\), \\((A_i)_{i = 1}^n\\in\\mathcal{A}\\) with \\(A\\subseteq \\bigcup_{i = 1}^n A_i\\) we have \\(\\mu(A)\\le \\sum_{i = 1}^n \\mu(A_i)\\). \\(\\sigma\\)-subadditive if for any choice of countably many sets \\(A\\in\\mathcal{A}\\), \\((A_i)_{i = 1}^\\infty\\in\\mathcal{A}\\) with \\(A\\subseteq \\bigcup_{i = 1}^\\infty A_i\\) we have \\(\\mu(A)\\le \\sum_{i = 1}^\\infty \\mu(A_i)\\). Remark. The notion of countably many objects becomes important when we move from discrete spaces to uncoutable spaces (like \\(\\mathbb{R}^n\\)). It’s impossible to assign probability weights to all subsets of an uncountable space, and still have a working theory of probability. For this reason, we must choose a much smaller (countable) collection of events which are interesting to work with. Definition 2.6 (Measures) Let \\(\\mathcal{A}\\) be a \\(\\sigma\\)-algebra. A set function \\(\\mu:\\mathcal{A}\\to[0, \\infty]\\) is called a measure on \\(\\mathcal{A}\\) if it satisfies the following conditions: \\(\\mu(\\phi) = 0\\). \\(\\mu\\) is \\(\\sigma\\)-additive. Furthermore, \\(\\mu\\) is called a probability measure if \\(\\mu(\\Omega) = 1\\). \\(\\mu\\) is said to be finite if \\(\\mu(A) &lt; \\infty\\) for every \\(A\\in\\mathcal{A}\\), and \\(\\mu\\) is said to be \\(\\sigma\\)-finite if there exists a sequence of sets \\((\\Omega_n)_{n = 1}^\\infty\\in\\mathcal{A}\\) such that \\(\\Omega = \\bigcup_{n = 1}^\\infty\\Omega_n\\) and \\(\\mu(\\Omega_n) &lt; \\infty\\) for all \\(n\\in\\mathbb{N}\\). 2.2.1 Basic properties of measures Proposition 2.1 (Basic properties of measures) Let \\(\\mathcal{A}\\) be a \\(\\sigma\\)-algebra and let \\(\\mu\\) be a measure on \\(\\mathcal{A}\\). Then the following hold: \\(\\mu(A\\cup B) = \\mu(A) + \\mu(B) - \\mu(A\\cap B)\\) for any \\(A, B\\in\\mathcal{A}\\). \\(\\mu\\) is monotone. Moreover, \\(\\mu(B) = \\mu(A) + \\mu(B\\setminus A)\\) for any sets \\(A, B\\in\\mathcal{A}\\) with \\(A\\subseteq B\\). \\(\\mu\\) is subadditive and also \\(\\sigma\\)-subadditive. \\(\\sum_{n = 1}^\\infty \\mu(A_n)\\le \\mu\\left(\\bigcup_{n = 1}^\\infty A_n\\right)\\) for any choice of countably many mutually disjoint sets \\((A_n)_{n = 1}^\\infty\\). 2.2.2 Continuity of measures The property of continuity of measures is of utter importance in probability theory. We first define what limits of sets mean, and then we state continuity results in the sense of these “set limits”. Definition 2.7 Let \\(A\\), \\((A_n)_{n = 1}^\\infty\\) be sets. We write \\(A_n\\uparrow A\\) if \\(A_i\\subseteq A_{i + 1}\\) for all \\(i\\ge 1\\) and \\(\\bigcup_{n = 1}^\\infty A_n = A\\). \\(A_n\\downarrow A\\) if \\(A_i\\supseteq A_{i + 1}\\) for all \\(i\\ge 1\\) and \\(\\bigcap_{n = 1}^\\infty A_n = A\\). "],["vector-calculus.html", "Chapter 3 Vector Calculus", " Chapter 3 Vector Calculus Much of this chapter is inspired from (John H. Hubbard 2009). Throughout this chapter, we’ll be using the word manifold to refer to smooth manifolds embedded in \\(\\mathbb{R}^n\\). References John H. Hubbard, Barbara Burke Hubbard. 2009. Vector Calculus, Linear Algebra and Differential Forms. Fifth. Matrix Editions. "],["the-inverse-and-implicit-function-theorems.html", "3.1 The Inverse and Implicit function theorems", " 3.1 The Inverse and Implicit function theorems This section just contains the statements of these two theorems without proof. For proofs, I’ll refer the reader to Lectures 9 through 11 of these amazing notes: https://www.cmi.ac.in/~pramath/ANA2/Lectures/. Theorem 3.1 (Inverse Function Theorem) Let \\(U\\) be an open subset of \\(\\mathbb{R}^n\\), \\(\\boldsymbol{f}: U\\to\\mathbb{R}^n\\) be a \\(\\mathscr{C}^1\\) map, \\(\\boldsymbol{a}\\in U\\) be a point such that \\(\\boldsymbol{f}&#39;(\\boldsymbol{a})\\) is invertible, and let \\(\\boldsymbol{b} = \\boldsymbol{a}\\). Then there exists an open neighborhood \\(V\\) of \\(\\boldsymbol{a}\\) in \\(U\\), an open neighborhood \\(W\\) of \\(\\boldsymbol{b}\\) in \\(\\mathbb{R}^n\\) such that \\(W = \\boldsymbol{f}(V)\\), \\(\\boldsymbol{f}|_{V}:V\\to W\\) is one-to-one, and the inverse function \\(\\boldsymbol{\\varphi}^{-1}:W\\to V\\) of \\(\\boldsymbol{f}|_{V}\\) is also \\(\\mathscr{C}^1\\). Theorem 3.2 (Implicit Function Theorem) Let \\(d, m\\) be non-negative integers, \\(n = d + m\\), and write all points of \\(\\mathbb{R}^n = \\mathbb{R}^d\\times \\mathbb{R}^m\\) in the form \\((\\boldsymbol{x}, \\boldsymbol{y})\\) with \\(\\boldsymbol{x}\\in\\mathbb{R}^d\\) and \\(\\boldsymbol{y}\\in\\mathbb{R}^m\\). Let \\(U\\) be an open subset of \\(\\mathbb{R}^n\\), \\(\\boldsymbol{p} = (\\boldsymbol{a}, \\boldsymbol{b})\\) a point in \\(U\\), and \\(\\boldsymbol{\\varphi}:U\\to\\mathbb{R}^m\\) a \\(\\mathscr{C}^1\\) map such that the \\(m\\times m\\) matrix \\[\\begin{equation} \\begin{split} \\frac{\\partial \\boldsymbol{\\varphi}(\\boldsymbol{p})}{\\partial \\boldsymbol{y}} := \\begin{bmatrix} D_{d + 1}\\boldsymbol{\\varphi}(\\boldsymbol{p}) &amp; D_{d + 2}\\boldsymbol{\\varphi}(\\boldsymbol{p}) &amp; \\cdots &amp; D_{n}\\boldsymbol{\\varphi}(\\boldsymbol{p}) \\end{bmatrix} \\end{split} \\end{equation}\\] is invertible. Let \\(\\boldsymbol{c} = \\boldsymbol{\\varphi}(\\boldsymbol{p})\\). Then there exists an open neighborhood \\(W\\) of \\(\\boldsymbol{a}\\) in \\(\\mathbb{R}^d\\), and a unique \\(\\mathscr{C}^1\\) map \\(\\boldsymbol{f}:W\\to\\mathbb{R}^m\\) with the following properties: \\(\\boldsymbol{f}(\\boldsymbol{a}) = \\boldsymbol{b}\\). \\((\\boldsymbol{x}, \\boldsymbol{f}(\\boldsymbol{x}))\\in U\\) for all \\(\\boldsymbol{x}\\in W\\). \\(\\boldsymbol{f}\\) is an implicit solution of the equation \\(\\boldsymbol{\\varphi}(\\boldsymbol{x}, \\boldsymbol{y}) = \\boldsymbol{c}\\), i.e \\[\\begin{equation} \\begin{split} \\boldsymbol{\\varphi}(\\boldsymbol{x}, \\boldsymbol{f}(\\boldsymbol{x})) &amp;= \\boldsymbol{c}\\quad\\forall \\boldsymbol{x}\\in W \\end{split} \\end{equation}\\] "],["smooth-manifolds-in-mathbbrn.html", "3.2 Smooth manifolds in \\(\\mathbb{R}^n\\)", " 3.2 Smooth manifolds in \\(\\mathbb{R}^n\\) The goal of this section is to define smooth manifolds in \\(\\mathbb{R}^n\\), without resorting to tools from general topological manifolds. This will be helpful to computer scientists who don’t want to deal with abstract notions pertaining to manifold theory. Definition 3.1 (Smooth Manifolds) A subset \\(M\\subset \\mathbb{R}^n\\) is a smooth \\(k\\)-dimensional manifold if locally it is the graph of a \\(\\mathscr{C}^1\\) mapping \\(\\boldsymbol{f}\\) expressing \\(n - k\\) variables as functions of the other \\(k\\) variables. In this book, we don’t deal with topological manifolds, but only subsets of \\(\\mathbb{R}^n\\) for any \\(n\\). More precisely, suppose we write each point \\(\\boldsymbol{p}\\in\\mathbb{R}^n\\) as a point \\((\\boldsymbol{x}, \\boldsymbol{y})\\in\\mathbb{R}^k\\times \\mathbb{R}^{n - k}\\), where we have implicitly assumed that the first \\(k\\) coordinates are independent. Then, this definition implies that for every \\((\\boldsymbol{x}, \\boldsymbol{y})\\in M\\), there is an open neighborhood \\(\\boldsymbol{x}\\in U\\subset \\mathbb{R}^k\\) of \\(\\boldsymbol{x}\\) and an open neighborhood \\(W\\subset\\mathbb{R}^n\\) of \\((\\boldsymbol{x}, \\boldsymbol{y})\\) such that \\[\\begin{equation} \\begin{split} M\\cap W &amp;= \\left\\{(\\boldsymbol{z}, \\boldsymbol{f}(\\boldsymbol{z})): z\\in U\\right\\} \\end{split} \\end{equation}\\] 3.2.1 Identifying manifolds, and manifolds under smooth transformations. Theorem 3.3 (Using the implicit function theorem to identify manifolds) Here’s how to show that a set represented as a locus is a smooth manifold: Let \\(U\\subset \\mathbb{R}^n\\) be open, \\(\\boldsymbol{F}:U\\to\\mathbb{R}^{n - k}\\) be a \\(\\mathscr{C}^1\\) mapping. Let \\(M\\) be a subset of \\(\\mathbb{R}^n\\) such that \\[ \\begin{aligned} M\\cap U &amp;= \\left\\{\\boldsymbol{z}\\in U | \\boldsymbol{F}(\\boldsymbol{z}) = \\boldsymbol{0}\\right\\} \\end{aligned} \\] If \\(D\\boldsymbol{F}(\\boldsymbol{z})\\) is onto for every \\(\\boldsymbol{z}\\in M\\cap U\\), then \\(M\\cap U\\) is a smooth \\(k\\)-dimensional manifold embedded in \\(\\mathbb{R}^n\\). If every \\(z\\in M\\) is in such a \\(U\\), then \\(M\\) is a \\(k\\)-dimensional manifold. Conversely, if \\(M\\) is a smooth \\(k\\)-dimensional manifold embedded in \\(\\mathbb{R}^n\\), then every point \\(\\boldsymbol{z}\\in M\\) has a neighborhood \\(U\\subset\\mathbb{R}^n\\) such that there exists a \\(\\mathscr{C}^1\\) mapping \\(\\boldsymbol{F}:U\\to\\mathbb{R}^{n - k}\\) with \\(D\\boldsymbol{F}(\\boldsymbol{z})\\) onto and \\(M\\cap U = \\left\\{\\boldsymbol{y}|\\boldsymbol{F}(\\boldsymbol{y}) = 0\\right\\}\\). Remark. The first part of the above theorem follows from the Implicit Function Theorem 3.2. For the second part, it is enough to consider the map \\[\\begin{equation} \\begin{split} (\\boldsymbol{x}, \\boldsymbol{y}) \\mapsto \\boldsymbol{y} - \\boldsymbol{f}(\\boldsymbol{x}) \\end{split} \\end{equation}\\] where the map \\(\\boldsymbol{f}\\) is considered right from the definition of a smooth manifold 3.1. The above theorem makes it easy to identify/prove that a certain object is a smooth manifold, without knowing the overall geometry of the object. The next theorem shows that the inverse image of a manifold under a sufficiently nice transformation still results in a smooth manifold. Theorem 3.4 Let \\(M\\subset \\mathbb{R}^m\\) be a \\(k\\)-dimensional manifold, \\(U\\) an open subset of \\(\\mathbb{R}^n\\), and \\(\\boldsymbol{f}:U\\to \\mathbb{R}^m\\) a \\(\\mathscr{C}^1\\) mapping whose derivative \\(D\\boldsymbol{f}(\\boldsymbol{x})\\) is surjective at every point \\(\\boldsymbol{x}\\in \\boldsymbol{f}^{-1}(M)\\). Then the inverse image \\(\\boldsymbol{f}^{-1}(M)\\) is a submanifold of \\(\\mathbb{R}^n\\) of dimension \\(k + n - m\\). Remark (Manifolds under rotations and translations are still manifolds). As a corollary of Theorem 3.4, it’s easy to show that for a mapping of the form \\(\\boldsymbol{x}\\mapsto A\\boldsymbol{x} + \\boldsymbol{c}\\) with \\(A\\) invertible, the image of a manifold is still a manifold. 3.2.2 Parametrizations of manifolds Another nice way of representing manifolds is via their parametrizations. Definition 3.2 (Parametrization of a manifold) A parametrization of a \\(k\\)-dimensional manifold \\(M\\subset\\mathbb{R}^n\\) is a mapping \\(\\gamma:U\\subset \\mathbb{R}^k\\to M\\) satisfying the following conditions: \\(U\\) is open. \\(\\gamma\\) is \\(\\mathscr{C}^1\\), one-to-one, and onto on \\(M\\). \\(D\\gamma(\\boldsymbol{u})\\) is one-to-one for every \\(\\boldsymbol{u}\\in U\\). Theorem 3.5 (Existence of local parametrizations) Let \\(M\\subset\\mathbb{R}^m\\) be a \\(k\\)-dimensional manifold. For every point \\(\\boldsymbol{p}\\in M\\), there is an open neighborhood \\(U\\subset\\mathbb{R}^m\\) of \\(\\boldsymbol{p}\\) such that \\(M\\cap U\\) admits a parametrization \\(\\gamma:\\mathbb{R}^k\\to M\\cap U\\). Proof. Without loss of generality, write \\(\\boldsymbol{p} = (\\boldsymbol{x}, \\boldsymbol{y})\\in\\mathbb{R}^k\\times\\mathbb{R}^{n - k}\\) where the first \\(k\\) variables are indepdendent. Let \\(W\\subset\\mathbb{R}^k\\) be an open neighborhood of \\(\\boldsymbol{x}\\) such that \\[ \\begin{aligned} M\\cap U &amp;= \\left\\{\\boldsymbol{z}, \\boldsymbol{f}(\\boldsymbol{z}): \\boldsymbol{z}\\in W\\right\\} \\end{aligned} \\] Now, define the mapping \\(\\gamma:W\\to M\\cap U\\) by \\[ \\begin{aligned} \\boldsymbol{z}\\mapsto (\\boldsymbol{z}, \\boldsymbol{f}(\\boldsymbol{z})) \\end{aligned} \\] Clearly, \\(\\gamma\\) is one-one and onto. Moreover, observe that \\[ \\begin{aligned} D\\gamma(\\boldsymbol{z}) &amp;= \\begin{bmatrix} I \\\\ D\\boldsymbol{f}(\\boldsymbol{z}) \\end{bmatrix} \\end{aligned} \\] Clearly, the first \\(k\\) rows of the Jacobian are linearly independent; hence, the matrix is invertible. So, \\(\\gamma\\) is indeed a parametrization of \\(M\\cap U\\). "],["tangent-spaces.html", "3.3 Tangent spaces", " 3.3 Tangent spaces Definition 3.3 (Tangent spaces) Let \\(M\\subset \\mathbb{R}^n\\) be a \\(k\\)-dimensional manifold given by the equation \\(\\boldsymbol{y} = \\boldsymbol{f}(\\boldsymbol{x})\\). The tangent space to \\(M\\) at \\(\\boldsymbol{z}_0 := (\\boldsymbol{x}_0, \\boldsymbol{y}_0)\\), denoted by \\(T_{\\boldsymbol{z}_0}M\\), is the graph of the linear map \\(D\\boldsymbol{f}(\\boldsymbol{x}_0)\\). 3.3.1 Computing tangent spaces We can use the machinery built by the implicit function theorem and parametrizations to easily compute tangent spaces. Without proof, here are two theorems (you can check the book for proofs) which essentially tell us what the tangent spaces really are. Theorem 3.6 Suppose \\(\\boldsymbol{F}(\\boldsymbol{z}) = \\boldsymbol{0}\\) describes a manifold \\(M\\), and \\(D\\boldsymbol{F}(\\boldsymbol{z}_0)\\) is onto for some \\(\\boldsymbol{z}_0\\in M\\). Then \\[\\begin{equation} \\begin{split} T_{\\boldsymbol{z}_0}M &amp;= \\text{ker}\\,D\\boldsymbol{F}(\\boldsymbol{z}_0) \\end{split} \\end{equation}\\] Theorem 3.7 Let \\(U\\subset \\mathbb{R}^k\\) be an open set and let \\(\\gamma:U\\to\\mathbb{R}^n\\) be a parametrization of a manifold \\(M\\). Then \\[ \\begin{aligned} T{\\gamma(\\boldsymbol{u})}M = \\text{img}\\,D\\gamma(\\boldsymbol{u}) \\end{aligned} \\] "],["differentiable-maps-on-manifolds.html", "3.4 Differentiable maps on manifolds", " 3.4 Differentiable maps on manifolds Definition 3.4 (Smooth maps on manifolds) Let \\(M\\subset\\mathbb{R}^n\\) be an \\(m\\)-dimensional manifold, and let \\(\\boldsymbol{f}:M\\to\\mathbb{R}^k\\) be a map. Then \\(\\boldsymbol{f}\\) is of class \\(\\mathscr{C}^p\\) if every \\(\\boldsymbol{x}\\in M\\) has a neighborhood \\(U\\subset \\mathbb{R}^n\\) such that there exists a \\(\\mathscr{C}^p\\) map \\(\\boldsymbol{\\tilde{f}}:U\\to\\mathbb{R}^k\\) with \\(\\boldsymbol{\\tilde{f}}|_{U\\cap M} = \\boldsymbol{f}|_{U\\cap M}\\). Moreover, for \\(p\\ge 1\\), we define the derivative of \\(\\boldsymbol{f}\\) to be \\[\\begin{equation} \\begin{split} D\\boldsymbol{f}(\\boldsymbol{x}): T_{\\boldsymbol{x}}M\\to \\mathbb{R}^k := D\\boldsymbol{\\tilde{f}}|_{T_{\\boldsymbol{x}}M} \\end{split} \\end{equation}\\] Theorem 3.8 (Choice of extension doesn't matter) In Definition 3.4, any choice of the extension map \\(\\boldsymbol{\\tilde{f}}\\) results in the same derivative. Theorem 3.9 (Chain rule for maps on manifolds.) Let \\(M\\subset\\mathbb{R}^n\\) be a manifold, and let \\(\\boldsymbol{f}:M\\to\\mathbb{R}^k\\) be a \\(\\mathscr{C}^1\\) map. Let \\(U\\subset\\mathbb{R}^l\\) be open and let \\(\\boldsymbol{g}:U\\to M\\) be a \\(\\mathscr{C}^1\\) map. Then, \\[\\begin{equation} \\begin{split} D(\\boldsymbol{f}\\circ\\boldsymbol{g})(\\boldsymbol{x}) &amp;= D\\boldsymbol{f}(\\boldsymbol{g}(\\boldsymbol{x}))D\\boldsymbol{g}(\\boldsymbol{x})\\quad \\text{for all }\\boldsymbol{x}\\in U \\end{split} \\end{equation}\\] "],["taylor-polynomials-in-multiple-variables.html", "3.5 Taylor polynomials in multiple variables", " 3.5 Taylor polynomials in multiple variables 3.5.1 Taylor’s theorem in one variable Theorem 3.10 (Taylor's theorem without remainder in one variable) Let \\(U\\subseteq\\mathbb{R}\\) be an open subset and let \\(f:U\\to\\mathbb{R}\\) be \\(k\\)-times continuously differentiable on \\(U\\). Then, for any \\(a\\in U\\), the polynomial \\[\\begin{equation} \\begin{split} p_{f, a}^k(a + h) := \\sum_{i = 0}^k \\frac{f^{(i)}(a)}{i!}h^i \\end{split} \\end{equation}\\] is the unique polynomial of degree \\(\\le k\\) such that \\[\\begin{equation} \\begin{split} \\lim_{h\\to 0} \\frac{f(a + h) - p_{f, a}^k(a + h)}{h^k} = 0 \\end{split} \\end{equation}\\] In other words, \\(p_{f, a}^k\\) is the best local approximation to \\(f\\) at the point \\(a\\) by a polynomial of degree atmost \\(k\\). 3.5.2 Multi-exponents To represent Taylor polynomials in higher dimensions, the multi-exponent notation comes in handy. Formally, a multi-exponent \\(I\\) is just an ordered finite list of non-negative whole numbers: \\[\\begin{equation} \\begin{split} I := (i_1, \\cdots, i_n) \\end{split} \\end{equation}\\] The total degree of \\(I\\) is the sum \\(\\sum_{k = 1}^N i_k\\), and the factorial of \\(I\\) is the product \\(\\prod_{k = 1}^n i_k!\\). For a multi-exponent \\(I\\), the notation \\(\\boldsymbol{x}^I := x_1^{i_1}\\cdots x_n^{i_n}\\), i.e a monomial on \\(\\mathbb{R}^n\\). So, any polynomial \\(p\\) of degree \\(m\\) in \\(n\\) variables can be written using the notation \\[\\begin{equation} \\begin{split} p(\\boldsymbol{x}) &amp;= \\sum_{k = 0}^m\\sum_{I}a_I\\boldsymbol{x}^I \\end{split} \\end{equation}\\] We use the notation \\(\\mathcal{I}^m_n\\) to denote the set of all multi-exponents of total degree \\(m\\) in \\(n\\) variables. 3.5.3 Equality of mixed partials Mutli-exponents can be used to nicely state a theorem about equality of mixed partials of multivariate function. For a function \\(f:U\\to \\mathbb{R}\\) with \\(U\\subset \\mathbb{R}^n\\) being open and a multi-exponent \\(I\\), we define \\[\\begin{equation} \\begin{split} D_If := D_1^{i_1}D_2^{i_2}\\cdots D_n^{i_n}f \\end{split} \\end{equation}\\] The use of multi-exponents in mixed partials can be justified by the following theorem on equality of crossed partials: Theorem 3.11 (Equality of mixed partials) Let \\(U\\) be an open subset of \\(\\mathbb{R}^n\\), and \\(f:U\\to\\mathbb{R}\\) a function such that all first partial derivatives \\(D_if\\) are differentiable at \\(\\boldsymbol{a}\\in U\\). Then, for any \\(i, j\\in[n]\\), we have \\[\\begin{equation} \\begin{split} D_j(D_if)(\\boldsymbol{a}) &amp;= D_i(D_jf)(\\boldsymbol{a}) \\end{split} \\end{equation}\\] Corollary 3.1 If \\(f:U\\to\\mathbb{R}\\) is a function, all of whose partial derivatives upto order \\(k\\) are continuous, then the partial derivatives of order upto \\(k\\) do not depend on the order in which they are computed. A special case of this theorem is to compute derivatives of a polynomial, which can be succintly written using multi-exponents: Proposition 3.1 (Coefficients expressed in terms of partial derivatives at 0) Let \\(p\\) be a polynomial \\[\\begin{equation} \\begin{split} p(\\boldsymbol{x}) := \\sum_{m = 0}^k\\sum_{J} a_J\\boldsymbol{x}^J \\end{split} \\end{equation}\\] with \\(\\boldsymbol{x}\\) a monomial with \\(n\\) variables. Then, for any multi-exponent \\(I\\), we have \\[\\begin{equation} \\begin{split} I!a_I &amp;= D_Ip(\\boldsymbol{0}) \\end{split} \\end{equation}\\] 3.5.4 Taylor polynomials in higher dimensions Definition 3.5 (Taylor polynomial in higher dimensions) Let \\(U\\subset\\mathbb{R}^n\\) be an open subset and let \\(f:U\\to\\mathbb{R}\\) be a \\(\\mathscr{C}^k\\) function. Then the polynomial of degree \\(k\\) \\[\\begin{equation} \\begin{split} P_{f, \\boldsymbol{a}}^k(\\boldsymbol{a} + \\boldsymbol{h}) := \\sum_{m = 0}^k\\sum_{I} \\frac{D_If(\\boldsymbol{a})\\boldsymbol{h}^I}{I!} \\end{split} \\end{equation}\\] is called the Taylor polynomial of degree \\(k\\) of \\(f\\) at \\(\\boldsymbol{a}\\). If \\(\\boldsymbol{f}:U\\to\\mathbb{R}^n\\) is a \\(\\mathscr{C}^k\\) function, its Taylor polynomial is the polynomial map \\(U\\to\\mathbb{R}^n\\) whose coordinate functions are the Taylor polynomials of the coordinate functions of \\(\\boldsymbol{f}\\). Theorem 3.12 (Taylor's theorem without remainder in higher dimensions) Let \\(U\\subseteq\\mathbb{R}^n\\) be open, \\(\\boldsymbol{a}\\in U\\) be a point, and \\(f:U\\to\\mathbb{R}\\) be a \\(\\mathscr{C}^k\\) function. The polynomial \\(P_{f, \\boldsymbol{a}}^k(\\boldsymbol{a} + \\boldsymbol{h})\\) is the unique polynomial of degree \\(k\\) with the same partial derivatives up to order \\(k\\) at \\(\\boldsymbol{a}\\) as \\(f\\). It best approximates \\(f\\) near \\(\\boldsymbol{a}\\): it is the unique polynomial of degree atmost \\(k\\) such that \\[\\begin{equation} \\begin{split} \\lim_{\\boldsymbol{h}\\to \\boldsymbol{0}} \\frac{f(\\boldsymbol{a} + \\boldsymbol{h}) - P^k_{f, \\boldsymbol{a}}(\\boldsymbol{a} + \\boldsymbol{h})}{\\lVert h\\rVert^k} = 0 \\end{split} \\end{equation}\\] "],["quadratic-forms.html", "3.6 Quadratic Forms", " 3.6 Quadratic Forms Definition 3.6 (Quadratic Forms) A quadratic form \\(Q:\\mathbb{R}^n\\to\\mathbb{R}\\) is a polynomial all of whose terms are of degree \\(2\\). 3.6.1 Quadratic forms as sums of squares Theorem 3.13 For any quadratic form \\(Q:\\mathbb{R}^n\\to\\mathbb{R}\\), there exist \\(m = k + l\\) linearly independent linear functions \\(\\alpha_1,\\cdots,\\alpha_m:\\mathbb{R}^n\\to\\mathbb{R}\\) such that \\[ \\begin{aligned} Q(\\boldsymbol{x}) &amp;= (\\alpha_1(\\boldsymbol{x}))^2 + \\cdots + (\\alpha_k(\\boldsymbol{x}))^2 - (\\alpha_{k + 1}(\\boldsymbol{x}))^2 - \\cdots + (\\alpha_{k + l}(\\boldsymbol{x}))^2 \\end{aligned} \\] The number \\(k\\) of plus signs and the number \\(l\\) of minus signs only depend on \\(Q\\). The pair \\((k, l)\\) is called the signature of the quadratic form \\(Q\\). Definition 3.7 (Positive and negative definite forms) A quadratic form \\(Q\\) is said to be positive definite if \\(Q(\\boldsymbol{x}) &gt; 0\\) for \\(\\boldsymbol{x}\\ne \\boldsymbol{0}\\), and negative definite if \\(Q(\\boldsymbol{x}) &lt; 0\\) whenever \\(\\boldsymbol{x} \\ne \\boldsymbol{0}\\). Proposition 3.2 Suppose \\(Q(\\boldsymbol{x}) = (\\alpha_1(\\boldsymbol{x}))^2 + \\cdots + (\\alpha_k(\\boldsymbol{x}))^2 - (\\alpha_{k + 1}(\\boldsymbol{x}))^2 - \\cdots + (\\alpha_{k + l}(\\boldsymbol{x}))^2\\) where the \\(\\alpha_i\\)’s are linearly independent linear functions from \\(\\mathbb{R}^n\\to\\mathbb{R}\\). Then, \\(k\\) is the largest dimension of a subspace of \\(\\mathbb{R}^n\\) on which \\(Q\\) is positive definite, and \\(l\\) is the largest dimension of a subspace of \\(\\mathbb{R}^n\\) on which \\(Q\\) is negative definite. Corollary 3.2 If \\(Q:\\mathbb{R}^n\\to\\mathbb{R}\\) is a quadratic form and \\(A:\\mathbb{R}^n\\to\\mathbb{R}\\) is invertible, then \\(Q\\circ A\\) is a quadratic form with the same signature as \\(Q\\). 3.6.2 Classification of quadratic forms Definition 3.8 (Rank of a quadratic form) The rank of a quadratic form is the number of linearly independent squares that appear when the quadratic form is represented as a sum of linearly independent squares. Equivalently, if \\((k, l)\\) is the signature of the form, it’s rank is the sum \\(k + l\\). Definition 3.9 (Degeneracy of forms) A quadratic form on \\(\\mathbb{R}^n\\) is non-degenerate if it’s rank is \\(n\\). Otherwise, it’s said to be degenerate. Proposition 3.3 If \\(Q:\\mathbb{R}^n\\to\\mathbb{R}\\) is a positive definite quadratic form, then there exists a constant \\(C &gt; 0\\) such that \\[ \\begin{aligned} Q(\\boldsymbol{x}) \\ge C\\lVert\\boldsymbol{x}\\rVert^2 \\end{aligned} \\] for all \\(\\boldsymbol{x}\\in\\mathbb{R}^n\\). 3.6.3 Quadratic forms as symmetric matrices Theorem 3.14 The mapping \\(A\\mapsto Q_A\\), where \\(Q_A(\\boldsymbol{x}) = \\boldsymbol{x}^TA \\boldsymbol{x}\\), is a bijective map from the space of \\(n\\times n\\) symmetric matrices to the space of quadratic forms on \\(\\mathbb{R}^n\\). "],["classifying-critical-points-of-a-function.html", "3.7 Classifying critical points of a function", " 3.7 Classifying critical points of a function Here’s a well-known theorem used for classifying the critical points of a single-variable function: Theorem 3.15 (Extrema of functions of one variable) Let \\(U\\subset\\mathbb{R}\\) be an open set and let \\(f:U\\to\\mathbb{R}\\) be a differentiable function. If \\(x_0\\in U\\) is a local minimum or a local maximum of \\(f\\), then \\(f&#39;(x_0) = 0\\). If \\(f\\) is twice differentiable, and \\(f&#39;(x_0) = 0\\) and \\(f&#39;&#39;(x_0) &gt; 0\\), then \\(x_0\\) is a strict local minimum of \\(f\\). If \\(f\\) is twice differentiable, and \\(f&#39;(x_0) = 0\\) and \\(f&#39;&#39;(x_0) &lt; 0\\), then \\(x_0\\) is a strict local maximum of \\(f\\). We’ll now see some machinery for higher-dimensional functions. Theorem 3.16 (Critical points and critical values.) Let \\(U\\subseteq \\mathbb{R}^n\\) be open and let \\(f:U\\to\\mathbb{R}\\) be a differentiable function. A critical point of \\(f\\) is a point where the derivative vanishes. The value of \\(f\\) at a critical points is a critical value. A simple consequence of Theorem 3.15 is the following high-dimensional version: Theorem 3.17 Let \\(U\\subset\\mathbb{R}^n\\) be an open subset and let \\(f:U\\to\\mathbb{R}\\) be a differentiable function. If \\(\\boldsymbol{x}_0\\in U\\) is a local minimum or local maximum of \\(f\\), then \\(Df(\\boldsymbol{x}_0) = \\boldsymbol{0}\\). 3.7.1 Second derivative conditions Just like in one dimension, we usually look at the second derivative of a function to classify a point as being a local maximum or a local minimum. The situation for higher dimensions is similar, where instead we look at the quadratic form consisting of the even degree terms of the Taylor polynomial. Theorem 3.18 (Signature of a critical point) Let \\(U\\subset \\mathbb{R}^n\\) be an open set, and let \\(f:U\\to\\mathbb{R}\\) be of class ^2. Let \\(\\boldsymbol{a}\\in U\\) be a critical point of \\(f\\). The signature of the critical point \\(\\boldsymbol{a}\\) is the signature of the quadratic form \\[ \\begin{aligned} Q_{f, \\boldsymbol{a}}(\\boldsymbol{h}) := \\sum_{I\\in\\mathcal{I}^2_n} \\frac{1}{I!}(D_If(\\boldsymbol{a}))\\boldsymbol{h}^I \\end{aligned} \\] It turns out that the symmetric matrix associated with the quadratic form \\(Q_{f, \\boldsymbol{a}}\\) is the Hessian of \\(f\\), i.e \\[ \\begin{aligned} Q_{f, \\boldsymbol{a}}(\\boldsymbol{h}) &amp;= \\frac{1}{2}(\\boldsymbol{h}^TH \\boldsymbol{h}) \\end{aligned} \\] where \\(H\\) is the Hessian of \\(f\\) at \\(\\boldsymbol{a}\\). Definition 3.10 A critical point \\(\\boldsymbol{a}\\) of \\(f\\) is said to be degenerate or non-degenerate precisely when the quadratic form \\(Q_{f, \\boldsymbol{a}}\\) is degenerate or non-degenerate. Theorem 3.19 (Quadratic forms and extrema) Let \\(U\\subset\\mathbb{R}^n\\) be an open set, \\(f:U\\to\\mathbb{R}\\) be of class \\(\\mathscr{C}^2\\), and let \\(\\boldsymbol{a}\\in U\\) be a critical point of \\(f\\). If the signature of \\(\\boldsymbol{a}\\) is \\((n, 0)\\), i.e if \\(Q_{f, \\boldsymbol{a}}\\) is positive definite, then \\(\\boldsymbol{a}\\) is a strict local minimum of \\(f\\). If the signature of \\(\\boldsymbol{a}\\) is \\((k, l)\\) with \\(l &gt; 0\\), then \\(\\boldsymbol{a}\\) is not a local minimum. An analogous result holds for when the quadratic form is negative definite, with local minimum replaced by local maximum. Definition 3.11 (Saddle points) If \\(\\boldsymbol{a}\\) is a critical point of a \\(\\mathscr{C}^2\\) function \\(f\\), and the quadratic form \\(Q_{f, \\boldsymbol{a}}\\) has signature \\((k, l)\\) with \\(k, l &gt; 0\\), then \\(\\boldsymbol{a}\\) is said to be a saddle point. Theorem 3.20 (Behavior of a function near a saddle) Let \\(U\\subset\\mathbb{R}^n\\) be an open set, and let \\(f:U\\to\\mathbb{R}\\) be a \\(\\mathscr{C}^2\\) function. If \\(f\\) has a saddle at \\(\\boldsymbol{a}\\in U\\), then in every neighborhood of \\(\\boldsymbol{a}\\) there are points \\(\\boldsymbol{b}\\) and \\(\\boldsymbol{c}\\) with \\(f(\\boldsymbol{b}) &gt; f(\\boldsymbol{a})\\) and \\(f(\\boldsymbol{c}) &lt; f(\\boldsymbol{a})\\). "],["constrainted-critical-points-and-lagrange-multipliers.html", "3.8 Constrainted critical points and Lagrange Multipliers", " 3.8 Constrainted critical points and Lagrange Multipliers Definition 3.12 (Critical point of functions defined on manifolds) Let \\(X\\subset \\mathbb{R}^n\\) be a manifold, and let \\(f:X\\to\\mathbb{R}\\) be a \\(\\mathscr{C}^1\\) function. A critical point of \\(f\\) is a point \\(\\boldsymbol{x}\\in X\\) where \\(Df(\\boldsymbol{x}) = \\boldsymbol{0}\\). Such points are also called constrained critical points. Analogous to the unconstrained optimization problem, we have the following theorem: Theorem 3.21 Let \\(X\\subset\\mathbb{R}^n\\) be a manifold, \\(f:X\\to\\mathbb{R}\\) a \\(\\mathscr{C}^1\\) function, and \\(\\boldsymbol{c}\\in X\\) a local extremum of \\(f\\). Then \\(\\boldsymbol{c}\\) is a constrained critical point of \\(f\\). 3.8.1 Lagrange Multipliers Suppose we’re given a differentiable function on an open set, and we want to optimize it on a subset which is constrained by another function. So, we have an objective function and a constrain function as part of the problem data. In such a scenario, it turns out that computing critical points amounts to calculating a bunch of scalars called Lagrange Multipliers. Here’s the formal statement of the fact: Theorem 3.22 (Lagrange Multipliers) Let \\(U\\subset\\mathbb{R}^n\\) be open, and let \\(\\boldsymbol{F}:U\\to\\mathbb{R}^m\\) be a \\(\\mathscr{C}^1\\) mapping defining a manifold \\(X\\), with \\(D\\boldsymbol{F}(\\boldsymbol{x})\\) onto for every \\(\\boldsymbol{x}\\in X\\). Let \\(f:U\\to\\mathbb{R}\\) be a \\(\\mathscr{C}^1\\) mapping. Then \\(\\boldsymbol{a}\\in X\\) is a critical point of \\(f|_{X}\\) if and only if there exist numbers \\(\\lambda_1, ..., \\lambda_m\\) such that \\[ \\begin{aligned} Df(\\boldsymbol{a}) &amp;= \\sum_{i = 1}^m \\lambda_i DF_i(\\boldsymbol{a}) \\end{aligned} \\] The numbers \\((\\lambda_i)_{i = 1}^m\\) are called Lagrange Multipliers. "],["integration.html", "3.9 Integration", " 3.9 Integration I’ll try to cover all fundamental definitions of the integral right from the basics. I feel this is important since different sources might treat the integral in different ways, so it’s better to have one coherent structure and go with it. 3.9.1 Preliminary definitions Definition 3.13 (Support of a function) Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\). Then the support of \\(f\\) is defined as \\[ \\begin{aligned} \\text{Supp}(f) := \\overline{\\left\\{\\boldsymbol{x}\\in\\mathbb{R}^n: f(\\boldsymbol{x})\\ne 0\\right\\}} \\end{aligned} \\] where the bar denotes the closure. Definition 3.14 (Oscillations) The oscillation of a function \\(f\\) over a set \\(A\\), denoted by \\(\\text{osc}_A(f)\\), is defined as \\[ \\begin{aligned} \\text{osc}_A(f) := M_A(f) - m_A(f) \\end{aligned} \\] where \\(M_A(f) := \\sup_{\\boldsymbol{x}\\in A}f(\\boldsymbol{x})\\), and \\(m_A(f)\\) is similarly defined with the supremum replaced by the infimum. When \\(|f|\\) is bounded and \\(f\\) has bounded support, all of the above quantities are well-defined. 3.9.2 Dyadic pavings With the oscillation now defined, we’ll define the so called dyadic pavings of \\(\\mathbb{R}^n\\); this is just for convenience. Infact we can use any kind of paving to define the integral. Definition 3.15 (Dyadic cube) A dyadic cube \\(C_{\\boldsymbol{k}, N}\\subset \\mathbb{R}^n\\) is defined as \\[ \\begin{aligned} C_{\\boldsymbol{k}, N} := \\left\\{\\boldsymbol{x}\\in\\mathbb{R}^n: \\frac{k_i}{2^N}\\le x_i\\le \\frac{k_i + 1}{2^N} \\text{ for }1\\le i\\le n\\right\\} \\end{aligned} \\] where \\(\\boldsymbol{k}\\in\\mathbb{Z}^n\\) is a vector of integers. The collection of all such cubes forms the \\(N\\)th dyadic paving of \\(\\mathbb{R}^n\\). The volume of such a cube is simply defined to be \\[ \\begin{aligned} \\text{vol}_n(C) := \\frac{1}{2^{Nn}} \\end{aligned} \\] Having defined this, the usual Riemann integral of a function is then defined to be the limit of upper/lower Riemann sums with respect to the dyadic paving. Note that, as part of the definition of Riemann integrability, we require the function in question to be both bounded and have bounded support. The Lebesgue integral is more suited to handle more general functions. 3.9.3 Two useful integral rules Here are two rules of integration which are quite useful in some proofs. Proposition 3.4 A bounded function \\(f\\) with bounded support is integrable if and only if both \\(f^+\\) and \\(f^-\\) are integrable. If \\(f\\) and \\(g\\) are integrable, then so are \\(\\sup(f, g)\\) and \\(\\inf(f, g)\\). If \\(f_1:\\mathbb{R}^n\\to\\mathbb{R}\\) and \\(f_2:\\mathbb{R}^m\\to\\mathbb{R}\\) are integrable, then the function \\[ \\begin{aligned} g(\\boldsymbol{x}, \\boldsymbol{y}) := f_1(\\boldsymbol{x})f_2(\\boldsymbol{y}) \\end{aligned} \\] on \\(\\mathbb{R}^{n + m}\\) is integrable, and \\[ \\begin{aligned} \\int_{\\mathbb{R}^{n + m}}g|d^n \\boldsymbol{x}||d^m \\boldsymbol{y}| &amp;= \\left(\\int_{\\mathbb{R}^n} f_1 |d^n \\boldsymbol{x}|\\right)\\left(\\int_{\\mathbb{R}^m}f_2 |d^m \\boldsymbol{y}|\\right) \\end{aligned} \\] 3.9.4 Volumes Definition 3.16 (Higher dimensional volume) For any set \\(A\\subset\\mathbb{R}^n\\), we define the volume of \\(A\\) as \\[ \\begin{aligned} \\text{vol}_n(A) := \\int_{\\mathbb{R}^n}\\boldsymbol{1}_A|d^n \\boldsymbol{x}| \\end{aligned} \\] whenever the indicator function is integrable. If the integral is well-defined, then \\(A\\) is said to be pavable. Proposition 3.5 (Volume is invariant under translation) Let \\(A\\) be any pavable subset of \\(\\mathbb{R}^n\\) and \\(\\boldsymbol{v}\\in \\mathbb{R}^n\\) by any vector. Then, \\(A + \\boldsymbol{v}\\) is pavable, and \\[ \\begin{aligned} \\text{vol}_n(A + \\boldsymbol{v}) &amp;= \\text{vol}_n(A) \\end{aligned} \\] Proposition 3.6 (Set with volume 0) A bounded set \\(X\\subset \\mathbb{R}^n\\) has volume \\(0\\) if and only if for every \\(\\epsilon &gt; 0\\) there exists some \\(N\\) such that \\[ \\begin{aligned} \\sum_{C\\in\\mathcal{D}_N(\\mathbb{R}^n)\\\\ C\\cap X\\ne \\phi} \\text{vol}_n(C) \\le \\epsilon \\end{aligned} \\] where above \\(\\mathcal{D}_N\\) is the set of all dyadic cubes at depth \\(N\\). Motivated by the above proposition, we can also define the notion of \\(k\\)-dimensional volume \\(0\\) in \\(\\mathbb{R}^n\\), where \\(k &lt; n\\). Definition 3.17 (Lower dimensional volume 0) A bounded subset \\(X\\subset \\mathbb{R}^n\\) has \\(k\\)-dimensional volume \\(0\\) if \\[ \\begin{aligned} \\lim_{N\\to\\infty} \\sum_{C\\in\\mathcal{D}_N(\\mathbb{R}^n)\\\\C\\cap X\\ne \\phi} \\left(\\frac{1}{2^N}\\right)^k&amp;= 0 \\end{aligned} \\] An arbitrary subset \\(X\\subset\\mathbb{R}^n\\) has \\(k\\)-dimensional volume \\(0\\) if for all \\(R &gt; 0\\), the bounded set \\(X\\cap B_R(\\boldsymbol{0})\\) has \\(k\\)-dimensional volume \\(0\\). We have the following useful theorem, which is intuitive, but very useful. Theorem 3.23 (Lower dimensional volumes of manifolds) If integers \\(m, k, n\\) satisfy \\(0\\le m &lt; k \\le n\\) and \\(M\\subset \\mathbb{R}^n\\) is a manifold of dimension \\(m\\), any closed subset \\(X\\subset M\\) has \\(k\\)-dimensional volume \\(0\\). Proposition 3.7 (Scaling volume) If \\(A\\subset\\mathbb{R}^n\\) has volume and \\(t\\in\\mathbb{R}\\), then the set \\(tA\\) has volume, and \\[ \\begin{aligned} \\text{vol}_n(tA) &amp;= |t|^n\\text{vol}_n(A) \\end{aligned} \\] 3.9.5 What functions can be integrated? Theorem 3.24 (Criterion for integrability) A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is integrable if and only if it is bounded with bounded support, and for all \\(\\epsilon &gt; 0\\), there exists \\(N\\) such that \\[ \\begin{aligned} \\sum_{C\\in\\mathcal{D}_N : \\text{osc}_C(f) &gt; \\epsilon} \\text{vol}_n(C) &lt; \\epsilon \\end{aligned} \\] where above, \\(\\mathcal{D}_N\\) is the set of all dyadic cubes at depth \\(N\\). As a consequence of this criterion for integrability of functions, the following facts about volumes can be proven: Proposition 3.8 (Bounded part of graph of integrable function has volume 0) Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) be an integrable function with graph \\(\\Gamma(f)\\), and let \\(C_0\\in\\mathbb{R}^n\\) be any dyadic cube. Then \\[ \\begin{aligned} \\text{vol}_{n + 1}(\\Gamma(f)\\cap (C_0\\times \\mathbb{R})) &amp;= 0 \\end{aligned} \\] Theorem 3.25 Any continuous function \\(\\mathbb{R}^n\\to\\mathbb{R}\\) with bounded support is integrable. Corollary 3.3 Let \\(X\\subset \\mathbb{R}^n\\) be compact and let \\(f:X\\to\\mathbb{R}\\) be continuous. Then the graph \\(\\Gamma_f\\subset \\mathbb{R}^{n + 1}\\) has volume \\(0\\). Theorem 3.26 A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\), bounded with bounded support, is integrable if it is continuous except on a set of volume \\(0\\). Corollary 3.4 Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) be integrable, and let \\(g:\\mathbb{R}^n\\to\\mathbb{R}\\) be a bounded function. If \\(f = g\\) except on a set of volume \\(0\\), then \\(g\\) is integrable and \\[ \\begin{aligned} \\int_{\\mathbb{R}^n}f|d^n \\boldsymbol{x}| &amp;= \\int_{\\mathbb{R}^n}g|d^n \\boldsymbol{x}| \\end{aligned} \\] 3.9.6 Measure zero The notion of measure zero is more important than the notion of volume zero from a theoretical viewpoint, since it allows a nice characterization of integrable functions. In the following definition, a box in \\(\\mathbb{R}^n\\) of sidelength \\(\\delta &gt; 0\\) is a cube of the form \\[ \\begin{aligned} \\left\\{\\boldsymbol{x}\\in\\mathbb{R}^n: a_i &lt; x_i &lt; a_i + \\delta, i = 1, \\cdots, n\\right\\} \\end{aligned} \\] Definition 3.18 (Measure 0) A set \\(X\\subset \\mathbb{R}^n\\) has measure \\(0\\) if for every \\(\\epsilon &gt; 0\\), there exists a countable sequence of open boxes \\(B_i\\) such that \\[ \\begin{aligned} X\\subset \\bigcup B_i \\text{ and } \\sum \\text{vol}_n (B_i) \\le \\epsilon \\end{aligned} \\] Theorem 3.27 A countable union of sets of measure \\(0\\) has measure \\(0\\). Corollary 3.5 Let \\(B_R(\\boldsymbol{0})\\) be the ball of radius \\(R\\) centered at the origin. If for all \\(R\\ge 0\\) the subset \\(X\\subset \\mathbb{R}^n\\) satisfies \\(\\text{vol}_n(X\\cap B_R(\\boldsymbol{0})) = 0\\), then \\(X\\) has measure \\(0\\). Proposition 3.9 Let \\(X\\) be a subspace of \\(\\mathbb{R}^n\\) of dimension \\(k &lt; n\\). Then \\(X\\) has measure \\(0\\), and any translate of \\(X\\) has measure \\(0\\). Proof. Clearly, \\(X\\) is a \\(k\\)-dimensional manifold. Hence, by Theorem 3.23, the \\(n\\)-dimensional volume of \\(X\\cap B_R(\\boldsymbol{0})\\) is \\(0\\) for all \\(R\\ge 0\\), and applying the previous corollary the claim follows. 3.9.7 Integrability of almost continuous functions Theorem 3.28 (What functions are integrable) Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) be bounded with bounded support. Then \\(f\\) is integrable if and only if it is continuous except on a set of measure \\(0\\). As a result of this theorem, we can also prove the following intuitive fact, but often very useful in computations. Theorem 3.29 (Boundaries have volume zero) Let \\(A\\subset\\mathbb{R}^n\\) be a set such that \\(\\partial A\\) has a well-defined \\(n\\)-dimensional volume. Then, \\(\\text{vol}_n(\\partial A) = 0\\), i.e the volume must be zero. Proof. By definition, since \\(\\partial A\\) has a volume, the indicator \\(\\boldsymbol{1}_{\\partial A}\\) is integrable, i.e it is continuous almost everywhere. If \\(S\\) is the set of points where \\(\\boldsymbol{1}_{\\partial A}\\) is continous, we claim that \\[ \\begin{aligned} S\\subset \\left\\{\\boldsymbol{x} : \\boldsymbol{x}\\notin \\partial A\\right\\} \\end{aligned} \\] But this is obvious: note that if \\(\\boldsymbol{1}_{\\partial A}\\) is continuous at some \\(\\boldsymbol{z}\\), then for a sufficiently small open ball around \\(\\boldsymbol{z}\\), all \\(\\boldsymbol{y}\\) in the ball will have \\(|\\boldsymbol{1}_{\\partial A}(\\boldsymbol{z}) - \\boldsymbol{1}_{\\partial A}(\\boldsymbol{y})|\\) to be arbitrarily small, which will further imply that \\(\\boldsymbol{1}_{\\partial A}(\\boldsymbol{z}) = \\boldsymbol{1}_{\\partial A}(\\boldsymbol{y})\\) for all \\(\\boldsymbol{y}\\) in that open ball. Clearly, this must mean that \\(\\boldsymbol{z}\\notin \\partial A\\), since for every neighborhood of a point in the boundary of a set, we can find points in the set and points not in the set. This proves the claim. Taking compliments, the claim implies that \\(\\partial A\\subset S^c\\), where \\(S^c\\) is the set of discontinuities. Since \\(S^c\\) has measure zero (integrability), it follows that \\(\\partial A\\) also has measure \\(0\\). Since \\(\\text{vol}_n(\\partial A)\\) is well defined, this proves that \\(\\text{vol}_n(\\partial A) = 0\\). 3.9.8 Fubini’s Theorem and Iterated Integrals This is one of the most important tools to compute higher dimensional integrals. Essentially, Fubini’s Theorem reduces the problem of computing higher-dimensional integrals to the problem of single variable integrals. Theorem 3.30 (Fubini's Theorem) Let \\(f:\\mathbb{R}^n\\times \\mathbb{R}^m\\to\\mathbb{R}\\) be an integrable function. For a fixed \\(\\boldsymbol{x}\\), let the function \\(f_{\\boldsymbol{x}}:\\mathbb{R}^m\\to\\mathbb{R}\\) be defined by \\(\\boldsymbol{y}\\mapsto f(\\boldsymbol{x}, \\boldsymbol{y})\\), and let \\(f^{\\boldsymbol{y}}:\\mathbb{R}^n\\to\\mathbb{R}\\) be similarly defined. Then, the functions \\(U(f_{\\boldsymbol{x}})\\), \\(L(f_{\\boldsymbol{x}})\\), \\(U(f^{\\boldsymbol{y}})\\), \\(L(f^{\\boldsymbol{y}})\\) are all integrable, and \\[ \\begin{aligned} \\int_{\\mathbb{R}^n} U(f_{\\boldsymbol{x}}) |d^n \\boldsymbol{x}| &amp;= \\int_{\\mathbb{R}^n} L(f_{\\boldsymbol{x}})|d^n \\boldsymbol{x}| = \\int_{\\mathbb{R}^m} U(f_{\\boldsymbol{y}}) |d^n \\boldsymbol{y}| = \\int_{\\mathbb{R}^m} L(f_{\\boldsymbol{y}})|d^n \\boldsymbol{y}| = \\int_{\\mathbb{R}^n\\times\\mathbb{R}^m} f|d^n \\boldsymbol{x}||d^m \\boldsymbol{y}| \\end{aligned} \\] Here, \\(U\\) and \\(L\\) denote the upper and lower integrals of the respective functions. As a corollary of this theorem, we have the following useful facts: Corollary 3.6 The set of \\(\\boldsymbol{x}\\) such that \\(U(f_{\\boldsymbol{x}})\\ne L(f_{\\boldsymbol{x}})\\) and the set of \\(\\boldsymbol{y}\\) such that \\(U(f^\\boldsymbol{y})\\ne L(f^\\boldsymbol{y})\\) both have measure \\(0\\). Thus, the set of \\(\\boldsymbol{x}\\) such that \\(f_{\\boldsymbol{x}}\\) is not integrable has \\(n\\)-dimensional measure \\(0\\), and the set of \\(\\boldsymbol{y}\\) where \\(f^{\\boldsymbol{y}}\\) is not integrable has \\(m\\)-dimensional measure \\(0\\). 3.9.9 General Pavings While dyadic pavings simplify proofs and give us a concrete paving to work with, nothing stops us from developing the same theory using other pavings. Infact, we’ll give precise definitions of what a paving really is, and we’ll mention theorems stating that it doesn’t matter which paving we really use to develop the theory of integration. Definition 3.19 (Pavings) A paving of a subset \\(X\\subset \\mathbb{R}^n\\) is a collection \\(\\mathcal{P}\\) of bounded subsets \\(P\\subset X\\) such that the following hold: \\(\\bigcup_{P\\in\\mathcal{P}}P = X\\). \\(\\text{vol}_n(P_1\\cap P_2) = 0\\) when \\(P_1, P_2\\in\\mathcal{P}\\) and \\(P_1\\ne P_2\\). Any bounded subset of \\(X\\) intersects only finitely many \\(P\\in\\mathcal{P}\\). For all \\(P\\in\\mathcal{P}\\), we have \\(\\text{vol}_n(\\partial P) = 0\\). For any bounded function \\(f\\) with bounded support, we can define an upper sum \\(U_{\\mathcal{P}_N}(f)\\) and a lower sum \\(L_{\\mathcal{P}_N}(f)\\) with respect to any paving: \\[ \\begin{aligned} U_{\\mathcal{P}_N}(f) := \\sum_{P\\in\\mathcal{P}_N} M_P(f)\\text{vol}_n(P)\\text{ and } U_{\\mathcal{P}_N}(f) := \\sum_{P\\in\\mathcal{P}_N} m_P(f)\\text{vol}_n(P) \\end{aligned} \\] Definition 3.20 (Nested Partitions) A sequence \\(\\mathcal{P}_N\\) of pavings of \\(X\\subset\\mathbb{R}^n\\) is called a nested partition of \\(X\\) if the following hold: \\(\\mathcal{P}_{N + 1}\\) is a refinement of \\(\\mathcal{P}_N\\); i.e, every piece of \\(\\mathcal{P}_{N + 1}\\) is contained in a piece of \\(\\mathcal{P}_N\\). The pieces of \\(\\mathcal{P}_N\\) shrink to points as \\(N\\to\\infty\\), i.e \\[ \\begin{aligned} \\lim_{N\\to\\infty} \\sup_{P\\in\\mathcal{P}_N} \\text{diam}(P) = 0 \\end{aligned} \\] Theorem 3.31 (Integrals using arbitrary pavings) Let \\(X\\subset\\mathbb{R}^n\\) be a bounded subset, and let \\(\\mathcal{P}_N\\) be a nested partition of \\(X\\). If \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is integrable, then the limits \\[ \\begin{aligned} \\lim_{N\\to\\infty}U_{\\mathcal{P}_N}(f\\boldsymbol{1}_X)\\text{ and }\\lim_{N\\to\\infty}L_{\\mathcal{P}_N}(f\\boldsymbol{1}_X) \\end{aligned} \\] exist and are both equal to \\[ \\begin{aligned} \\int_X f(\\boldsymbol{x})|d^n \\boldsymbol{x}| := \\int_{\\mathbb{R}^n} f(\\boldsymbol{x})\\boldsymbol{1}_X(\\boldsymbol{x})|d^n \\boldsymbol{x}| \\end{aligned} \\] Conversely, if the mentioned limits are equal, then \\(f \\boldsymbol{1}_X\\) is integrable and \\[ \\begin{aligned} \\int_{\\mathbb{R}^n} f(\\boldsymbol{x})\\boldsymbol{1}_X(\\boldsymbol{x})|d^n \\boldsymbol{x}| \\end{aligned} \\] is equal to the common limit. 3.9.10 Volumes and determinants Theorem 3.32 (Determinant scales volumes) Let \\(T:\\mathbb{R}^n\\to\\mathbb{R}^n\\) be a linear transformation given by the matrix \\([T]\\). Then for any pavable set \\(A\\subset\\mathbb{R}^n\\), the image \\(T(A)\\) is pavable, and \\[ \\begin{aligned} \\text{vol}_n T(A) &amp;= |\\det[T]|\\text{vol}_n A \\end{aligned} \\] Definition 3.21 (k-parallelograms) Let \\(\\boldsymbol{v}_1, ..., \\boldsymbol{v}_k\\) be \\(k\\) vectors in \\(\\mathbb{R}^n\\). The \\(k\\)-parallelogram spanned by these vectors is the set of all \\[ \\begin{aligned} t_1 \\boldsymbol{v}_1 + \\cdots + t_k \\boldsymbol{v}_k \\end{aligned} \\] with \\(0\\le t_i\\le 1\\) for all \\(1\\le i\\le k\\). It is denoted \\(P(\\boldsymbol{v}_1, ..., \\boldsymbol{v}_k)\\). An application of 3.32 gives the following result. Proposition 3.10 (Volume of parallelograms) Let \\(\\boldsymbol{v}_1,...,\\boldsymbol{v}_n\\) be vectors in \\(\\mathbb{R}^n\\). Then \\[ \\begin{aligned} \\text{vol}_n P(\\boldsymbol{v}_1,...,\\boldsymbol{v}_n) &amp;= |\\det[\\boldsymbol{v}_1, ..., \\boldsymbol{v}_n]| \\end{aligned} \\] 3.9.10.1 Linear change of variables We’ll state a more general change of variables theorem. Here’s a simpler version of the theorem, where we use an invertible linear transformation as our substitution. Theorem 3.33 (Linear change of variables) Let \\(T:\\mathbb{R}^n\\to\\mathbb{R}^n\\) be an invertible linear transformation, and let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) be an integrable function. Then \\(f\\circ T\\) is integrable, and \\[ \\begin{aligned} \\int_{\\mathbb{R}^n} f(\\boldsymbol{y}) |d^n \\boldsymbol{y}| &amp;= |\\det T| \\int_{\\mathbb{R}^n} f(T(\\boldsymbol{x})) |d^n \\boldsymbol{x}| \\end{aligned} \\] 3.9.11 The general change of variables formula Theorem 3.34 (Change of variables formula) Let \\(X\\) be a compact subset of \\(\\mathbb{R}^n\\) with boundary \\(\\partial X\\) of volume \\(0\\); let \\(U\\subset \\mathbb{R}^n\\) be an open set containing \\(X\\). Let \\(\\boldsymbol{\\Phi}:U\\to\\mathbb{R}^n\\) be a \\(\\mathscr{C}^1\\) map that is injective on \\((X - \\partial X)\\) and has Lipschitz derivative, with \\([D\\boldsymbol{\\Phi}(\\boldsymbol{x})]\\) invertible at every \\(\\boldsymbol{x}\\in (X - \\partial X)\\). Set \\(Y = \\boldsymbol{\\Phi}(X)\\). Then, if \\(f:Y\\to\\mathbb{R}\\) is integrable, \\(f\\circ \\boldsymbol{\\Phi}|\\det [D \\boldsymbol{\\Phi}]|\\) is integrable on \\(X\\), and \\[ \\begin{aligned} \\int_Y f(\\boldsymbol{y})|d^n \\boldsymbol{y}| &amp;= \\int_X (f\\circ \\boldsymbol{\\Phi})(\\boldsymbol{x})|\\det [D \\boldsymbol{\\Phi}(\\boldsymbol{x})]||d^n \\boldsymbol{x}| \\end{aligned} \\] 3.9.12 Common change of variable transformations and coordinate systems 3.9.12.1 Polar coordinates Consider the compact subset \\(X = [0, R]\\times [0, 2\\pi]\\) of \\(\\mathbb{R}^2\\). Let \\(U\\) be the disc centered at the origin of radius \\(R + 2\\pi\\); so, \\(U\\) is an open set containing \\(X\\). Clearly, the boundary \\(\\partial X\\) has \\(2\\)-dimensional volume \\(0\\). The polar coordinate map \\(\\mathbb{R}^2\\to\\mathbb{R}^2\\) maps a point \\((r, \\theta)\\) to a point \\((x, y)\\) via the following transformation: \\[ \\begin{aligned} (r, \\theta) \\mapsto (r\\cos\\theta, r\\sin\\theta) \\end{aligned} \\] It is easy to see that the map \\(P\\) is \\(\\mathscr{C}^\\infty\\). Moreover, the map \\(P\\) is injective on \\((X - \\partial X)\\), and the derivative of \\(P\\) is given by \\[ \\begin{aligned} DP(r, \\theta) &amp;= \\begin{bmatrix} \\cos\\theta &amp; \\sin\\theta\\\\ -r\\sin\\theta &amp; r\\cos\\theta\\\\ \\end{bmatrix} \\end{aligned} \\] In particular, \\(\\det DP(r, \\theta) = r\\), and hence \\(DP(r, \\theta)\\) is invertible in \\((X - \\partial X)\\). Next, we check that \\(DP\\) is Lipschitz, where we restrict the domain of \\(P\\) to \\(U\\). To that end, we consider \\(DP:U\\to\\mathbb{R}^4\\) defined by \\[ \\begin{aligned} (r, \\theta) \\mapsto (\\cos\\theta, \\sin\\theta, r\\cos\\theta, r\\sin\\theta) \\end{aligned} \\] In that case, note that \\[ \\begin{aligned} D^2P(r, \\theta) &amp;= \\begin{bmatrix} 0 &amp; 0 &amp; \\cos\\theta &amp; \\sin\\theta \\\\ -\\sin\\theta &amp; \\cos\\theta &amp; -r\\sin\\theta &amp; r\\cos\\theta \\\\ \\end{bmatrix} \\end{aligned} \\] In particular, we see that \\[ \\begin{aligned} \\lVert D^2P(r, \\theta)\\rVert^2 &amp;= 1 + 1 + r^2 \\end{aligned} \\] Hence, the above norm is bounded for all points in \\(U\\), since \\(U\\) is itself bounded. So, we’ve proven that the map \\(P\\) satisfies all conditions of Theorem 3.34. In particular, we have proven that for any integrable function \\(f:\\mathbb{R}^2\\to\\mathbb{R}\\) (bounded with bounded support, by definition), we have \\[ \\begin{aligned} \\int_{\\mathbb{R}^2} f(x, y) |dx dy| &amp;= \\int_{[0, R]\\times [0, 2\\pi]} f(r\\cos \\theta, r\\sin\\theta) r|dr d\\theta| \\end{aligned} \\] 3.9.12.2 Spherical coordinates The spherical coordinate map maps a point \\((r, \\theta,\\varphi)\\) to the point \\((r\\cos\\theta\\cos\\varphi, r\\sin\\theta\\cos\\varphi, r\\sin\\varphi)\\). We can repeat a similar analysis as in the case of the polar coordinates map to derive a change of variables formula for \\(S\\). In this case, the compact subset \\(X\\) which we’ll have to consider will be \\(X = [0, R]\\times [-\\pi/2, \\pi/2]\\times [0, 2\\pi]\\). 3.9.13 Lebesgue Integrals The following definitions and motivation behind the Lebesgue integral have been taken from (John H. Hubbard 2009). This approach is quite different from the standard one, where Lebesgue integrals are defined from the ground up using the notion of simple functions. Nonetheless, this approach is better suited for the computation of Lebesgue integrals, and is equivalent to the standard approach. 3.9.13.1 Integrals and Limits One of the motivating properties behind defining the Lebesgue integral is that the Lebesgue integral behaves really well w.r.t limits. For the standard Riemann integral, uniform convergence is required, which is often not satisfied. Theorem 3.35 (Convergence for Riemann integrals) Let \\((f_k)\\) be a sequence of integrable functions \\(\\mathbb{R}^n\\to\\mathbb{R}\\), all with support in a fixed bal \\(B\\subset\\mathbb{R}^n\\), and converging uniformly to a function \\(f\\). Then \\(f\\) is integrable, and \\[ \\begin{aligned} \\lim_{k\\to\\infty}\\int_{\\mathbb{R}^n}f_k(\\boldsymbol{x})|d^n \\boldsymbol{x}| &amp;= \\int_{\\mathbb{R}^n} f(\\boldsymbol{x})|d^n \\boldsymbol{x}| \\end{aligned} \\] We also state the dominated convergence theorem for Riemann integrals, which requires one to assume that the limit function is itself integrable. Theorem 3.36 (Dominated convergence theorem for Riemann integrals) Let \\(f_k:\\mathbb{R}^n\\to\\mathbb{R}\\) be a sequence of Riemann integrable functions. Suppose there exists \\(R &gt; 0\\) such that all \\(f_k\\) have their support in \\(B_R(\\boldsymbol{0})\\) and satisfy \\(|f_k|\\le R\\). Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) be a Riemann integrable function such that \\(f(\\boldsymbol{x}) = \\lim_{k\\to\\infty} f_k(\\boldsymbol{x})\\) almost everywhere. Then \\[ \\begin{aligned} \\lim_{k\\to\\infty}\\int_{\\mathbb{R}^n} f_k(\\boldsymbol{x})|d^n \\boldsymbol{x}| &amp;= \\int_{\\mathbb{R}^n} f(\\boldsymbol{x})|d^n \\boldsymbol{x}| \\end{aligned} \\] 3.9.13.2 Lebesgue equality and the Lebesgue integral as a limit of Riemann integrals The following theorem will be our basis of defining higher dimensional Lebesgue integrals. Theorem 3.37 (Convergence except on a set of measure 0) If \\(f_k\\) is a sequence of Riemann-integrable functions on \\(\\mathbb{R}^n\\) such that \\[ \\begin{aligned} \\sum_{k = 1}^\\infty \\int_{\\mathbb{R}^n} |f_k(\\boldsymbol{x})| |d^n \\boldsymbol{x}|&lt; \\infty \\end{aligned} \\] then the series \\(\\sum_{k = 1}^\\infty f_k(\\boldsymbol{x})\\) converges almost everywhere. Motivated by this fact, we define our version of the Lebesgue integral. Theorem 3.38 (Lebesgue integral) Let \\((f_k)\\) and \\((g_k)\\) be two sequences of Riemann integrable functions \\(\\mathbb{R}^n\\to\\mathbb{R}\\) such that \\[ \\begin{aligned} \\sum_{k = 1}^\\infty \\int_{\\mathbb{R}^n} |f_k(\\boldsymbol{x})| |d^n \\boldsymbol{x}|&lt; \\infty \\text{ and } \\sum_{k = 1}^\\infty \\int_{\\mathbb{R}^n} |g_k(\\boldsymbol{x})| |d^n \\boldsymbol{x}|&lt; \\infty \\end{aligned} \\] and that \\[ \\begin{aligned} \\sum_{k = 1}^\\infty f_k = \\sum_{k = 1}^\\infty g_k \\end{aligned} \\] almost everywhere. Then \\[ \\begin{aligned} \\sum_{k = 1}^\\infty \\int_{\\mathbb{R}^n} |f_k(\\boldsymbol{x})| |d^n \\boldsymbol{x}|= \\sum_{k = 1}^\\infty \\int_{\\mathbb{R}^n} |g_k(\\boldsymbol{x})| |d^n \\boldsymbol{x}| \\end{aligned} \\] In that case, we define the Lebesgue integral of \\(f := \\sum_{k = 1}^\\infty f_k\\) by \\[ \\begin{aligned} \\int_{\\mathbb{R}^n} f(\\boldsymbol{x})|d^n \\boldsymbol{x}| := \\sum_{k = 1}^\\infty \\int_{\\mathbb{R}^n} f_k(\\boldsymbol{x})|d^n \\boldsymbol{x}| \\end{aligned} \\] 3.9.13.3 Important properties of the Lebesgue integral Proposition 3.11 The following hold: The Lebesgue integral is linear. If \\(f\\) is Lebesgue integrable on \\(\\mathbb{R}^n\\), and \\(g\\) is Riemann integrable, then \\(fg\\) is Lebesgue integrable. If \\(f\\) and \\(g\\) are Lebesgue integrable on \\(\\mathbb{R}^n\\) and if \\(f\\le g\\) almost everywhere, then \\[ \\begin{aligned} \\int_{\\mathbb{R}^n}f(\\boldsymbol{x})|d^n \\boldsymbol{x}| \\le \\int_{\\mathbb{R}^n}g(\\boldsymbol{x})|d^n \\boldsymbol{x}| \\end{aligned} \\] 3.9.13.4 Important theorems about Lebesgue integrals Theorem 3.39 (A first limit theorem for Lebesgue integrals) Let \\((f_k)\\) be a sequence of Lebesgue integrable functions such that \\[ \\begin{aligned} \\sum_{k = 1}^\\infty \\int_{\\mathbb{R}^n} |f_k(\\boldsymbol{x})||d^n \\boldsymbol{x}| &lt; \\infty \\end{aligned} \\] Then \\(f(\\boldsymbol{x}) := \\sum_{k = 1}^\\infty f_k(\\boldsymbol{x})\\) exists almost everywhere, the function \\(f\\) is Lebesgue integrable, and \\[ \\begin{aligned} \\int_{\\mathbb{R}^n} f(\\boldsymbol{x})|d^n \\boldsymbol{x}| &amp;= \\sum_{k = 1}^\\infty \\int_{\\mathbb{R}^n} f_k(\\boldsymbol{x})|d^n \\boldsymbol{x}| \\end{aligned} \\] Theorem 3.40 (Monotone convergence theorem) Let \\(0\\le f_1\\le f_2\\le \\cdots\\) be a sequence of Lebesgue integrable functions, where each inequality holds almost everywhere. If \\[ \\begin{aligned} \\sup_{k} \\int_{\\mathbb{R}^n} f_k(\\boldsymbol{x})|d^n \\boldsymbol{x}| &lt; \\infty \\end{aligned} \\] then the limit \\(f(\\boldsymbol{x}) := \\lim_{k\\to\\infty} f_k(\\boldsymbol{x})\\) exists for almost all \\(\\boldsymbol{x}\\), the function \\(f\\) is Lebesgue integrable, and \\[ \\begin{aligned} \\int_{\\mathbb{R}^n} f(\\boldsymbol{x})|d^n \\boldsymbol{x}| = \\sup_{k} \\int_{\\mathbb{R}^n} f_k(\\boldsymbol{x})|d^n \\boldsymbol{x}| \\end{aligned} \\] Conversely, if \\(f\\) exists almost everywhere and \\(\\sup_{k}\\int_{\\mathbb{R}^n} f_k(\\boldsymbol{x})|d^n \\boldsymbol{x}| = \\infty\\), then \\(f\\) is not Lebesgue integrable. Theorem 3.41 (Dominated convergence theorem) Let \\((f_k)\\) be a sequence of Lebesgue integrable functions that converges pointwise almost everywhere to some function \\(f\\). Suppose there is some Lebesgue integrable function \\(F:\\mathbb{R}^n\\to\\mathbb{R}\\) such that \\(|f_k(\\boldsymbol{x})|\\le F(\\boldsymbol{x})\\) for almost all \\(\\boldsymbol{x}\\). Then \\(f\\) is Lebesgue integrable and \\[ \\begin{aligned} \\int_{\\mathbb{R}^n} f(\\boldsymbol{x})|d^n \\boldsymbol{x}| &amp;= \\lim_{k\\to\\infty} \\int_{\\mathbb{R}^n} f_k(\\boldsymbol{x}) |d^n \\boldsymbol{x}| \\end{aligned} \\] Theorem 3.42 (Change of variables for Lebesgue integrals) Let \\(U, V\\) be open subsets of \\(\\mathbb{R}^n\\), and let \\(\\boldsymbol{\\Phi}:U\\to V\\) be bijective, of class \\(\\mathscr{C}^1\\), with inverse of class \\(\\mathscr{C}^1\\), such that both \\(\\boldsymbol{\\Phi}\\) and \\(\\boldsymbol{\\Phi}^{-1}\\) have Lipschitz derivatives. Let \\(f:V\\to\\mathbb{R}\\) be defined except perhaps on a set of measure \\(0\\). Then \\(f\\) is Lebesgue integrable on \\(V\\) if and only if \\(f\\circ \\boldsymbol{\\Phi}(\\det [D\\boldsymbol{\\Phi}])\\) is Lebesgue integrable on \\(U\\), and \\[ \\begin{aligned} \\int_{V} f(\\boldsymbol{v}) |d^n \\boldsymbol{v}| &amp;= \\int_{U} (f\\circ \\boldsymbol{\\Phi})(\\boldsymbol{u})|\\det [D\\boldsymbol{\\Phi}(\\boldsymbol{u})]| |d^n \\boldsymbol{u}| \\end{aligned} \\] Theorem 3.43 (Fubini's theorem for Lebesgue integrals) Let \\(f:\\mathbb{R}^n\\times \\mathbb{R}^m\\to\\mathbb{R}\\) be a Lebesgue integrable function. Then the function \\[ \\begin{aligned} \\boldsymbol{y} \\mapsto \\int_{\\mathbb{R}^n} f(\\boldsymbol{x}, \\boldsymbol{y})|d^n \\boldsymbol{x}| \\end{aligned} \\] is defined for almost all \\(\\boldsymbol{y}\\in\\mathbb{R}^m\\), is Lebesgue integrable on \\(\\mathbb{R}^m\\), and \\[ \\begin{aligned} \\int_{\\mathbb{R}^n\\times\\mathbb{R}^m} f(\\boldsymbol{x}, \\boldsymbol{y}) |d^n \\boldsymbol{x}||d^m \\boldsymbol{y}| &amp;= \\int_{\\mathbb{R}^m}\\int_{\\mathbb{R}^n} f(\\boldsymbol{x}, \\boldsymbol{y})|d^n \\boldsymbol{x}||d^m \\boldsymbol{y}| \\end{aligned} \\] Conversely, if \\(f:\\mathbb{R}^n\\times\\mathbb{R}^m\\to\\mathbb{R}\\) is a function such that every point \\((\\boldsymbol{x}, \\boldsymbol{y})\\in\\mathbb{R}^{n + m}\\) is the center of a ball on which \\(f\\) is Lebesgue integrable The function \\(\\boldsymbol{x}\\to |f(\\boldsymbol{x}, \\boldsymbol{y})|\\) is Lebesgue integrable on \\(\\mathbb{R}^n\\) for almost all \\(\\boldsymbol{y}\\) The function \\(\\boldsymbol{y}\\to \\int_{\\mathbb{R}^n} |f(\\boldsymbol{x}, \\boldsymbol{y})||d^n \\boldsymbol{x}|\\) is Lebesgue integrable on \\(\\mathbb{R}^m\\). Then \\(f\\) is Lebesgue integrable on \\(\\mathbb{R}^{n + m}\\), and the usual equation of Fubini’s Theorem holds. Quite often, one needs to differentiate functions which are expressed as integrals. This is usually referred to as differentiating under the integral sign, and there are many theorems related to when this is possible. We state one such theorem below. Theorem 3.44 (Differentiating under the integral) Let \\(f(t, \\boldsymbol{x}):\\mathbb{R}^{n + 1}\\to\\mathbb{R}\\) be a function such that for each fixed \\(t\\), the integral \\[ \\begin{aligned} F(t) := \\int_{\\mathbb{R}^n} f(t, \\boldsymbol{x})|d^n \\boldsymbol{x}| \\end{aligned} \\] exists. Suppose \\(D_tf\\) exists for almost all \\(\\boldsymbol{x}\\). If there exists \\(\\epsilon &gt; 0\\) and a Lebesgue integrable function \\(g\\) such that for all \\(s\\ne t\\), \\[ \\begin{aligned} |s - t| &lt; \\epsilon \\implies \\left|\\frac{f(s, \\boldsymbol{x}) - f(t, \\boldsymbol{x})}{s - t}\\right| \\le g(\\boldsymbol{x}) \\end{aligned} \\] then \\(F\\) is differentiable, and it’s derivative is given by \\[ \\begin{aligned} F&#39;(t) &amp;= \\int_{\\mathbb{R}^n} D_t f(t, \\boldsymbol{x}) |d^n \\boldsymbol{x}| \\end{aligned} \\] References John H. Hubbard, Barbara Burke Hubbard. 2009. Vector Calculus, Linear Algebra and Differential Forms. Fifth. Matrix Editions. "],["volume-of-manifolds.html", "3.10 Volume of manifolds", " 3.10 Volume of manifolds In the section on integration, we briefly discussed volumes of manifolds. This chapter will mostly cover more details about volumes of manifolds. 3.10.1 Parallelograms and their volumes We start with a simple formula, which allows us to extend the definition of volumes to \\(k\\)-parallelograms in \\(\\mathbb{R}^n\\). Proposition 3.12 (Volume of a parallelogram) Let \\(\\boldsymbol{v}_1, ..., \\boldsymbol{v}_k\\) be \\(k\\) vectors in \\(\\mathbb{R}^k\\), and let \\(T = [\\boldsymbol{v}_1, ..., \\boldsymbol{v}_k]\\) be the corresponding \\(k\\times k\\) square matrix. Then \\[ \\begin{aligned} \\text{vol}_kP(\\boldsymbol{v}_1, ..., \\boldsymbol{v}_k) &amp;= \\sqrt{\\det(T^TT)} \\end{aligned} \\] Note that the above formula agrees with the one Proposition 3.10, but it also allows us to define \\(k\\)-dimensional volumes of manifolds in \\(\\mathbb{R}^n\\), even when \\(k &lt; n\\). Definition 3.22 (General volumes of parallelograms) Let \\(T = [\\boldsymbol{v}_1, ... \\boldsymbol{v}_k]\\) be an \\(n\\times k\\) matrix. Then the \\(k\\)-dimensional volume of \\(P(\\boldsymbol{v}_1, ..., \\boldsymbol{v}_k)\\) (the parallelogram spanned by the vectors) is defined to be \\[ \\begin{aligned} \\text{vol}_k P(\\boldsymbol{v}_1, ..., \\boldsymbol{v}_k) := \\sqrt{\\det(T^TT)} \\end{aligned} \\] 3.10.2 Parametrizations Next, we’ll define a relaxed version of parametrizations of manifolds, which will help us compute volumes. The main reason behind not using the more stricter version of 3.2 is to allow things to go wrong on sets of volume \\(0\\), which are irrelevant when computing volumes. Definition 3.23 (Relaxed parametrizations of manifolds) Let \\(M\\subseteq\\mathbb{R}^n\\) be a \\(k\\)-dimensional manifold and let \\(U\\subset\\mathbb{R}^k\\) be a subset with boundary of \\(k\\)-dimensional volume \\(0\\). Let \\(X\\subset U\\) be such that \\(U - X\\) is open. Then a continuous mapping \\(\\gamma:U\\to\\mathbb{R}^n\\) parametrizes \\(M\\) if \\(\\gamma(U)\\supset M\\); \\(\\gamma(U - X)\\subset M\\); \\(\\gamma:(U - X)\\to M\\) is one to one, and of class \\(\\mathscr{C}^1\\); the derivative \\(D\\gamma(\\boldsymbol{u})\\) is one to one for all \\(\\boldsymbol{u}\\) in \\(U - X\\). \\(X\\) has \\(k\\)-dimensional volume \\(0\\), as does \\(\\gamma(X)\\cap C\\) for any compact subset \\(C\\subset M\\). Theorem 3.45 (Existence of relaxed parametrizations) All manifolds can be parametrized with a relaxed parametrization. 3.10.3 Defining volumes of manifolds Let \\(M\\) be a \\(k\\)-dimensional manifold embedded in \\(\\mathbb{R}^n\\). We’ll use the definition of the \\(k\\)-dimensional volume of a \\(k\\)-parallelogram in \\(\\mathbb{R}^n\\), along with the change of variables formula, as motivation to define generalized volumes of manifolds such as \\(M\\). We denote the volume of \\(M\\) by the following notation: \\[ \\begin{aligned} \\text{vol}_k M &amp;= \\int_M |d^k \\boldsymbol{x}| \\end{aligned} \\] The integrand \\(|d^k \\boldsymbol{x}|\\) is called the \\(k\\)-th dimensional element of volume. Intuitively, we define volumes of manifolds via their relaxed parametrizations (which we know exist for all manifolds). Then, to compute the volume, we compute values of \\(k\\)-parallelograms spanned by the partial derivatives of \\(\\gamma\\), sum them, and take the limit as the tiling becomes finer. Theorem 3.46 (Volume of a manifold) Let \\(M\\subset\\mathbb{R}^n\\) be a smooth \\(k\\)-dimensional manifold, \\(U\\) a pavable subset of \\(\\mathbb{R}^k\\), and let \\(\\gamma: U\\to M\\) be a relaxed parametrization of \\(M\\). Let \\(X\\) be as in the definition of the relaxed parametrization. Then \\[ \\begin{aligned} \\text{vol}_k M := \\int_{U - X} \\sqrt{\\det(D\\gamma(\\boldsymbol{u})^T D\\gamma(\\boldsymbol{u}))}|d^k \\boldsymbol{u}| \\end{aligned} \\] Using this notion, we can also define integrals over manifolds w.r.t volumes. Definition 3.24 (Integrals over manifolds w.r.t volumes) Let \\(M\\subset \\mathbb{R}^n\\) be a smooth \\(k\\)-dimensional manifold, \\(U\\) be a pavable subset of \\(\\mathbb{R}^k\\), and \\(\\gamma: U\\to M\\) be a relaxed parametrization. Let \\(X\\) be as in the definition of a relaxed parametrization. Then, \\(f:M\\to\\mathbb{R}\\) is said to be integrable over \\(M\\) with respect to volume if the following integral exists: \\[ \\begin{aligned} \\int_{M} f(\\boldsymbol{x})|d^k \\boldsymbol{x}| &amp;= \\int_{U - X} f(\\gamma(\\boldsymbol{u}))\\sqrt{\\det(D\\gamma(\\boldsymbol{u})^TD\\gamma(\\boldsymbol{u}))}|d^k \\boldsymbol{u}| \\end{aligned} \\] The following fact ensures that it doesn’t matter which parametrization we use: Proposition 3.13 (Integrals are independent of parametrizations) Let \\(M\\) be a \\(k\\)-dimensional manifold in \\(\\mathbb{R}^n\\) and \\(f:M\\to\\mathbb{R}\\) a function. Let \\(U_1\\), \\(U_2\\) be subsets of \\(\\mathbb{R}^k\\), and let \\(\\gamma_1: U_1\\to M\\), \\(\\gamma_2:U_2\\to M\\) be two parametrizations of \\(M\\). Then \\[ \\begin{aligned} \\int_{U_1 - X_1} f(\\gamma_1(\\boldsymbol{u})) \\sqrt{\\det(D\\gamma_1(\\boldsymbol{u})^T D\\gamma_1(\\boldsymbol{u}))}|d^k \\boldsymbol{u}| \\end{aligned} \\] exists if and only if the integral \\[ \\begin{aligned} \\int_{U_2 - X_2} f(\\gamma_2(\\boldsymbol{u})) \\sqrt{\\det(D\\gamma_2(\\boldsymbol{u})^T D\\gamma_2(\\boldsymbol{u}))}|d^k \\boldsymbol{u}| \\end{aligned} \\] exists, and in that case, both the integrals are equal. "],["forms-and-vector-calculus.html", "3.11 Forms and vector calculus", " 3.11 Forms and vector calculus (John H. Hubbard 2009) is, as mentioned before, the book on which the definitions and theory here are based. However, (Arnold 1989) is one of the best sources for understanding the intuition and applications of many of these concepts. 3.11.1 Forms on \\(\\mathbb{R}^n\\) Intuitively, forms are just analogs of the determinant, i.e they are multilinear alternating functions. The only difference is that forms in \\(\\mathbb{R}^n\\) may operate on fewer than \\(n\\) vectors (recall that the determinant is only defined for square matrices). Forms, just like the derivative, are useful since they have connections to signed volumes. Theorem 3.47 (k-forms) A \\(k\\)-form on \\(\\mathbb{R}^n\\) is a function \\(\\varphi:\\mathbb{R}^n\\to\\mathbb{R}\\) which is multilinear and antisymmetric, i.e switching two arguments in \\(\\varphi\\) switches the sign of the output. Example 3.1 (Standard forms) Let \\(i_1, ..., i_k\\) be any \\(k\\) integers in the set \\([1, n]\\). We define a form \\(dx_{i_1}\\land \\cdots\\land dx_{i_k}\\) which takes \\(k\\) vectors \\(\\boldsymbol{v}_1,..., \\boldsymbol{v}_k\\) in \\(\\mathbb{R}^n\\), stacks these vectors horizontally to make an \\(n\\times k\\) matrix, and selects the \\(k\\) rows of this matrix indexed by \\(i_1,...,i_k\\) (in order), and computes the determinant of the resultant matrix. It is clear that this form is multilinear and antisymmetric. A \\(0\\)-form can be identified by a constant \\(\\in\\mathbb{R}\\). It is also straightforward to see that if \\(k &gt; n\\), any \\(k\\)-form is the trivial form, i.e the zero function. 3.11.2 A geometric interpretation of standard forms Consider a \\(k\\)-form \\(\\varphi = dx_{i_1}\\land \\cdots\\land dx_{i_k}\\) in \\(\\mathbb{R}^n\\). From our definition, we see that evaluating \\(\\varphi\\) on a set of vectors \\(\\boldsymbol{v}_1,...,\\boldsymbol{v}_k\\) returns the signed \\(k\\)-dimensional volume of the projection of the parallelogram spanned by \\(\\boldsymbol{v}_1,...,\\boldsymbol{v}_k\\) onto the \\((i_1,..., i_k)\\)-axes (the projection will itself be a parallelogram). In general, a \\(k\\)-form takes in a \\(k\\)-parallelogram (it takes as input \\(k\\) vectors, but we interpret those vectors as the parallogram which they span), and returns a number proportional to the \\(k\\)-dimensional volume of the parallelogram. The details of this intuitive description of \\(k\\)-forms can be found in this discussion: https://math.stackexchange.com/questions/548131/whats-the-geometrical-intuition-behind-differential-forms. Here is another interesting article about the intuition behind forms (and other notions of integration) by Terry Tao: https://terrytao.wordpress.com/2007/12/25/pcm-article-differential-forms/. As we’ll see, \\(k\\)-forms are just the kind of integrand we need to integrate over \\(k\\)-dimensional manifolds with orientation. Intuitively, a \\(k\\)-form will give us the signed volume of a parallelogram based on a point on a manifold, spanned by vectors of partial derivatives of a parametrization of the manifold. This is very similar to when we compute unsigned volumes of manifolds, with the only difference here being that the signed volume takes orientation into account. Definition 3.25 (Elementary forms) An elementary \\(k\\)-form is a form of type \\[ \\begin{aligned} d_{x_1}\\land\\cdots\\land dx_{i_k} \\end{aligned} \\] where \\(1\\le i_1 &lt; i_2 &lt; \\cdots &lt; i_k\\le n\\). The only elementary \\(0\\)-form is the form, denoted by \\(1\\), which evaluated on zero vectors returns \\(1\\). Definition 3.26 (Space of forms) The space of \\(k\\)-forms on \\(\\mathbb{R}^n\\) is a vector space and is denoted by \\(A^k_c(\\mathbb{R}^n)\\). Theorem 3.48 (Elementary k-forms form a basis of the space of forms) The elementary \\(k\\)-forms form a basis of \\(A^k_c(\\mathbb{R}^n)\\). Hence, every \\(k\\)-form \\(\\varphi\\) can be uniquely written as a linear combination \\[ \\begin{aligned} \\varphi &amp;= \\sum_{1\\le i_1 &lt; \\cdots &lt; i_k\\le n} a_{i_1 ,..., i_k}dx_{i_1}\\land \\cdots\\land dx_{i_k} \\end{aligned} \\] where the coefficients are given by \\[ \\begin{aligned} a_{i_1,...,i_k} &amp;= \\varphi(\\boldsymbol{e}_{i_1}, ..., \\boldsymbol{e}_{i_k}) \\end{aligned} \\] Hence, \\(\\dim A^k_c(\\mathbb{R}^n) = \\binom{n}{k}\\). 3.11.3 The wedge (exterior) product The wedge product takes two forms and returns another form which takes more arguments. The definition will justify the use of the \\(\\land\\) symbol in elementary forms. Definition 3.27 (Wedge product) The wedge product of two forms \\(\\varphi\\in A^k_c(\\mathbb{R}^n)\\) and \\(\\omega\\in A^l_c(\\mathbb{R}^n)\\) is the element \\(\\varphi\\land\\omega\\in A^{k + l}_c(\\mathbb{R}^n)\\) defined by \\[ \\begin{aligned} (\\varphi\\land\\omega)(\\boldsymbol{v}_1,...,\\boldsymbol{v}_{k + l}) &amp;= \\sum_{\\sigma\\in \\text{Perm}(k, l)}\\text{sgn}(\\sigma)\\varphi(\\boldsymbol{v}_{\\sigma(1)}, ..., \\boldsymbol{v}_{\\sigma(k)})\\omega(\\boldsymbol{v}_{\\sigma(k + 1)}, ..., \\boldsymbol{v}_{\\sigma(k + l)}) \\end{aligned} \\] where above, \\(\\text{Perm}(k, l)\\) is the set of all permutations \\(\\sigma\\) of \\([1, k + l]\\) such that \\(\\sigma(1) &lt; \\sigma(2) \\cdots &lt; \\sigma(k)\\) and \\(\\sigma(k + 1) &lt; \\cdots &lt; \\sigma(k + l)\\). Proposition 3.14 (Properties of wedge products) The wedge product satisfies the following properties: Distributivity, i.e \\(\\varphi\\land (\\omega_1 + \\omega_2) = \\varphi\\land \\omega_1 + \\varphi\\land \\omega_2\\). Associativity, i.e \\((\\varphi_1\\land\\varphi_2)\\land \\varphi_3 = \\varphi_1\\land (\\varphi_2\\land \\varphi_3)\\). Skew commutativity, i.e if \\(\\varphi\\) is a \\(k\\)-form and \\(\\omega\\) is an \\(l\\)-form, then \\[ \\begin{aligned} \\varphi\\land\\omega &amp;= (-1)^{kl}\\omega\\land\\varphi \\end{aligned} \\] 3.11.4 Form fields In general, an \\(x\\)-field on a set \\(U\\subset\\mathbb{R}^n\\) is just a mapping that assigns to each point of \\(U\\) an object of type \\(x\\). This is exactly what a form field is going to be; these fields are also called differential forms. Definition 3.28 (Form fields) A \\(k\\)-form field on an open subset \\(U\\subset\\mathbb{R}^n\\) is a map \\(\\varphi:U\\to A^k_c(\\mathbb{R}^n)\\). The space of \\(k\\)-form fields on \\(U\\) is denoted by \\(A^k(U)\\). Let’s disect the definition a bit and see what’s really going on. Let \\(\\varphi\\in A^k(U)\\) be a \\(k\\)-form field on \\(U\\). So, for each \\(\\boldsymbol{x}\\in U\\), \\(\\varphi(\\boldsymbol{x})\\in A^k_c(\\mathbb{R}^n)\\). We can write this form as a linear combination of the elementary forms (since they are a basis): \\[ \\begin{aligned} \\varphi(\\boldsymbol{x}) &amp;= \\sum_{1\\le i_1 &lt; i_2 &lt; \\cdots &lt; i_k\\le n} a_{i_1,...,i_k}(\\boldsymbol{x})dx_{i_1}\\land\\cdots \\land dx_{i_k} \\end{aligned} \\] Note that the coefficients \\(a_{i_1,...,i_k}\\) are really functions \\(U\\to\\mathbb{R}\\) (Equivalently, the coefficients are \\(0\\)-form fields, which are just functions). So, such a form field \\(\\varphi\\) is to be said of class \\(\\mathscr{C}^p\\) if each of these coefficient functions are of class \\(\\mathscr{C}^p\\). From this, we can immediately (kind-of) see a notion of integration of forms fields. Note that, a form field associates to \\(U\\) a set of functions (the coefficient functions) and signed volume elements (namely the elementary forms); this is exactly the data we need to perform integration. However, as we’ll see later, to make such a notion of integration well-defined, we’ll also need a notion of orienation. Finally, an important interpretation: let \\(\\varphi\\in A^k(U)\\) be a \\(k\\)-form field on \\(U\\). To each point \\(\\boldsymbol{x}\\in U\\), the form field associated a \\(k\\)-form \\(\\varphi(\\boldsymbol{x})\\) to \\(\\boldsymbol{x}\\). Recall that \\(k\\)-forms are nothing but objects that take as input a parallelogram, and return a number proportional to the \\(k\\)-dimensional volume of a projection of the parallelogram. To that end, we introduce the notation \\[ \\begin{aligned} \\varphi(P_{\\boldsymbol{x}}(\\boldsymbol{v}_1,...,\\boldsymbol{v}_k)) := \\varphi(\\boldsymbol{x})(\\boldsymbol{v}_1,...,\\boldsymbol{v}_k) \\end{aligned} \\] where above, as usual, \\(P_{\\boldsymbol{x}}(\\boldsymbol{v}_1 ,..., \\boldsymbol{v}_k)\\) is the parallelogram spanned by these vectors. This geometric intuition is important to understand why we deal with forms in the first place: they provide a notion of a signed integrand, helpful to perform integration with orientation. 3.11.5 Integrating form fields over parametrized domains First, we’ll define what it means to integrate a form over a manifold given by a parametrization. In this setting, the orientation of the manifold will be part of the parametrization. Definition 3.29 (Integrating a form field over a parametrized domain) Let \\(U\\subset\\mathbb{R}^k\\) be a bounded open set with \\(\\text{vol}_k \\partial U = 0\\). Let \\(\\gamma: U\\to\\mathbb{R}^n\\) be a \\(\\mathscr{C}^1\\) mapping. The set \\(\\gamma(U)\\) is said to be a domain in \\(\\mathbb{R}^n\\) parametrized by \\(U\\), and the pair \\((U, \\gamma)\\) is denoted by \\([\\gamma(U)]\\) (the pair is usually thought of as the image set along with some extra data, i.e how \\(\\gamma\\) defines an implicit orientation on \\(U\\)). Let \\(V\\subset\\mathbb{R}^n\\) be open, and suppose \\(\\gamma(U)\\subset V\\). Also, let \\(\\varphi\\) be a \\(k\\)-form field on \\(V\\). The integral of \\(\\varphi\\) over \\([\\gamma(U)]\\) is defined as \\[ \\begin{aligned} \\int_{[\\gamma(U)]}\\varphi := \\int_U \\varphi\\left(P_{\\gamma(\\boldsymbol{u})}(D_1\\gamma(\\boldsymbol{u})), ..., D_k(\\gamma(\\boldsymbol{u}))\\right) |d^k \\boldsymbol{u}| \\end{aligned} \\] i.e we compute the form field over the parallelogram spanned by the partial derivatives of the parametrization \\(\\gamma\\). Remark (Parametrized domains need not be parametrizations of manifolds). It should be noted that in this definition, \\(\\gamma\\) is any \\(\\mathscr{C}^1\\) map (and need not satisfy the definitions of parametrizations/relaxed parametrizations of manifolds that we have defined). References Arnold, V. I. 1989. Mathematical Methods of Classical Mechanics. Second. Springer. John H. Hubbard, Barbara Burke Hubbard. 2009. Vector Calculus, Linear Algebra and Differential Forms. Fifth. Matrix Editions. "],["exercises-for-section-6.html", "3.12 Exercises for Section 6.2", " 3.12 Exercises for Section 6.2 6.2.1 In this exercise, we’ll compute integrals of some form fields over parametrized domains. \\(\\int_{[\\gamma(I)]}xdy + ydz\\), where \\(I = [-1, 1]\\) and \\(\\gamma(t) = (\\sin t, \\cos t, t)\\). For this one, the calculation is straightforward: \\[ \\begin{aligned} \\int_{[\\gamma(I)]}xdy + ydz &amp;= \\int_{-1}^1(xdy + ydz)\\begin{pmatrix}\\partial \\sin t/\\partial t \\\\ \\partial\\cos t/\\partial t \\\\ \\partial t / \\partial t\\end{pmatrix} |dt|\\\\ &amp;= \\int_{-1}^1(xdy + ydz)\\begin{pmatrix}\\cos t\\\\ -\\sin t\\\\ 1\\end{pmatrix} |dt|\\\\ &amp;= \\int_{-1}^1 (\\cos t(-\\sin t) - \\sin t)|dt|\\\\ &amp;= \\int_{-1}^1 -\\sin t(1 + \\cos t)|dt| \\end{aligned} \\] \\(\\int_{\\gamma(U)} x_1dx_2\\land dx_3 + x_2dx_3\\land dx_4\\), where \\(U = \\left\\{(u, v): 0\\le u, v; u + v\\le 2\\right\\}\\) and \\(\\gamma(u, v) = (uv, u^2 + v^2, u - v, \\ln(u + v + 1))\\). We have the following: \\[ \\begin{aligned} \\int_{\\gamma(U)} x_1dx_2\\land dx_3 + x_2dx_3\\land dx_4 &amp;= \\int_U (x_1dx_2\\land dx_3 + x_2dx_3\\land dx_4)\\begin{pmatrix} v &amp; u \\\\ 2u &amp; 2v \\\\ 1 &amp; -1 \\\\ \\frac{1}{u + v - 1} &amp; \\frac{1}{u + v - 1}\\end{pmatrix} |du||dv|\\\\ &amp;= \\int_{U} -2uv(u + v) + (u^2 + v^2)\\cdot 0 |du||dv|\\\\ &amp;= \\int_U -2uv(u + v) |du||dv|\\\\ &amp;= \\int_{0}^2\\int_{0}^{2 - v}-2uv(u + v)|du||dv| \\end{aligned} \\] "],["orientation-of-manifolds.html", "3.13 Orientation of manifolds", " 3.13 Orientation of manifolds We first begin by defining orientations of vector spaces. "],["higher-dimensional-statistics.html", "Chapter 4 Higher dimensional statistics ", " Chapter 4 Higher dimensional statistics "],["tail-and-concentration-bounds.html", "4.1 Tail and concentration bounds", " 4.1 Tail and concentration bounds Tail and concentration bounds can be used to bound the tail-probabilities of a random variable, or to study probability bounds on the deviation of a random variable from it’s mean. This is usually done by controlling the higher-order moments of a random variable; the better we can do it, the better bounds we get. 4.1.1 Classical bounds: Markov, Chebyshev, Chernoff The first useful bound is Markov’s inequality, which states that for a non-negative random variable \\(X\\) with finite mean, we have: \\[ \\begin{aligned} \\mathbb{P}[X\\ge t] \\le \\frac{\\mathbb{E}[X]}{t}\\quad \\forall t &gt; 0 \\end{aligned} \\] By applying this bound to the random variable \\((X - \\mathbb{E}[X])^2\\) for a random variable with finite second moment, we can obtain Chebyshev’s inequality: \\[ \\begin{aligned} \\mathbb{P}[|X - \\mathbb{E}[X]| \\ge \\epsilon] \\le \\frac{\\textbf{Var}[X]}{\\epsilon^2} \\end{aligned} \\] Let’s try generalizing this to higher-order moments. So, let \\(X\\) be a random variable with finite \\(k\\)-th order moment, i.e \\(\\mathbb{E}[|X|^k] &lt; \\infty\\) (and hence all \\(r\\)th moments with \\(k &lt; r\\) also exist), and consider the random variable \\(X - \\mathbb{E}[X]\\). In that case, for any \\(t &gt; 0\\), we have \\[ \\begin{aligned} |X - \\mathbb{E}[X]| \\ge t \\iff |X - \\mathbb{E}[X]|^k \\ge t^k \\end{aligned} \\] and hence \\[ \\begin{aligned} \\mathbb{P}[|X - \\mathbb{E}[X]|\\ge t] &amp;= \\mathbb{P}[|X - \\mathbb{E}[X]|^k\\ge t^k]\\\\ \\le \\frac{\\mathbb{E}[|X - \\mathbb{E}[X]|^k]}{t^k} \\end{aligned} \\] We can also apply Markov’s inequality to any function to get even better results. For example, suppose \\(X\\) is a random variable such that the function \\(\\varphi(\\lambda) := \\mathbb{E}[e^{\\lambda (X - \\mathbb{E}[X])}] &lt; \\infty\\) for all \\(|\\lambda| \\le b\\). In that case, applying Markov’s inequality to the random variable \\(e^{\\lambda(X - \\mathbb{E}[X])}\\) with \\(\\lambda\\in[0, b]\\), we get \\[ \\begin{aligned} \\mathbb{P}[X - \\mathbb{E}[X] \\ge t] &amp;= \\mathbb{P}[e^{\\lambda (X - \\mathbb{E}[X])}\\ge e^{\\lambda t}]\\\\ &amp;\\le \\frac{\\mathbb{E}[e^{\\lambda(X - \\mathbb{E}[X])}]}{e^{\\lambda t}} \\end{aligned} \\] The Chernoff bound is obtained by optimizing for the optimal value of \\(\\lambda\\): \\[ \\begin{aligned} \\log\\mathbb{P}[(X - \\mathbb{E}[X])\\ge t] &amp;\\le \\inf_{\\lambda \\in[0, b]}\\log \\mathbb{E}[\\lambda(X - \\mathbb{E}[X])] - \\lambda t \\end{aligned} \\] "],["sub-gaussian-variables-and-hoeffding-bounds.html", "4.2 Sub-Gaussian variables and Hoeffding bounds", " 4.2 Sub-Gaussian variables and Hoeffding bounds In this section we motivate the notion of sub-Gaussian random variables. Let \\(X\\sim \\mathcal{N}(\\mu, \\sigma^2)\\) be a Gaussian random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then, for any \\(\\lambda\\in\\mathbb{R}\\), we have \\[ \\begin{aligned} \\mathbb{E}[e^{\\lambda X}] &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^\\infty e^{\\lambda x}e^{\\frac{-(x - \\mu)^2}{2\\sigma^2}} dx \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int_{-\\infty}^\\infty e^{\\lambda x - \\frac{x^2}{2\\sigma^2} - \\frac{-\\mu^2}{2\\sigma^2} + \\frac{x\\mu}{\\sigma^2}}dx \\end{aligned} \\] "],["convex-optimization.html", "Chapter 5 Convex optimization ", " Chapter 5 Convex optimization "],["convex-sets.html", "5.1 Convex sets", " 5.1 Convex sets Definition 5.1 (Lines and line segments) Let \\(\\boldsymbol{x}_1\\ne \\boldsymbol{x}_2\\) be two points in \\(\\mathbb{R}^n\\). Points of the form \\[ \\begin{aligned} y &amp;= \\theta \\boldsymbol{x}_1 + (1 - \\theta)\\boldsymbol{x}_2 \\end{aligned} \\] for \\(\\theta\\in\\mathbb{R}\\) form the line passing through \\(\\boldsymbol{x}_1\\) and \\(\\boldsymbol{x}_2\\). Points for \\(\\theta\\in[0, 1]\\) form the line segment between \\(\\boldsymbol{x}_1\\) and \\(\\boldsymbol{x}_2\\). Definition 5.2 (Affine sets and affine hulls) A set \\(C\\subset \\mathbb{R}^n\\) is affine if the line through any two distinct points in \\(C\\) lies in \\(C\\). A point of the form \\(\\theta_1 \\boldsymbol{x}_1 + \\cdots + \\theta_k \\boldsymbol{x}_k\\) where \\(\\theta_1 + \\cdots + \\theta_k = 1\\) is called an affine combination of the points \\(\\boldsymbol{x}_1, ..., \\boldsymbol{x}_k\\). For any set \\(C\\subset\\mathbb{R}^n\\), the affine hull of \\(C\\) is defined to be the set of all affine combinations of points in \\(C\\). It is the smallest affine set that contains \\(C\\). Proposition 5.1 If \\(C\\) is an affine set and if \\(\\boldsymbol{x}_0\\in C\\), then the set \\[ \\begin{aligned} V := C - \\boldsymbol{x}_0 \\end{aligned} \\] is a subspace. Moreover, this subspace does not depend on the choice of \\(\\boldsymbol{x}_0\\). The dimension of the affine set \\(C\\) is defined to be the dimension of \\(V\\). 5.1.1 Affine dimension and relative interior Definition 5.3 The affine dimension of a set \\(C\\) is the dimension of its affine hull. The relative interior of a set \\(C\\), denoted by \\(\\textbf{relint}(C)\\), is the interior relative to \\(\\textbf{aff}(C)\\): \\[ \\begin{aligned} \\textbf{relint}(C) := \\left\\{\\boldsymbol{x}\\in C | B(\\boldsymbol{x}, r)\\cap \\textbf{aff}(C)\\subset C\\text{ for some }r &gt; 0\\right\\} \\end{aligned} \\] The relative boundary is then defined to be \\(\\overline{C}\\setminus \\textbf{relint}(C)\\), where \\(\\overline{C}\\) is the closure of \\(C\\). 5.1.2 Convex sets Definition 5.4 A set \\(C\\) is convex if the line segment between any two points in \\(C\\) lies in \\(C\\). The convex hull of a set \\(C\\), denoted by \\(\\textbf{conv}(C)\\), is the set of all convex combinations of points in \\(C\\); it is the smallest convex set that contains \\(C\\). 5.1.3 Generalized inequalities Definition 5.5 (Proper cones) A cone \\(K\\subset \\mathbb{R}^n\\) is called a proper cone if the following are true: \\(K\\) is convex. \\(K\\) is closed. \\(K\\) is solid, i.e it has a non-empty interior. \\(K\\) is pointed, i.e it contains no line. Proper cones can be used to define so called generalized inequalities in \\(\\mathbb{R}^n\\); these are partial orderings of \\(\\mathbb{R}^n\\). For a proper cone \\(K\\), we define \\[ \\begin{aligned} \\boldsymbol{x} \\preceq_K \\boldsymbol{y} \\iff \\boldsymbol{y} - \\boldsymbol{x}\\in K \\end{aligned} \\] An associated strict partial ordering is defined by the following: \\[ \\begin{aligned} \\boldsymbol{x} \\prec_K \\boldsymbol{y} \\iff \\boldsymbol{y} - \\boldsymbol{x}\\in \\textbf{int}(K) \\end{aligned} \\] 5.1.4 Separating and supporting hyperplanes In this section, we’ll state and prove a famous theorem about the existence of hyperplanes that separate disjoint convex sets. Theorem 5.1 (Separating hyperplane theorem) Let \\(C\\) and \\(D\\) be nonempty disjoint convex sets, i.e \\(C\\cap D = \\phi\\). Then, there exist \\(\\boldsymbol{a}\\ne 0\\) and \\(b\\) such that \\(\\boldsymbol{a}^T \\boldsymbol{x}\\le b\\) for all \\(x\\in C\\) and \\(\\boldsymbol{a}^T \\boldsymbol{x}\\ge b\\) for all \\(x\\in D\\). Such a hyperplane is said to be a separating hyperplane. Proof. We first assume that the distance between \\(C\\) and \\(D\\) is positive, and that there are points in \\(C\\) and \\(D\\) which achieve this distance; the distance is defined to be \\[ \\begin{aligned} \\textbf{dist}(C, D) &amp;= \\inf\\left\\{\\lVert \\boldsymbol{u} - \\boldsymbol{v}\\rVert_2: \\boldsymbol{u}\\in C, \\boldsymbol{v}\\in D\\right\\} \\end{aligned} \\] After proving the theorem with this assumption, we’ll do the general proof. So, let \\(\\boldsymbol{c}\\in C\\) and \\(\\boldsymbol{d}\\in D\\) be points that achieve the minimum distance. Define \\[ \\begin{aligned} \\boldsymbol{a} := \\boldsymbol{d} - \\boldsymbol{c}\\quad,\\quad b = \\frac{\\lVert \\boldsymbol{d}\\rVert_2^2 - \\lVert \\boldsymbol{c}\\rVert_2^2}{2} \\end{aligned} \\] We then show that the affine function \\(f(b\\boldsymbol{x}) := \\boldsymbol{a}^T \\boldsymbol{x} - b\\) is nonpositive on \\(C\\) and nonnegative on \\(D\\). It can be checked that this hyperplane is perpendicular to the line segment between \\(\\boldsymbol{c}\\) and \\(\\boldsymbol{d}\\) and that it passes through it’s midpoint. We show that \\(f\\) is nonnegative on \\(D\\), and a symmetric argument will complete the proof of nonpositivity on \\(C\\) by considering \\(-f\\). So, suppose there were a point \\(\\boldsymbol{u}\\in D\\) for which \\[ \\begin{aligned} f(\\boldsymbol{u}) &amp;= (\\boldsymbol{d} - \\boldsymbol{c})^T\\left(\\boldsymbol{u} - \\frac{1}{2}(\\boldsymbol{d} + \\boldsymbol{c})\\right) &lt; 0 \\end{aligned} \\] Now, \\(f(\\boldsymbol{u})\\) can be written as \\[ \\begin{aligned} f(\\boldsymbol{u}) &amp;= (\\boldsymbol{d} - \\boldsymbol{c})^T(\\boldsymbol{u} - \\boldsymbol{d}) + (1/2)(\\boldsymbol{d} - \\boldsymbol{c}))\\\\ &amp;= (\\boldsymbol{d} - \\boldsymbol{c})^T(\\boldsymbol{u} - \\boldsymbol{d}) + 1/2\\lVert \\boldsymbol{d} - \\boldsymbol{c}\\rVert_2^2 \\end{aligned} \\] Note that \\(f(\\boldsymbol{u}) &lt; 0\\) implies that \\((\\boldsymbol{d} - \\boldsymbol{c})^T(\\boldsymbol{u} - \\boldsymbol{d}) &lt; 0\\). Moreover, observe that \\[ \\begin{aligned} \\left.\\frac{d}{dt}\\lVert \\boldsymbol{d} + t(\\boldsymbol{u} - \\boldsymbol{d}) - \\boldsymbol{c}\\rVert_2^2\\right|_{t = 0} &amp;= 2(\\boldsymbol{d} - \\boldsymbol{c})^T(\\boldsymbol{u} - \\boldsymbol{d}) &lt; 0 \\end{aligned} \\] and hence for some \\(t &gt; 0\\), with \\(t\\le 1\\) we have \\[ \\begin{aligned} \\left\\lVert \\boldsymbol{d} + t(\\boldsymbol{u} - \\boldsymbol{d}) - \\boldsymbol{c}\\right\\rVert_2 &lt; \\left\\lVert \\boldsymbol{d} - \\boldsymbol{c}\\right\\rVert_2 \\end{aligned} \\] In simple words, the point \\(\\boldsymbol{d} + t(\\boldsymbol{u} - \\boldsymbol{d})\\) is closer to \\(\\boldsymbol{c}\\) than \\(\\boldsymbol{d}\\) is. Since \\(D\\) is convex and contains \\(\\boldsymbol{d}\\) and \\(\\boldsymbol{u}\\), we have \\(\\boldsymbol{d} + t(\\boldsymbol{u} - \\boldsymbol{d})\\in D\\). But this is impossible, since \\(\\boldsymbol{d}\\) is assumed to be the point in \\(D\\) which is the closest to \\(\\boldsymbol{c}\\). Next, we consider the general case. So, let \\(C, D\\) be non-empty, disjoint convex sets. Now, consider the set \\(C - D\\); clearly, \\(C - D\\) does not contain the origin. Moreover, \\(C - D\\) is a convex set (which is easily seen). Now, let \\(\\boldsymbol{p}\\in C - D\\) be any point, and consider the point \\(-\\boldsymbol{p}\\). Arnold, V. I. 1989. Mathematical Methods of Classical Mechanics. Second. Springer. John H. Hubbard, Barbara Burke Hubbard. 2009. Vector Calculus, Linear Algebra and Differential Forms. Fifth. Matrix Editions. Klenke, Achim. 2020. Probability Theory: A Comprehensive Course. 3rd ed. Springer. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
